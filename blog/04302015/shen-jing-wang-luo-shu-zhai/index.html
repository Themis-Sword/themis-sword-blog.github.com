
<!DOCTYPE HTML>
<html>
<head>
	<script data-cfasync="false" type="text/javascript" src="//use.typekit.net/axj3cfp.js"></script>
	<script data-cfasync="false" type="text/javascript">try{Typekit.load();}catch(e){}</script>
	<meta charset="utf-8">
	<title>神經網絡(書摘).  | Themis_Sword's Blog</title>


<meta name="author" content="Themis_Sword"> 

<meta name="description" content="神經網絡(書摘)"> <meta name="keywords" content="神經網絡">


<meta name="msvalidate.01" content="3084BF25D05B2AD0D58CBE0DEA3D888E" />

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<link href="/atom.xml" rel="alternate" title="Themis_Sword's Blog" type="application/atom+xml">
	<link rel="canonical" href="">
	<link href="/favicon.png" rel="shortcut icon">
	<link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
	<!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
	<script src="//ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js"></script>
	<script type="text/javascript" src="/javascripts/jquery.fancybox.pack.js"></script>

<script language="Javascript" type="text/javascript">
$(document).ready(
  function() {
    (function($) {
      $(".fancybox[data-content-id]").each(function() {
        this.href = $(this).data('content-id');
      });
      $(".fancybox").fancybox({
        beforeLoad: function() {
          var el, 
              id = $(this.element).data('title-id');

          if (id) {
            el = $('#' + id);

            if (el.length) {
              this.title = el.html();
            }
          }
          if ($(this).data('content')) {
            this.content = $(this).data('content');
          }
        },
        helpers: {
          title: {
            type: 'inside'
          }
        }
      });
    })(jQuery);
  }
);
</script>

	

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-48083272-1', 'aprilzephyr.com');
  ga('send', 'pageview');

</script>

</head>



<body>
	<header id="header" class="inner"><h1><a href="/">Themis_Sword's Blog</a></h1>
<h4>Never Scared Endeavoring.</h4>
<nav id="main-nav"><ul>
	<li><a href="/">Home</a></li>
	<li><a href="/blog/categories">Categories</a></li>
    <li><a href="/portfolio">Portfolio</a>
    <li><a href="/gallery">Gallery</a><li>
	<li><a href="/author">Author</a></li>
</ul>



</nav>
<nav id="mobile-nav">
	<div class="alignleft menu">
		<a class="button">Menu</a>
		<div class="container"><ul>
	<li><a href="/">Home</a></li>
	<li><a href="/blog/categories">Categories</a></li>
    <li><a href="/portfolio">Portfolio</a>
    <li><a href="/gallery">Gallery</a><li>
	<li><a href="/author">Author</a></li>
</ul>



</div>
	</div>
	<div class="alignright search">
		<a class="button"></a>
		<div class="container">
			<form action="/search" method="get">
				<input type="text" name="q" results="0">
				<input type="hidden" name="q" value="site:www.aprilzephyr.com">
			</form>
		</div>
	</div>
</nav>


</header>

	<div id="content" class="inner"><article class="post">
	<h3 class="title">神經網絡(書摘).
</h3>
	<div class="entry-content"><p><img src="/images/astonishing.jpg"></p>

<p> “……我相信，對一個模型的最好的檢驗是它的設計者能否回答這些問題：‘現在你知道哪些原本不知道的東西？’以及‘你如何證明它是否是對的？’”  ——詹姆斯·鮑爾（<a href="http://en.wikipedia.org/wiki/James_M._Bower">James M.Bower</a>)<!--more--></p>

<p>神經網絡是由具有各種相互聯系的單元組成的集合。每個單元具有極為簡化的神經元的特性。神經網絡常常被用來模擬神經系統中某些部分的行為，生產有用的商業化裝置以及檢驗腦是如何工作的一般理論。</p>

<p>神經科學家們究竟為什麽那麽需要理論呢？如果他們能了解單個神經元的確切行為，他們就有可能預測出具有相互作用的神經元群體的特性。令人遺憾的是，事情並非如此輕而易舉。事實上，單個神經元的行為通常遠不那麽簡單，而且神經元幾乎總是以一種復雜的方式連接在一起。此外，整個系統通常是高度非線性的。線性系統，就其最簡單形式而言，當輸入加倍時，它的輸出也嚴格加倍——即輸出與輸入呈比例關系。①例如，在池塘的表面，當
兩股行進中的小湍流彼此相遇時，它們會彼此穿過而互不幹擾。為了計算兩股小水波聯合產生的效果，人們只需把第一列波與第二列波的效果在空間和時間的每一點上相加即可。這樣，每一列波都獨立於另一列的行為。對於大振幅的波則通常不是這樣。物理定律表明，大振幅情況下均衡性被打破。沖破一列波的過程是高度非線性的：一旦振幅超過某個閾值，波的行為完全以全新的方式出現。那不僅僅是“更多同樣的東西”，而是某些新的特性。非線性行為在日常生活中很普遍，特別是在愛情和戰爭當中。正如歌中的：“吻她一次遠不及吻她兩次的一半那麽美妙。”</p>

<p>如果一個系統是非線性的，從數學上理解它通常比線性系統要困難得多。它的行為可能更為復雜。因此對相互作用的神經元群體進行預測變得十分困難，特別是最終的結果往往與直覺相反。</p>

<p>高速數字計算機是近50年來最重要的技術發展之一。它時常被稱作馮.諾依曼計算機，以紀念這位傑出的科學家、計算機的締造者。由於計算機能像人腦一樣對符號和數字進行操作，人們自然地想像腦是某種形式相當復雜的馮·諾依曼計算機。這種比較，如果陷入極端的話，將導致不切實際的理論。</p>

<p>計算機是構建在固有的高速組件之上的。即便是個人計算機，其基本周期，或稱時鐘頻率，也高於每秒1000萬次操作。相反地，一個神經元的典型發放率僅僅在每秒100個脈沖的範圍內。計算機要快上百萬倍。而像克雷型機那樣的高速超級計算機速度甚至更高。大致說來，計算機的操作是序列式的，即一條操作接著一條操作。與此相反，腦的工作方式則通常是大規模並行的，例如，從每只眼睛到達腦的軸突大約有100萬個，它們全都同時工作。
在系統中這種高度的並行情況幾乎重復出現在每個階段。這種連線方式在某種程度上彌補了神經元行為上的相對緩慢性。它也意味著即使失去少數分散的神經元也不大可能明顯地改變腦的行為。用專業術語講，腦被稱作“故障弱化”（degrade gracefully)。而計算機則是脆弱的，哪怕是對它極小的損傷，或是程序中的一個小錯誤，也會引起大的災難。計算機中出現錯誤則是災難性的（degrade catastrophically)。</p>

<p>計算機在工作中是高度穩定的。因為其單個組件是很可靠的，當給定相同的輸入時通常產生完全同樣的輸出。反之，單個神經元則具有更多的變化。它們受可以調節其行為的信號所支配，有些特性邊“計算”邊改變。</p>

<p>一個典型的神經元可能具有來自各處的上百乃至數萬個輸入，其軸突又有大量投射。而計算機的一個基本元件——晶體管，則只有極少數的輸入和輸出。</p>

<p>在計算機中，信息被編碼成由0和1組成的脈沖序列。計算機通過這種形式高度精確地將信息從一個特定的地方傳送到另一個地方。信息可以到達特定的地址，提取或者改變那裏所儲存的內容。這樣就能夠將信息存入記憶體的某個特殊位置，並在以後的某些時刻進一步加以利用。這種精確性在腦中是不會出現的。盡管一個神經元沿它的軸突發送的脈沖的模式（而不僅僅是其平均發放率）可能攜帶某些信息，但並不存在精確的由脈沖編碼的信息。①這樣，記憶必然將以不同的形式“存儲”。</p>

<p>腦看起來一點也不像通用計算機。腦的不同部分，甚至是新皮層的不同部分，都是專門用來處理不同類型的信息的（至少在某種程度上是這樣的）。看來大多數記憶存儲在進行當前操作的那個地方。所有這些與傳統的馮·諾依曼計算機完全不同，因為執行計算機的基本操作（如加法.乘法等等）僅在一個或少數幾個地方，而它的記憶卻存貯在許多很不同的地方。</p>

<p>最後，計算機是由工程師精心設計出來的，而腦則是動物經自然選擇一代又一代進化而來的。這就產生了如第一章所述的本質上不同的設計形式。</p>

<p>人們習慣於從硬件和軟件的角度來談論計算機。由於人們編寫軟件（計算機程序）時幾乎不必了解硬件（回路等）的細節，所以人們——特別是心理學家——爭論說沒必要了解有關腦的“硬件”的任何知識。實際上想把這種理論強加到腦的操作過程中是不恰當的，腦的硬件與軟件之間並沒有明顯的差異。對於這種探討的一種合理的解釋是，雖然腦的活動是高度並行的，在所有這些平行操作的頂端有某些形式的（由註意控制的）序列機制，因而，在腦的操作的較高層次，在那些遠離感覺輸入的地方，可以膚淺地說腦與計算機有某種相似之處。</p>

<p>人們可以從一個理論途徑的成果來對它作判斷。計算機按編寫的程序執行，因而擅長解決諸如大規模數字處理、嚴格的邏輯推理以及下棋等某些類型的問題。這些事情大多數人都沒有它們完成得那麽快、那麽好。但是，面對常人能快速、不費氣力就能完成的任務，如觀察物體並理解其意義，即便是最現代的計算機也顯得無能為力。</p>

<p>近幾年在設計新一代的、以更加並行方式工作的計算機方面取得了重要進展。大多數設計使用了許多小型計算機，或是小型計算機的某些部件。它們被連接在一起，並同時運行。由一些相當復雜的設備來處理小計算機之間的信息交換並對計算進行全局控制。像天氣預測等類似問題，其基本要素在多處出現。此時超級計算機特別有用。</p>

<p>人工智能界也采取了行動設計更具有腦的特點的程序。他們用一種模糊邏輯取代通常計算中使用的嚴格的邏輯。命題不再一定是真的或假的，而只需是具有更大或更小的可能性。程序試圖在一組命題中發現具有最大可能性的那種組合，並以之作為結論，而不是那些它認為可能性較小的結論。</p>

<p>在概念的設置上，這種方法確實比早期的人工智能方法與腦更為相像，但在其他方面，特別是在記憶的存貯上，則不那麽像腦。因此，要檢查它與真實的腦在所有層次上行為的相似性可能會有困難。</p>

<p>一群原先很不知名的理論工作者發展了一種更具有腦的特性的方法。如今它被稱為PDP方法（即平行分布式處理）。這個話題有很長的歷史，我只能概述一二。在1943年沃侖·麥卡洛克（Warrenc McCulloch）和沃爾特·皮茲（Walter Pitts）的工作是這方面最早的嘗試之一。他們表明，在原則上由非常簡單的單元連接在一起組成的“網絡”可以對任何邏輯和算術函數進行計算。因為網絡的單元有些像大大簡化的神經元，它現在常被稱作“神經網絡”。</p>

<p>這個成就非常令人鼓舞，以致它使許多人受到誤導，相信腦就是這樣工作的。或許它對現代計算機的設計有所幫助，但它的最引人註目的結論就腦而言則是極端錯誤的。</p>

<p>下一個重要的進展是弗蘭克·羅森布拉特（Frank Rosenblatt）發明的一種非常簡單的單層裝置，他稱之為感知機（Perceptron)。意義在於，雖然它的連接最初是隨機的，它能使用一種簡單而明確的規則改變這些連接，因而可以教會它執行某些簡單的任務，如識別固定位置的印刷字母。感知機的工作方式是，它對任務只有兩種反應：正確或是錯誤。你只需告訴它它所作出的（暫時的）回答是否正確。然後它根據一種感知機學習規則來改變其連接。羅森布拉特證明，對於某一類簡單的問題——“線性可分”的問題——感知機通過有限次訓練就能學會正確的行為。</p>

<p>由於這個結果在數學上很優美，從而吸引了眾人的註目。只可惜它時運不濟，它的影響很快就消退了。馬文·明斯基（MarVinMinsky)和西摩·佩伯特（Segmour Papert)證明感知機的結構及學習規則無法執行“異或問題”（如，判斷這是蘋果還是桔子，但不是二者皆是），因而也不可能學會它。他們寫了一本書，通篇詳述了感知機的局限性。這在許多年內扼殺了人們對感知機的興趣（明斯基後來承認做得過分了）。此問大部分工作將註意力轉向人工智能方法。①</p>

<p>用簡單單元構建一個多層網絡，使之完成簡單的單層網絡所無法完成的異或問題（或類似任務），這是可能的。這種網絡必定具有許多不同層次上的連接，問題在於，對哪些最初是隨機的連接進行修改才能使網絡完成所要求的操作。如果明斯基和佩伯特為這個問題提供了解答，而不是把感知機打入死路的話，他們的貢獻會更大些。</p>

<p>下一個引起廣泛註意的發展來自約翰·霍普菲爾德（John Hop-field)，一位加利福尼亞州理工學院的物理學家，後來成為分子生物學家和腦理論家。1982年他提出了一種網絡，現在被稱為霍普菲爾德網絡(見圖53）。這是一個具有自反饋的簡單網絡。每個單元只能有兩種輸出：一1（表示抑制）或十1（表示興奮）。但每個單元具有多個輸入。每個連接均被指派一個特定的強度。在每個時刻單元把來自它的全部連接的效果(2)總和起來。如果這個總和大於0則置輸出狀態為十1（平均而言，當單元興奮性輸入大於抑制性輸人時，則輸出為正），否則就輸出一1。有些時候這意味著一個單元的輸出會因為來自其他單元的輸入發生了改變而改變。</p>

<p>盡管如此，仍有不少理論工作者默默無聞地繼續工作。這其中包括斯蒂芬.格羅斯伯格（stephen Grossberg），吉姆·安德森（Jim Anderson），托伊沃.科霍寧（TeuvoKohonen）和戴維·威爾肖（Devid Willshaw）。(2)每個輸入對單元的影響是將當前的輸入信號（+1或-1）與其相應的權值相乘而得到的。（如果當前信號是-1，權重是+2，則影響為-2。）</p>

<p>計算將被一遍遍地反復進行，直到所有單元的輸出都穩定為止。①在霍普菲爾德網絡中，所有單元的狀態並不是同時改變的，而是按隨機次序一個接一個進行，霍普菲爾德從理論上證明了，給定一組權重（連接強度）以及任何輸入，網絡將不會無限制地處於漫遊狀態，也不會進入振蕩，而是迅速達到一個穩態。①</p>

<p>霍普菲爾德的論證令人信服，表達也清晰有力。他的網絡對數學家和物理學家有巨大的吸引力，他們認為終於找到了一種他們可以涉足腦研究的方法（正如我們在加利福尼亞州所說的）。雖然這個網絡在許多細節上嚴重違背生物學，但他們並不對此感到憂慮。</p>

<p>如何調節所有這些連接的強度呢？194年，加拿大心理學家唐納德·赫布（Donald Hebb）出版了《行為的組織》一書。當時人們就像現在一樣普遍相信，在學習過程中，一個關鍵因素是神經元的連接（突觸）強度的調節。赫布意識到，僅僅因為一個突觸是活動的，就增加其強度，這是不夠的。他期望一種只在兩個神經元的活動相關時才起作用的機制。他的書中有一個後來被廣泛引用的段落：“當細胞A的一個軸突和細胞B 很近，足以對它產生影響，並且持久地、不斷地參與了對細胞B 的興奮，那麽在這兩個細胞或其中之一會發生某種生長過程或新陳代謝變化，以致於A作為能使B 興奮的細胞之一，它的影響加強了。”這個機制以及某些類似規則，現在稱為“赫布律”。</p>

<p>霍普菲爾德在他的網絡中使用了一種形式的赫布規則來調節連接權重。對於問題中的一種模式，如果兩個單元具有相同的輸出，則它們之間的相互連接權重都設為+1。如果它們具有相反的輸出，則兩個權重均設為-1。大致他說，每個單元激勵它的“朋友”並試圖削弱它的“敵人”。</p>

<p>霍普菲爾德網絡是如何工作的呢？如果網絡輸入的是正確的單元活動模式，它將停留在該狀態。這並沒有什麽特別的，因為此時給予它的就是答案。值得註意的是，如果僅僅給出模式的一小部分作為“線索”，它在經過短暫的演化後，會穩定在正確的輸出即整個模式上，在不斷地調節各個單元的輸出之後，網絡所揭示的是單元活動的穩定聯系。最終它將有效地從某些僅僅與其存貯的“記憶”接近的東西中恢復出該記憶，此外，這種記憶也被稱作是按“內容尋址”的——即它沒有通常計算機中具有的分離的、唯一用於作為“地址”的信號。輸入模式的任何可察覺的部分都將作為地址。這開始與人的記憶略微有些相似了。</p>

<p>請註意記憶並不必存貯在活動狀態中，它也可以完全是被動的，因為它是鑲嵌在權重的模式之中的即在所有各個單元之間的連接強度之中。網絡可以完全不活動（所有輸出置為0），但只要有信號輸入，網絡突然活動起來並在很短時間內進入與其應當記住的模式相對應的穩定的活動狀態。據推測，人類長期記憶的回憶具有這種一般性質（只是活動模式不能永久保持）。你能記住大量現在一時想不起來的事情。</p>

<p>神經網絡（特別是霍普菲爾德網絡）能“記住”一個模式，但是除此以外它還能再記住第二個模式嗎？如果幾個模式彼此不太相似，一個網絡是能夠全部記住這幾個不同模式，即給出其中一個模式的足夠大的一部分，網絡經過少數幾個周期後將輸出該模式。因為任何一個記憶都是分布在許多連接當中的，所以整個系統中記憶是分布式的。因為任何一個連接都可能包含在多個記憶中，因而記憶是可以疊加的。此外，記憶具有魯棒性，改變少數連接通常不會顯著改變網絡的行為。</p>

<p>為了實現這些特性就需要付出代價，這不足為奇。如果將過多的記憶加到網絡之中則很容易使它陷入混亂。即使給出線索，甚至以完整的模式作為輸入，網絡也會產生毫無意義的輸出。①</p>

<p>有人提出這是我們做夢時出現的現象（弗洛伊德稱之為“凝聚”——condensation），但這是題外話。值得註意的是，所有這些特性是“自然發生”的。它們並不是網絡設計者精心設置的，而是由單元的本性、它們連接的模式以及權重調節規則所決定的。</p>

<p>霍普菲爾德網絡還有另一個性質，即當幾個輸人事實上彼此大致相似時，在適當計算網絡的連接權重後，它“記住”的將是訓練的模式的某種平均。這是另一個與腦有些類似的性質。對我們人類而言，當我們聽某個特定的聲調時，即便它在一定範圍內發生變化，我們也會覺得它是一樣的。輸入是相似但不同的，而輸出——我們所聽到的——則是一樣的。</p>

<p>這些簡單網絡是不能和腦的復雜性相提並論的，但這種簡化確實使我們可能對它們的行為有所了解，即使是簡單網絡中出現的特點也可能出現在具有相同普遍特性的更復雜的網絡中，此外，它們向我們提供了多種觀點，表明特定的腦回路所可能具有的功能。例如，海馬中有一個稱為CA3的區域，它的連接事實上很像一個按內容尋址的網絡。當然，這是否正確尚需實驗檢驗。</p>

<p>有趣的是，這些簡單的神經網絡具有全息圖的某些特點。在全息圖中，幾個影像可以彼此重疊地存貯在一起；全息圖的任何一部分都能用來恢復整個圖像，只不過清晰度會下降；全息圖對於小的缺陷是魯棒的。對腦和全息圖兩者均知之甚少的人經常會熱情地支持這種類比。幾乎可以肯定這種比較是沒有價值的。原因有兩個。詳細的數學分析表明神經網絡和全息圖在數學上是不同的。更重要的是，雖然神經網絡是由那些與真實神經元有些相似的單元
構建的，沒有證據表明腦中具有全息圖所需的裝置或處理過程。（1）</p>

<p>一本更新的書產生了巨大的沖擊力，這就是戴維·魯梅爾哈特（David Rumelhart）、詹姆斯·麥克萊蘭（James McClelland）和PDP小組所編的一套很厚的兩卷著作《平行分布式處理》（1)。該書於1986年問世，並很快至少在學術界成為最暢銷書。名義上我也是PDP小組的成員，並和淺沼智行（Chiko Asanuma）合寫了其中的一個章節。不過我起的作用很小。我幾乎只有一個貢獻，就是堅持要求他們停止使用神經元一詞作為他們網絡的單元。</p>

<p>加利福尼亞州立大學聖叠戈分校心理系離索爾克研究所僅有大約一英裏。在70年代末80年代初我經常步行去參加他們的討論小組舉行的小型非正式會議。那時我時常漫步的地方如今已變成了巨大的停車場。生活的步伐越來越快，我現在已改為驅車飛馳於兩地之間了。</p>

<p>研究小組當時是由魯梅爾哈特和麥克萊蘭領導的，但是不久麥克萊蘭就離開前往東海岸了。他們倆最初都是心理學家，但他們對符號處理器感到失望並共同研制了處理單詞的“相互作用激勵器”的模型。在克裏斯托夫·朗格特-希金斯（Christopher Longuet-Higgins）的另一位學生傑弗裏·希爾頓（Geoffrey Hinton）的鼓勵下，他們著手研究一個更加雄心勃勃的“聯結主義”方案。他們采納了平行分布式處理這個術語，因為它比以前的術語——聯想記憶②——的覆蓋面更廣。</p>

<p>在人們發明網絡的初期，一些理論家勇敢地開始了嘗試。他們把一些仍顯笨拙的小型電子回路（其中常包括有老式繼電器）連接在一起來模擬他們的非常簡單的網絡。現在已發展出了復雜得多的神經網絡，這得益於現代計算機的運算速度得到了極大的提高，也很便宜。現在可以在計算機（這主要是數字計算機）上模擬檢驗關於網絡的新思想，而不必像早期的研究那樣僅靠粗糙的模擬線路或是用相當困難的數學論證。</p>

<p>1986年出版的《平行分布式處理》一書從1981年底開始經過了很長時間的醞釀。這很幸運，因為它是一個特殊算法的最新發展（或者說是它的復興或應用），在其早期工作基礎上，很快給人留下了深刻的印象。該書的熱情讀者不僅包括腦理論家和心理學家，還有數學家、物理學家和工程師，甚至有人工智能領域的工作者。不過後者最初的反應是相當敵視的。最終神經科學家和分子生物學家也對它的消息有所耳聞。</p>

<p>該書的副標題是“認知微結構的探索”。它是某種大雜燴，但是其中一個的特殊的算法產生了驚人的效果。該算法現在稱作“誤差反傳算法”，通常簡稱為“反傳法”。為了理解這個算法，你需要知道一些關於學習算法的一般性知識。</p>

<p>在神經網絡有些學習形式被稱作是“無教師的”。這意味著沒有外界輸入的指導信息。對任何連接的改變只依賴於網絡內部的局部狀態。簡單的赫布規則具有這種特點。與之相反，在有教師學習中，從外部向網絡提供關於網絡執行狀況的指導信號。</p>

<p>無教師學習具有很誘人的性質，因為從某種意義上說網絡是在自己指導自己。理論家們設計了一種更有效的學習規則，但它需要一位“教師”來告訴網絡它對某些輸入的反應是好、是差還是很糟。這種規則中有一個稱作“δ律”。</p>

<p>訓練一個網絡需要有供訓練用的輸入集合，稱作“訓練集”。很快我們在討論網絡發音器（NETtalk）時將看到一個這樣的例子。這有用的訓練集必須是網絡在訓練後可能遇到的輸入的合適的樣本。通常需要將訓練集的信號多次輸入，因而在網絡學會很好地執行之前需要進行大量的訓練。其部分原因是這種網絡的連接通常是隨機的。而從某種意義上講，腦的初始連接是由遺傳機制控制的，通常不完全是隨機的。</p>

<p>網絡是如何進行訓練的呢？當訓練集的一個信號被輸入到網絡中，網絡就會產生一個輸出。這意味著每個輸出神經元都處在一個特殊的活動狀態。教師則用信號告訴每個輸出神經元它的誤差，即它的狀態與正確之間的差異，δ這個名稱便來源於這個真實活動與要求之間的差異（數學上δ常用來表示小而有限的差異）。網絡的學習規則利用這個信息計算如何調整權重以改進網絡的性能。</p>

<p>Adaline網絡是使用有教師學習的一個較早的例子。它是1960年由伯納德·威德羅（Bernard widrow）和霍夫（M.E.Hoff）設計的，因此δ律又稱作威德羅-霍夫規則。他們設計規則使得在每一步修正中總誤差總是下降的。①這意味著隨著訓練過程網絡最終會達到一個誤差的極小值。這是毫無疑問的，但還不能確定它是真正的全局極小還是僅僅是個局域極小值。用自然地理的術語說就是，我們達到的是一個火山口中的湖，還是較低的池塘。海洋，還是像死海那樣的凹下去的海（低於海平面的海）？</p>

<p>訓練算法是可以調節的，因而趨近局域極小的步長可大可小。如果步長過大，算法會使網絡在極小值附近跳來跳去（開始時它會沿下坡走，但走得太遠以致又上坡了）。如果步子小，算法就需要極長的時間才能達到極小值的底端。人們也可以使用更精細的調節方案。</p>

<p>反傳算法是有教師學習算法中的一個特殊例子。為了讓它工作，網絡的單元需要具有一些特殊性質。它們的輸出不必是二值的（即，或0，或者＋1或-1），而是分成若幹級。它通常在0到+1之間取值。理論家們盲目地相信這對應於神經元的平均發放率（取最大發放率為＋1），但他們常常說不清應該在什麽時候取這種平均。</p>

<p>如何確定這種“分級”輸出的大小呢？像以前一樣，每個單元對輸入加權求和，但此時不再有一個真實的閾值。如果總和很小，輸出幾乎是0。總和稍大一些時，輸出便增加。當總和很大時，輸出接近於最大值。圖54所示的S形函數（sigmoid函數）體現了這種輸入總和與輸出間的典型關系。如果將一個真實神經元的平均發放率視為它的輸出，那麽它的行為與此相差不大。</p>

<p>這條看似平滑的曲線有兩個重要性質。它在數學上是“可微的”，即任意一處的斜率都是有限的；反傳算法正依賴於這個特性。更重要的是，這條曲線是非線性的，而真實神經元即是如此。當（內部）輸入加倍時輸出並不總是加倍。這種非線性使得它能處理的問題比嚴格的線性系統更加廣泛。</p>

<p>現在讓我們看一個典型的反傳網絡。它通常具有三個不同的單元層（見圖55）。最底層是輸入層。下一層被稱作“隱單元”層，因為這些單元並不直接與網絡外部的世界連接。最頂層是輸出層。最底層的每個單元都與上一層的所有單元連接。中間層也是如此。網絡只有前向連接，而沒有側向連接，除了訓練以外也沒有反向的投射。它的結構幾乎不能被簡化。</p>

<p>訓練開始的時候，所有的權重都被隨機賦值，因而網絡最初對所有信號的反應是無意義的。此後給定一個訓練輸入，產生輸出並按反傳訓練規則調節權重。過程如下：在網絡對訓練產生輸出以後，告訴高層的每個單元它的輸出與“正確”輸出之間的差。單元利用該信息來對每個從低層單元達到它的突觸的權重進行小的調整。然後它將該信息反傳到隱層的每個單元。每個隱層單元則收集所有高層單元傳未的誤差信息，並以此調節來自最底層的所有突觸。</p>

<p>從整體上看具體的算法使得網絡總是不斷調節以減小誤差。這個過程被多次重復。（該算法是普適的，可以用於多於三層的前向網絡。）</p>

<p>經過了足夠數量的訓練之後網絡就可以使用了。此時有一個輸入的測試集來檢驗網絡。測試集是經過選擇的，它的一般（統計）特性與訓練集相似，但其他方面則不同。（權重在這個階段保持不變，以便考察訓練後網絡的行為。）如果結果不能令人滿意，設計者會從頭開始，修改網絡的結構、輸入和輸出的編碼方式、訓練規則中的參數或是訓練總數。</p>

<p>所有這些看上去顯得很抽象。舉個例子或許能讓讀者清楚一些。特裏·塞吉諾斯基和查爾斯·羅森堡（Charles Rosenberg）在1987年提供了一個著名的演示。他們把他們的網絡稱為網絡發音器（NETtalk）。它的任務是把書寫的英文轉化成英文發音。英文的拼法不規則,這使它成為一門發音特別困難的語言，因而這個任務並不那麽簡單易行。當然，事先並不把英語的發音規則清楚地告訴網絡。在訓練過程中，網絡每次嘗試後將得到修正信號，網絡則從中學習。輸入是通過一種特殊的方式一個字母接一個字母地傳到網絡中。NETtalk的全部輸出是與口頭發音相對應的一串符號，為了讓演示更生動，網絡的輸出與一個獨立的以前就有的機器（一種數字發音合成器）耦合。它能將NETtallk的輸出變為發音，這樣就可以聽到機器“朗讀”英語了。</p>

<p>由於一個英語字母的發音在很大程度上依賴於它前後的字母搭配，輸入層每次讀入一串7個字母。①輸出層中的單元與音素所要求的21個發音特征②相對應，還有5個單元處理音節分界和重音。圖56給出了它的一般結構。③</p>

<p>他們使用了兩段文字的摘錄來訓練網絡，每段文字都附有訓練機器所需的標音法。第一段文字摘自梅裏亞姆-韋伯斯特袖珍詞典。第二段摘錄則多少有些令人奇怪，是一個小孩的連續說話。初始權重具有小的隨機值，並在訓練期內每處理一個詞更新一次。他們編寫程序使得計算機能根據提供的輸入和（正確的）輸出信息自動地完成這一步。在對真實的輸出進行判斷時，程序會采納一個與真實發音最接近的音素作為最佳猜測，通常有好幾個“發音”輸出單元對此有關系。</p>

<p>聆聽機器學著“讀”英語是一件令人著迷的事情。①最初，由於初始連接是隨機的，只能聽到一串令人困惑的聲音。NETtalk很快就學會了區分元音和輔音。但開始時它只知道一個元音和一個輔音，因此像在咿呀學語。後來它能識別詞的邊界，並能發出像詞那樣的一串聲音。在對訓練集進行了大約十次操作之後，單詞變得清楚，讀的聲音也和幼兒說話很像了。</p>

<p>實際結果並不完美，在某種情況下英語發音依賴於詞意，而NETtalk對此一無所知。一些相似的發音通常引起混淆，如論文（thesis)和投擲（throw）的“th”音。把同一個小孩的另一段例文作為檢測，機器完成得很好，表明它能把從相當小的訓練集（1024個單詞）中學到的推廣到它從未遇到的新詞上。②這稱為“泛化”。</p>

<p>顯然網絡不僅僅是它所訓練過的每一個單詞的查詢表。它的泛化能力取決於英語發音的冗余度。並不是每一個英語單詞都按自己唯一的方式發音，雖然首次接觸英語的外國人容易這樣想。（這個問題是由於英語具有兩個起源造成的，即拉丁語系和日爾曼語系，這使得英語的詞匯十分豐富。）</p>

<p>相對於大多數從真實神經元上收集的資料而言，神經網絡的一個優點在於在訓練後很容易檢查它的每一個隱單元的感受野。一個字母僅會激發少數幾個隱單元，還是像全息圖那樣它的活動在許多隱單元中傳播呢？答案更接近於前者。雖然在每個字母一發音對應中並沒有特殊的隱單元，但是每個這種對應並不傳播到所有的隱單元。</p>

<p>因此便有可能檢查隱單元的行為如何成簇的（即具有相同的特性）。塞吉諾斯基和羅森堡發現“……最重要的區別是元音與輔音完全分離，然而在這兩類之中隱單元簇具有不同的模式，對於元音而言，下一個重要的變量是字母，而輔音成簇則按照了一種混合的策略，更多地依賴於它們聲音的相似性。”</p>

<p>這種相當雜亂的布置在神經網絡中是典型現象，其重要性在於它與許多真實皮層神經元（如視覺系統中的神經元）的反應驚人地相似，而與工程師強加給系統的那種巧妙的設計截然不同。</p>

<p>他們的結論是：
NETtalk是一個演示，是學習的許多方面的縮影。首先，網絡在開始時具有一些合理的“先天”的知識，體現為由實驗者選擇的輸入輸出的表達形式，但沒有關於英語的特別知識——網絡可以對任何具有相同的字母和音素集的語言進行訓練。其次，網絡通過學習獲得了它的能力，其間經歷了幾個不同的訓練階段，並達到了一種顯著的水平。最後，信息分布在網絡之中，因而沒有一個單元或連接是必不可少的，作為結果，網絡具有容錯能力，對增長的損害是故障弱化的。此外，網絡從損傷中恢復的速度比重新學習要快得多。</p>

<p>盡管這些與人類的學習和記憶很相似，但NETtalk過於簡單，還不能作為人類獲得閱讀能力的一個好的模型。網絡試圖用一個階段完成人類發育中兩個階段出現的過程，即首先是兒童學會說話；只有在單詞及其含義的表達已經建立好以後，他們才學習閱讀。同時，我們不僅具有使用字母-發音對應的能力，似乎還能達到整個單詞的發音表達，但在網絡中並沒有單詞水平的表達。註意到網絡上並沒有什麽地方清楚地表達英語的發音規則，這與標準的
計算機程序不同。它們內在地鑲嵌在習得的權重模式當中。這正是小孩學習語言的方式。它能正確他說話，但對它的腦所默認的規則一無所知。①</p>

<p>NETtalk有幾條特性是與生物學大為抵觸的。網絡的單元違背了一條規律，即一個神經元只能產生興奮性或抑制性輸出，而不會二者皆有。更為嚴重的是，照字面上說，反傳算法要求教師信息快速地沿傳遞向前的操作信息的同一個突觸發送回去。這在腦中是完全不可能發生的。試驗中用了獨立的回路來完成這一步，但對我而言它們顯得過於勉強，並不符合生物原型。</p>

<p>盡管有這些局限性，NETtalk展示了一個相對簡單的神經網絡所能完成的功能，給人印象非常深刻。別忘了那裏只有不足500個神經元和2萬個連接。如果包括（在前面的腳註中列出的）某些限制和忽略，這個數目將會大一些，但恐怕不會大10倍。而在每一側新皮層邊長大約四分之一毫米的一小塊表面（比針尖還小）有大約5000個神經元。因而與腦相比，NETtalk僅是極小的一部分。②所以它能學會這樣相對復雜的任務給人印象格外深刻。</p>

<p>另一個神經網絡是由西德尼·萊基（Sidney Lehky）和特裏·塞吉諾斯基設計的。他們的網絡所要解決的問題是在不知道光源方向的情況下試圖從某些物體的陰影中推斷出其三維形狀（第四章　描述的所謂從陰影到形狀問題）。對隱層單元的感受野進行檢查時發現了令人吃驚的結果。其中一些感受野與實驗中在腦視覺第一區（V1區）發現的一些神經元非常相似。它們總是成為邊緣檢測器或棒檢測器，但在訓練過程中，並未向網絡呈現過邊或棒，設計者也未強行規定感受野的形狀。它們的出現是訓練的結果。此外，當用一根棒來測試網絡時，其輸出層單元的反應類似於V1區具有端點抑制（end-stopping）的復雜細胞。</p>

<p>網絡和反傳算法二者都在多處與生物學違背，但這個例子提出了這樣一個回想起來應該很明顯的問題：僅僅從觀察腦中一個神經元的感受野並不能推斷出它的功能，正如第十一章描述的那樣，了解它的投射野，即它將軸突傳向哪些神經元，也同樣重要。</p>

<p>我們已經關註了神經網絡中“學習”的兩種極端情況：由赫布規則說明的無教師學習和反傳算法那樣的有教師學習。此外還有若幹種常見的類型。一種同樣重要的類型是“競爭學習”。①其基本思想是網絡操作中存在一種勝者為王機制，使得能夠最好地表達了輸入的含義的那個單元（或更實際他說是少數單元）抑制了其他所有單元。學習過程中，每一步中只修正與勝者密切相關的那些連接，而不是系統的全部連接。這通常用一個三層網絡進行模擬，如同標準的反傳網絡，但又有顯著差異，即它的中間層單元之間具有強的相互連接。這些連接的強度通常是固定的，並不改變。通常短程連接是興奮性的，而長程的則是抑制性的，一個單元傾向於與其近鄰友好而與遠處的相對抗。這種設置意味著中間層的神經元為整個網絡的活動而競爭。在一個精心設計的網絡中，在任何一次試驗中通常只有一個勝者。</p>

<p>這種網絡並沒有外部教師。網絡自己尋找最佳反應。這種學習算法使得只有勝者及其近鄰單元調節輸入權重。這種方式使得當前的那種特殊反應在將來出現可能性更大。由於學習算法自動將權重推向所要求的方向，每個隱單元將學會與一種特定種類的輸入相聯系。①</p>

<p>到此為止我們考慮的網絡處理的是靜態的輸入，並在一個時間間隔後產生一個靜態的輸出。很顯然在腦中有一些操作能表達一個時間序列，如口哨吹出一段曲調或理解一種語言並用之交談。人們初步設計了一些網絡來著手解決這個問題，但目前尚不深入。（NETtalk確實產生了一個時間序列，但這只是數據傳入和傳出網絡的一種方法，而不是它的一種特性。）</p>

<p>語言學家曾經強調，目前在語言處理方面（如句法規則）根據人工智能理論編寫的程序處理更為有效。其本質原因是網絡擅長於高度並行的處理，而這種語言學任務要求一定程度的序列式處理。腦中具有註意系統，它具有某種序列式的本性，對低層的並行處理進行操作，迄今為止神經網絡並未達到要求的這種序列處理的復雜程度，雖然它應當出現。</p>

<p>真實神經元（其軸突、突觸和樹突）都存在不可避免的時間延遲和處理過程中的不斷變化。神經網絡的大多數設計者認為這些特性很討厭，因而回避它們。這種態度也許是錯的。幾乎可以肯定進化就建立在這些改變和時間延遲上，並從中獲益。</p>

<p>對這些神經網絡的一種可能的批評是，由於它們使用這樣一種大體上說不真實的學習算法，事實上它們並不能揭示很多關於腦的情況。對此有兩種答案。一種是嘗試在生物學看來更容易接受的算法，另一種方法更有效且更具有普遍性。加利福尼亞州立大學聖叠戈分校的戴維·齊帕澤（David Zipser），一個由分子生物學家轉為神經理論學家，曾經指出，對於鑒別研究中的系統的本質而言，反傳算法是非常好的方法。他稱之為“神經系統的身份證明”。他的觀點是，如果一個網絡的結構至少近似於真實物體，並了解了系統足夠多的限制，那麽反傳算法作為一種最小化誤差的方法，通常能達到一個一般性質相似於真實生物系統的解。這樣便在朝著了解生物系統行為的正確方向上邁出了第一步。</p>

<p>如果神經元及其連接的結構還算逼真，並已有足夠的限制被加入到系統中，那麽產生的模型可能是有用的，它與現實情況足夠相似。這樣便允許仔細地研究模型各組成部分的行為。與在動物上做相同的實驗相比，這更加快速也更徹底。</p>

<p>我們必須明白科學目標並非到此為止，這很重要。例如，模型可能會顯示，在該模型中某一類突觸需要按反傳法確定的某種方式改變。但在真實系統中反傳法並不出現。因此模擬者必須為這一類突觸找到合適的真實的學習規則。例如，那些特定的突觸或許只需要某一種形式的赫布規則。這些現實性的學習規則可能是局部的，在模型的各個部分不盡相同。如果需要的話，可能會引入一些全局信號，然後必須重新運行該模型。</p>

<p>如果模型仍能工作，那麽實驗者必須表明這種學習方式確實在預測的地方出現，並揭示這種學習所包含的細胞和分子機制以支持這個觀點。只有如此我們才能從這些“有趣”的演示上升為真正科學的有說服力的結果。</p>

<p>所有這些意味著需要對大量的模型及其變體進行測試。幸運的是，隨著極高速而又廉價的計算機的發展，現在可以對許多模型進行模擬。這樣人們就可以檢測某種設置的實際行為是否與原先所希望的相同，但即便使用最先進的計算機也很難檢驗那些人們所希望的巨大而復雜的模型。</p>

<p>“堅持要求所有的模型應當經過模擬檢驗，這令人遺憾地帶來了兩個副產品。如果一個的假設模型的行為相當成功，其設計者很難相信它是不正確的。然而經驗告訴我們，若幹差異很大的模型也會產生相同的行為。為了證明這些模型哪個更接近於事實，看來還需要其他證據，諸如真實神經元及腦中該部分的分子的準確特性。</p>

<p>另一種危害是，對成功的模型過分強調會抑制對問題的更為自由的想像，從而會阻礙理論的產生。自然界是以一種特殊的方式運行的。對問題過於狹隘的討論會使人們由於某種特殊的困難而放棄極有價值的想法。但是進化或許使用了某些額外的小花招來回避這些困難。盡管有這些保留，模擬一個理論，即便僅僅為了體會一下它事實上如何工作，也是有用的。</p>

<p>我們對神經網絡能總結出些什麽呢？它們的基礎設計更像腦，而不是標準計算機的結構，然而，它們的單元並沒有真實神經元那樣復雜，大多數網絡的結構與新皮層的回路相比也過於簡單。目前，如果一個網絡要在普通計算機上在合理的時間內進行模擬，它的規模只能很小。隨著計算機變得越來越快，以及像網絡那樣高度並行的計算機的生產商業化，這會有所改善，但仍將一直是嚴重的障礙。</p>

<p>盡管神經網絡有這些局限性，它現在仍然顯示出了驚人的完成任務的能力。整個領域內充滿了新觀點。雖然其中許多網絡會被人們遺忘，但通過了解它們，抓住其局限性並設計改進它們的新方法，肯定會有堅實的發展。這些網絡有可能具有重要的商業應用。盡管有時它會導致理論家遠離生物事實，但最終會產生有用的觀點和發明。也許所有這些神經網絡方面的工作的最重要的結果是它提出了關於腦可能的工作方式的新觀點。</p>

<p>在過去，腦的許多方面看上去是完全不可理解的。得益於所有這些新的觀念，人們現在至少瞥見了將來按生物現實設計腦模型的可能性，而不是用一些毫無生物依據的模型僅僅去捕捉腦行為的某些有限方面。即便現在這些新觀念已經使我們對實驗的討論更為敏銳，我們現在更多地了解了關於個體神經元所必須掌握的知識。我們可以指出回路的哪些方面我們尚不足夠了解（如新皮層的向回的通路），我們從新的角度看待單個神經元的行為，並意識到在實驗日程上下一個重要的任務是它們整個群體的行為。神經網絡還有很長的路要走，但它們終於有了好的開端。<br/>
  ======================<br/>
①查爾斯·安德森（charles Anderson）和戴維·範·埃森提出腦中有些裝置將信息按規定路線從一處傳至另一處。不過這個觀點尚有爭議。<br/>
①該網絡以一個早期網絡為基礎。那個網絡被稱為“自旋玻璃”，是物理學家受一種理論概念的啟發而提出的。<br/>
①這對應於一個適定的數學函數（稱為“能量函數”，來自自旋玻璃）的（局域）極小值。霍普菲爾德還給出了一個確定權重的簡單規則以使網絡的每個特定的活動模式對應於能量函數的一個極小值。<br/>
①對於霍普菲爾德網絡而言，輸出可視為網絡存貯的記憶中與輸出（似為“輸入”之誤——譯者註）緊密相關的那些記憶的加權和。<br/>
①在1968年，克裏斯托夫·朗格特-希金斯（Christopher Longuet-Higgins）從全息圖出發發明了一種稱為“聲音全息記器”（holophone）的裝置。此後他又發明了另一種裝置稱為“相關圖”，並最終形成了一種特殊的神經網絡形式。他的學生戴維·威爾肖在完成博士論文期間對其進行了詳細的研究。<br/>
(2)他們和其他一些想法接近的理論家合作，在1981年完成了《聯想記憶的並行模式》，由傑弗裏·希爾頓（Geoffrey Hinton）和吉姆·安德森編著。這本書的讀者主要是神經網絡方面的工作者，它的影響並不像後一本書那樣廣泛。<br/>
（1)PDP即平行分布式處理（Parallel Distributed Processing）的縮寫。<br/>
①更準確他說是誤差的平方的平均值在下降，因此該規則有時又叫做最小均方（LMS）規則。<br/>
①29個“字母”各有一個相應的單元；這包括字母表中的26個字母，還有三個表示標點和邊界。因而輸入層需要29x7=203個單元。<br/>
②例如，因為輔音p和b發音時都是以攏起嘴唇開始的，所以都稱作“唇止音”。<br/>
③中間層（隱層）最初有80個隱單元，後來改為120個，結果能完成得更好。機器總共需要調節大約2萬個突觸。權重可正可負。他們並沒有構造一個真正的平行的網絡來做這件事，而是在一臺中型高速計算機上（一臺VAX 11//780FPA）模擬這個網絡。<br/>
①計算機的工作通常不夠快，不能實時地發音，因而需要先把輸出錄下來，再加速播放，這樣人們才能聽明白。<br/>
②塞吉諾斯基和羅森堡還表明，網絡對於他們設置的連接上的隨機損傷具有相當的抵抗力。在這種環境下它的行為是”故障弱化”。他們還試驗以11個字母（而不是7個字母）為一組輸入。這顯著改善了網絡的成績。加上第二個隱單元層並不能改善它的成績，但有助於網絡更好地進行泛化。<br/>
①除了上面列出的以外，NEttalk還有許多簡化。雖然作者們信奉分布式表達，在輸入輸出均有“祖母細胞”即，例如有一個單元代表“窗口中第三個位置上的字母a”。這樣做是為了降低計算所需要的時間，是一種合理的簡化形式。雖然數據順序傳入7個字母的方式在人工智能程序是完全可以接受的，卻顯得與生物事實相違背。輸出的“勝者為王”這一步並不是由“單元”完成的，也不存在一組單元去表達預計輸出與實際輸出之間的差異（即教師信號）。這些運算都是由程序執行的。<br/>
②這種比較不太公平，因為神經網絡的一個單元更好的考慮是等價於腦中一小群相神經元。因而更合適的數字大約是8萬個神經元（相當於一平方毫米皮層下神經元的數目）。<br/>
①它是由斯蒂芬·格羅斯伯格、托伊沃·科霍寧等人發展的。<br/>
①我不打算討論競爭網絡的局限性。顯然必須有足夠多的隱單元來容納網絡試圖從提供的輸入中所學的所有東西，訓練不能太快，也不能太慢，等等。這種網絡要正確工作需要仔細設計。毫無疑問，不久的將來會發明出基於競爭學習基本思想的更加復雜的應用。</p>

<p><a href="The%20Astonishing%20Hypothesis">《驚人的假說&mdash;靈魂的科學探索》Francis Crick 1994&#8217;</a> 湖南科學技術出版社出版</p>
</div>


<div class="meta">
	<div class="date">








  


<time datetime="2015-04-30T11:15:35+08:00" pubdate data-updated="true">Apr 30<span>th</span>, 2015</time></div>
	

<div class="tags">

	<a class='category' href='/blog/categories/artificial-intelligence/'>artificial-intelligence</a>

</div>


	
</div>
</article>
<!-- Copyright Info BEGIN -->


<b>
   <div class="entry-content"> <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" ></a>版權聲明：保持署名-非商用-非衍生
<br />
Attribution-NonCommercial-NoDerivs
<br />
 <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/3.0/deed.zh" >Creative Commons BY-NC-ND 3.0 </a>
</div>
</b>

<!-- Copyright Info END -->


	<span class='st_facebook_large' displayText='Facebook'></span>
<span class='st_twitter_large' displayText='Tweet'></span>
<span class='st_sina_large' displayText='Sina'></span>
<span class='st_evernote_large' displayText='Evernote'></span>
<span class='st_email_large' displayText='Email'></span>
<span class='st_sharethis_large' displayText='ShareThis'></span>

<script type="text/javascript">var switchTo5x=true;</script>
<script type="text/javascript" src="http://w.sharethis.com/button/buttons.js"></script>
<script type="text/javascript">stLight.options({publisher: "546620ac-cd31-4320-baca-f6d07850d74b", doNotHash: false, doNotCopy: false, hashAddressBar: false});</script>


</div>

    

<div align='center'>
    <form class="navbar-form" action="/search/">
        <input type="text" class="form-control" placeholder="Google Search" name="q">
    </form>
 </div>

	<footer id="footer" class="inner">Copyright &copy; 2015

    Themis_Sword

<br>
Powered by Octopress.
</footer>
	<script src="/javascripts/slash.js"></script>
<script src="/javascripts/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
	$('.fancybox').fancybox();
})(jQuery);
</script> <!-- Delete or comment this line to disable Fancybox -->




	<script type="text/javascript">
		var _gaq = _gaq || [];
		_gaq.push(['_setAccount', 'UA-48083272-1']);
		_gaq.push(['_trackPageview']);

		(function() {
			var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://' : 'http://') + 'stats.g.doubleclick.net/dc.js';
			var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
		})();
	</script>



</body>
</html>
