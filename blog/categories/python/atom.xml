<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: python, | Themis_Sword's Blog]]></title>
  <link href="http://www.aprilzephyr.com/blog/categories/python/atom.xml" rel="self"/>
  <link href="http://www.aprilzephyr.com/"/>
  <updated>2015-05-14T11:25:39+08:00</updated>
  <id>http://www.aprilzephyr.com/</id>
  <author>
    <name><![CDATA[Themis_Sword]]></name>
    <email><![CDATA[licong0419@outlook.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Python Tools for Machine Learning(FW)]]></title>
    <link href="http://www.aprilzephyr.com/blog/05142015/python-tools-for-machine-learning-fw/"/>
    <updated>2015-05-14T11:15:31+08:00</updated>
    <id>http://www.aprilzephyr.com/blog/05142015/python-tools-for-machine-learning-fw</id>
    <content type="html"><![CDATA[<p>Python is one of the best programming languages out there, with an extensive coverage in scientific computing: computer vision, artificial intelligence, mathematics, astronomy to name a few. Unsurprisingly, this holds true for machine learning as well.</p>

<p>Of course, it has some disadvantages too; one of which is that the tools and libraries for Python are scattered. If you are a unix-minded person, this works quite conveniently as every tool does one thing and does it well. However, this also requires you to know different libraries and tools, including their advantages and disadvantages, to be able to make a sound decision for the systems that you are building. Tools by themselves do not make a system or product better, but with the right tools we can work much more efficiently and be more productive. Therefore, knowing the right tools for your work domain is crucially important.<!--more--></p>

<p>This post aims to list and describe the most useful machine learning tools and libraries that are available for Python. To make this list, we did not require the library to be written in Python; it was sufficient for it to have a Python interface. We also have a small section on Deep Learning at the end as it has received a fair amount of attention recently.</p>

<p>We do not aim to list <strong>all</strong> the machine learning libraries available in Python (the Python package index returns 139 results for “machine learning”) but rather the ones that we found useful and well-maintained to the best of our knowledge. Moreover, although some of modules could be used for various machine learning tasks, we included libraries whose main focus is machine learning. For example, although <a href="http://docs.scipy.org/doc/scipy/reference/index.html">Scipy</a> has some <a href="http://docs.scipy.org/doc/scipy/reference/cluster.vq.html#module-scipy.cluster.vq">clustering algorithms</a>, the main focus of this module is not machine learning but rather in being a comprehensive set of tools for scientific computing. Therefore, we excluded libraries like Scipy from our list (though we use it too!).</p>

<p>Another thing worth mentioning is that we also evaluated the library based on how it integrates with other scientific computing libraries because machine learning (either supervised or unsupervised) is part of a data processing system. If the library that you are using does not fit with your rest of data processing system, then you may find yourself spending a tremendous amount of time to creating intermediate layers between different libraries. It is important to have a great library in your toolset but it is also important for that library to integrate well with other libraries.</p>

<p>If you are great in another language but want to use Python packages, we also briefly go into how you could integrate with Python to use the libraries listed in the post.</p>

<h3>Scikit-Learn</h3>

<p><a href="http://scikit-learn.org/stable/">Scikit Learn</a> is our machine learning tool of choice at CB Insights. We use it for classification, feature selection, feature extraction and clustering. What we like most about it is that it has a consistent API which is easy to use while also providing <strong>a lot of</strong> evaluation, diagnostic and cross-validation methods out of the box (sound familiar? Python has batteries-included approach as well). The icing on the cake is that it uses Scipy data structures under the hood and fits quite well with the rest of scientific computing in Python with Scipy, Numpy, Pandas and Matplotlib packages. Therefore, if you want to visualize the performance of your classifiers (say, using a precision-recall graph or Receiver Operating Characteristics (ROC) curve) those could be quickly visualized with help of Matplotlib. Considering how much time is spent on cleaning and structuring the data, this makes it very convenient to use the library as it tightly integrates to other scientific computing packages.</p>

<p>Moreover, it has also limited Natural Language Processing feature extraction capabilities as well such as bag of words, tfidf, preprocessing (stop-words, custom preprocessing, analyzer). Moreover, if you want to quickly perform different benchmarks on toy datasets, it has a datasets module which provides common and useful datasets. You could also build toy datasets from these datasets for your own purposes to see if your model performs well before applying the model to the real-world dataset. For parameter optimization and tuning, it also provides grid search and random search. These features could not be accomplished if it did not have great community support or if it was not well-maintained. We look forward to its first stable release.</p>

<h3>Statsmodels</h3>

<p><a href="http://statsmodels.sourceforge.net/">Statsmodels</a> is another great library which focuses on statistical models and is used mainly for predictive and exploratory analysis. If you want to fit linear models, do statistical analysis, maybe a bit of predictive modeling, then Statsmodels is a great fit. The statistical tests it provides are quite comprehensive and cover validation tasks for most of the cases. If you are R or S user, it also accepts R syntax for some of its statistical models. It also accepts Numpy arrays as well as Pandas data-frames for its models making creating intermediate data structures a thing of the past!</p>

<h3>PyMC</h3>

<p><a href="http://pymc-devs.github.io/pymc/">PyMC</a> is the tool of choice for <strong>Bayesians</strong>. It includes Bayesian models, statistical distributions and diagnostic tools for the convergence of models. It includes some hierarchical models as well. If you want to do Bayesian Analysis, you should check it out.</p>

<h3>Shogun</h3>

<p><a href="http://www.shogun-toolbox.org/page/home/">Shogun</a> is a machine learning toolbox with a focus on Support Vector Machines (SVM) that is written in C++. It is actively developed and maintained, provides a Python interface and the Python interface is mostly documented well. However, we’ve found its API hard to use compared to Scikit-learn. Also, it does not provide many diagnostics or evaluation algorithms out of the box. However, its speed is a great advantage.</p>

<h3>Gensim</h3>

<p><a href="http://radimrehurek.com/gensim/">Gensim</a> is defined as “topic modeling for humans”. As its homepage describes, its main focus is Latent Dirichlet Allocation (LDA) and its variants. Different from other packages, it has support for Natural Language Processing which makes it easier to combine NLP pipeline with other machine learning algorithms. If your domain is in NLP and you want to do clustering and basic classification, you may want to check it out. Recently, they introduced Recurrent Neural Network based text representation called word2vec from Google to their API as well. This library is written purely in Python.</p>

<h3>Orange</h3>

<p><a href="http://orange.biolab.si/">Orange</a> is the only library that has a Graphical User Interface (GUI) among the libraries listed in this post. It is also quite comprehensive in terms of classification, clustering and feature selection methods and has some cross-validation methods. It is better than Scikit-learn in some aspects (classification methods, some preprocessing capabilities) as well, but it does not fit well with the rest of the scientific computing ecosystem (Numpy, Scipy, Matplotlib, Pandas) as nicely as Scikit-learn.</p>

<p>Having a GUI is an important advantage over other libraries however. You could visualize cross-validation results, models and feature selection methods (you need to install Graphviz for some of the capabilities separately). Orange has its own data structures for most of the algorithms so you need to wrap the data into Orange-compatible data structures which makes the learning curve steeper.</p>

<h3>PyMVPA</h3>

<p><a href="http://www.pymvpa.org/index.html">PyMVPA</a> is another statistical learning library which is similar to Scikit-learn in terms of its API. It has cross-validation and diagnostic tools as well, but it is not as comprehensive as Scikit-learn.</p>

<h3>Deep Learning</h3>

<p>Even though deep learning is a subsection Machine Learning, we created a separate section for this field as it has received tremendous attention recently with various acqui-hires by Google and Facebook.</p>

<h4>Theano</h4>

<p><a href="http://deeplearning.net/software/theano/">Theano</a> is the most mature of deep learning library. It provides nice data structures (tensors) to represent layers of neural networks and they are efficient in terms of linear algebra similar to Numpy arrays. One caution is that, its API may not be very intuitive, which increases learning curve for users. There are a lot of libraries which build on top of Theano exploiting its data structures. It has support for GPU programming out of the box as well.</p>

<h4>PyLearn2</h4>

<p>There is another library built on top of Theano, called <a href="http://deeplearning.net/software/pylearn2/">PyLearn2</a> which brings modularity and configurability to Theano where you could create your neural network through different configuration files so that it would be easier to experiment different parameters. Arguably, it provides more modularity by separating the parameters and properties of neural network to the configuration file.</p>

<h4>Decaf</h4>

<p><a href="http://caffe.berkeleyvision.org/">Decaf</a> is a recently released deep learning library from UC Berkeley which has state of art neural network implementations which are tested on the Imagenet classification competition.</p>

<h4>Nolearn</h4>

<p>If you want to use excellent Scikit-learn library api in deep learning as well, <a href="http://packages.python.org/nolearn/">Nolearn</a> wraps Decaf to make the life easier for you. It is a wrapper on top of Decaf and it is compatible(mostly) with Scikit-learn, which makes Decaf even more awesome.</p>

<h4>OverFeat</h4>

<p><a href="https://github.com/sermanet/OverFeat">OverFeat</a> is a recent winner of <a href="https://plus.google.com/+PierreSermanet/posts/GxZHEH9ynoj">Dogs vs Cats (kaggle competition)</a> which is written in C++ but it comes with a Python wrapper as well(along with Matlab and Lua). It uses GPU through Torch library so it is quite fast. It also won the detection and localization competition in ImageNet classification. If your main domain is in computer vision, you may want to check it out.</p>

<h4>Hebel</h4>

<p><a href="https://github.com/hannes-brt/hebel">Hebel</a> is another neural network library comes along with GPU support out of the box. You could determine the properties of your neural networks through YAML files(similar to Pylearn2) which provides a nice way to separate your neural network from the code and quickly run your models. Since it has been recently developed, documentation is lacking in terms of depth and breadth. It is also limited in terms of neural network models as it only has one type of neural network model(feed-forward). However, it is written in pure Python and it will be nice library as it has a lot of utility functions such as schedulers and monitors which we did not see any library provides such functionalities.</p>

<h4>Neurolab</h4>

<p><a href="https://code.google.com/p/neurolab/">NeuroLab</a> is another neural network library which has nice api(similar to Matlab’s api if you are familiar) It has different variants of Recurrent Neural Network(RNN) implementation unlike other libraries. If you want to use RNN, this library might be one of the best choice with its simple API.</p>

<h3>Integration with other languages</h3>

<p>You do not know any Python but great in another language? Do not despair! One of the strengths of Python (among many other) is that it is a perfect glue language that you could use your tool of choice programming language with these libraries through access from Python. Following packages for respective programming languages could be used to combine Python with other programming languages:</p>

<ul>
<li>R &ndash;> <a href="http://rpython.r-forge.r-project.org/">RPython</a></li>
<li>Matlab &ndash;> <a href="http://algoholic.eu/matpy/">matpython</a></li>
<li>Java &ndash;> <a href="http://www.jython.org/jythonbook/en/1.0/JythonAndJavaIntegration.html">Jython</a></li>
<li>Lua &ndash;> <a href="http://labix.org/lunatic-python">Lunatic Python</a></li>
<li>Julia &ndash;> <a href="https://github.com/stevengj/PyCall.jl">PyCall.jl</a></li>
</ul>


<h3>Inactive Libraries</h3>

<p>These are the libraries that did not release any updates for more than one year, we are listing them because some may find it useful, but it is unlikely that these libraries will be maintained for bug fixes and especially enhancements in the future:</p>

<ul>
<li><a href="https://github.com/mdp-toolkit/mdp-toolkit">MDP</a></li>
<li><a href="http://mlpy.sourceforge.net/docs/3.5/">MlPy</a></li>
<li><a href="http://ffnet.sourceforge.net/">FFnet</a></li>
<li><a href="http://pybrain.org/">PyBrain</a></li>
</ul>


<p>If we are missing one of your favorite packages in Python for machine learning, feel free to let us know in the comments. We will gladly add that library to our blog post as well.</p>

<p><a href="https://www.cbinsights.com/blog/python-tools-machine-learning/">Origin</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python數據分析入門(轉)]]></title>
    <link href="http://www.aprilzephyr.com/blog/05142015/pythonshu-ju-fen-xi-ru-men-zhuan/"/>
    <updated>2015-05-14T10:46:30+08:00</updated>
    <id>http://www.aprilzephyr.com/blog/05142015/pythonshu-ju-fen-xi-ru-men-zhuan</id>
    <content type="html"><![CDATA[<p>最近，<a href="http://alstatr.blogspot.com/">Analysis with Programming</a>加入了<a href="http://planetpython.org/">Planet Python</a>。作為該網站的首批特約博客，我這裏來分享一下如何通過Python來開始數據分析。具體內容如下：</p>

<ol>
<li>數據導入<br/>
** 導入本地的或者web端的CSV文件；</li>
<li>數據變換；</li>
<li>數據統計描述；</li>
<li>假設檢驗<br/>
** 單樣本t檢驗；</li>
<li>可視化；</li>
<li>創建自定義函數。<!--more--></li>
</ol>


<h3>數據導入</h3>

<p>這是很關鍵的一步，為了後續的分析我們首先需要導入數據。通常來說，數據是CSV格式，就算不是，至少也可以轉換成CSV格式。在Python中，我們的操作如下：</p>

<pre><code>import pandas as pd

# Reading data locally
df = pd.read_csv('/Users/al-ahmadgaidasaad/Documents/d.csv')

# Reading data from web
data_url = "https://raw.githubusercontent.com/alstat/Analysis-with-Programming/master/2014/Python/Numerical-Descriptions-of-the-Data/data.csv"
df = pd.read_csv(data_url)
</code></pre>

<p>為了讀取本地CSV文件，我們需要pandas這個數據分析庫中的相應模塊。其中的read_csv函數能夠讀取本地和web數據。</p>

<h3>數據變換</h3>

<p>既然在工作空間有了數據，接下來就是數據變換。統計學家和科學家們通常會在這一步移除分析中的非必要數據。我們先看看數據：</p>

<pre><code># Head of the data
print df.head()

# OUTPUT
    Abra  Apayao  Benguet  Ifugao  Kalinga
0   1243    2934      148    3300    10553
1   4158    9235     4287    8063    35257
2   1787    1922     1955    1074     4544
3  17152   14501     3536   19607    31687
4   1266    2385     2530    3315     8520

# Tail of the data
print df.tail()

# OUTPUT
 Abra  Apayao  Benguet  Ifugao  Kalinga
74   2505   20878     3519   19737    16513
75  60303   40065     7062   19422    61808
76   6311    6756     3561   15910    23349
77  13345   38902     2583   11096    68663
78   2623   18264     3745   16787    16900
</code></pre>

<p>對R語言程序員來說，上述操作等價於通過print(head(df))來打印數據的前6行，以及通過print(tail(df))來打印數據的後6行。當然Python中，默認打印是5行，而R則是6行。因此R的代碼head(df, n = 10)，在Python中就是df.head(n = 10)，打印數據尾部也是同樣道理。</p>

<p>在R語言中，數據列和行的名字通過colnames和rownames來分別進行提取。在Python中，我們則使用columns和index屬性來提取，如下：</p>

<pre><code># Extracting column names
print df.columns

# OUTPUT
Index([u'Abra', u'Apayao', u'Benguet', u'Ifugao', u'Kalinga'], dtype='object')

# Extracting row names or the index
print df.index

# OUTPUT
Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78], dtype='int64')
</code></pre>

<p>數據轉置使用T方法，</p>

<pre><code># Transpose data
print df.T

# OUTPUT
            0      1     2      3     4      5     6      7     8      9  
Abra      1243   4158  1787  17152  1266   5576   927  21540  1039   5424  
Apayao    2934   9235  1922  14501  2385   7452  1099  17038  1382  10588  
Benguet    148   4287  1955   3536  2530    771  2796   2463  2592   1064  
Ifugao    3300   8063  1074  19607  3315  13134  5134  14226  6842  13828  
Kalinga  10553  35257  4544  31687  8520  28252  3106  36238  4973  40140  

         ...       69     70     71     72     73     74     75     76     77 
Abra     ...    12763   2470  59094   6209  13316   2505  60303   6311  13345  
Apayao   ...    37625  19532  35126   6335  38613  20878  40065   6756  38902  
Benguet  ...     2354   4045   5987   3530   2585   3519   7062   3561   2583  
Ifugao   ...     9838  17125  18940  15560   7746  19737  19422  15910  11096  
Kalinga  ...    65782  15279  52437  24385  66148  16513  61808  23349  68663  

            78 
Abra      2623 
Apayao   18264 
Benguet   3745 
Ifugao   16787 
Kalinga  16900 

Other transformations such as sort can be done using &lt;code&gt;sort&lt;/code&gt; attribute. Now let's extract a specific column. In Python, we do it using either &lt;code&gt;iloc&lt;/code&gt; or &lt;code&gt;ix&lt;/code&gt; attributes, but &lt;code&gt;ix&lt;/code&gt; is more robust and thus I prefer it. Assuming we want the head of the first column of the data, we have
</code></pre>

<p>其他變換，例如排序就是用sort屬性。現在我們提取特定的某列數據。Python中，可以使用iloc或者ix屬性。但是我更喜歡用ix，因為它更穩定一些。假設我們需數據第一列的前5行，我們有：</p>

<pre><code>print df.ix[:, 0].head()

# OUTPUT
0     1243
1     4158
2     1787
3    17152
4     1266
Name: Abra, dtype: int64
</code></pre>

<p>順便提一下，Python的索引是從0開始而非1。為了取出從11到20行的前3列數據，我們有：</p>

<pre><code>print df.ix[10:20, 0:3]

# OUTPUT
    Abra  Apayao  Benguet
10    981    1311     2560
11  27366   15093     3039
12   1100    1701     2382
13   7212   11001     1088
14   1048    1427     2847
15  25679   15661     2942
16   1055    2191     2119
17   5437    6461      734
18   1029    1183     2302
19  23710   12222     2598
20   1091    2343     2654
</code></pre>

<p>上述命令相當於df.ix[10:20, [&lsquo;Abra&rsquo;, &lsquo;Apayao&rsquo;, &lsquo;Benguet&rsquo;]]。</p>

<p>為了舍棄數據中的列，這裏是列1(Apayao)和列2(Benguet)，我們使用drop屬性，如下：</p>

<pre><code>print df.drop(df.columns[[1, 2]], axis = 1).head()

# OUTPUT
    Abra  Ifugao  Kalinga
0   1243    3300    10553
1   4158    8063    35257
2   1787    1074     4544
3  17152   19607    31687
4   1266    3315     8520
</code></pre>

<p>axis 參數告訴函數到底舍棄列還是行。如果axis等於0，那麽就舍棄行。</p>

<h3>統計描述</h3>

<p>下一步就是通過describe屬性，對數據的統計特性進行描述：</p>

<pre><code>print df.describe()

# OUTPUT
               Abra        Apayao      Benguet        Ifugao       Kalinga
count     79.000000     79.000000    79.000000     79.000000     79.000000
mean   12874.379747  16860.645570  3237.392405  12414.620253  30446.417722
std    16746.466945  15448.153794  1588.536429   5034.282019  22245.707692
min      927.000000    401.000000   148.000000   1074.000000   2346.000000
25%     1524.000000   3435.500000  2328.000000   8205.000000   8601.500000
50%     5790.000000  10588.000000  3202.000000  13044.000000  24494.000000
75%    13330.500000  33289.000000  3918.500000  16099.500000  52510.500000
max    60303.000000  54625.000000  8813.000000  21031.000000  68663.000000
</code></pre>

<h3>假設檢驗</h3>

<p>Python有一個很好的統計推斷包。那就是scipy裏面的stats。ttest_1samp實現了單樣本t檢驗。因此，如果我們想檢驗數據Abra列的稻谷產量均值，通過零假設，這裏我們假定總體稻谷產量均值為15000，我們有：</p>

<pre><code>from scipy import stats as ss

# Perform one sample t-test using 1500 as the true mean
print ss.ttest_1samp(a = df.ix[:, 'Abra'], popmean = 15000)

# OUTPUT
(-1.1281738488299586, 0.26270472069109496)
</code></pre>

<p>返回下述值組成的元祖：<br/>
* t : 浮點或數組類型<br/>
t統計量<br/>
* prob : 浮點或數組類型<br/>
two-tailed p-value 雙側概率值</p>

<p>通過上面的輸出，看到p值是0.267遠大於α等於0.05，因此沒有充分的證據說平均稻谷產量不是150000。將這個檢驗應用到所有的變量，同樣假設均值為15000，我們有：</p>

<pre><code>print ss.ttest_1samp(a = df, popmean = 15000)

# OUTPUT
(array([ -1.12817385,   1.07053437, -65.81425599,  -4.564575  ,   6.17156198]),
 array([  2.62704721e-01,   2.87680340e-01,   4.15643528e-70,
          1.83764399e-05,   2.82461897e-08]))
</code></pre>

<p>第一個數組是t統計量，第二個數組則是相應的p值。</p>

<h3>可視化</h3>

<p>Python中有許多可視化模塊，最流行的當屬matpalotlib庫。稍加提及，我們也可選擇bokeh和seaborn模塊。之前的博文中，我已經說明了matplotlib庫中的盒須圖模塊功能。<br/>
<img src="/images/pda/1.jpg"></p>

<pre><code># Import the module for plotting
import matplotlib.pyplot as plt
 plt.show(df.plot(kind = 'box'))
</code></pre>

<p>現在，我們可以用pandas模塊中集成R的ggplot主題來美化圖表。要使用ggplot，我們只需要在上述代碼中多加一行，</p>

<pre><code>import matplotlib.pyplot as plt
pd.options.display.mpl_style = 'default' # Sets the plotting display theme to ggplot2
df.plot(kind = 'box')
</code></pre>

<p>這樣我們就得到如下圖表：
<img src="/images/pda/2.jpg"></p>

<p>比matplotlib.pyplot主題簡潔太多。但是在本博文中，我更願意引入seaborn模塊，該模塊是一個統計數據可視化庫。因此我們有：<br/>
<img src="/images/pda/3.jpg"></p>

<pre><code># Import the seaborn library
import seaborn as sns
 # Do the boxplot
plt.show(sns.boxplot(df, widths = 0.5, color = "pastel"))
</code></pre>

<p>多性感的盒式圖，繼續往下看。<br/>
<img src="/images/pda/4.jpg"></p>

<pre><code>plt.show(sns.violinplot(df, widths = 0.5, color = "pastel"))
</code></pre>

<p><img src="/images/pda/5.jpg"></p>

<pre><code>plt.show(sns.distplot(df.ix[:,2], rug = True, bins = 15))
</code></pre>

<p><img src="/images/pda/6.jpg"></p>

<pre><code>with sns.axes_style("white"):
    plt.show(sns.jointplot(df.ix[:,1], df.ix[:,2], kind = "kde"))
</code></pre>

<p><img src="/images/pda/7.jpg"></p>

<pre><code>plt.show(sns.lmplot("Benguet", "Ifugao", df))
</code></pre>

<h3>創建自定義函數</h3>

<p>在Python中，我們使用def函數來實現一個自定義函數。例如，如果我們要定義一個兩數相加的函數，如下即可：</p>

<pre><code>def add_2int(x, y):
    return x + y

print add_2int(2, 2)

# OUTPUT
4
</code></pre>

<p>順便說一下，Python中的縮進是很重要的。通過縮進來定義函數作用域，就像在R語言中使用大括號{…}一樣。這有一個我們之前博文的例子：</p>

<ol>
<li>產生10個正態分布樣本，其中u=3和σ<sup>2</sup>=5</li>
<li>基於95%的置信度，計算x̄和<img src="http://latex.codecogs.com/gif.latex?\bar{x}\mp&space;z_{a/}(2\frac{\sigma&space;}{\sqrt{n}})" title="\bar{x}\mp z_{a/}(2\frac{\sigma }{\sqrt{n}})" />;</li>
<li>重復100次; 然後</li>
<li>計算出置信區間包含真實均值的百分比</li>
</ol>


<p>Python中，程序如下：</p>

<pre><code>import numpy as np
import scipy.stats as ss

def case(n = 10, mu = 3, sigma = np.sqrt(5), p = 0.025, rep = 100):
    m = np.zeros((rep, 4))

    for i in range(rep):
        norm = np.random.normal(loc = mu, scale = sigma, size = n)
        xbar = np.mean(norm)
        low = xbar - ss.norm.ppf(q = 1 - p) * (sigma / np.sqrt(n))
        up = xbar + ss.norm.ppf(q = 1 - p) * (sigma / np.sqrt(n))

        if (mu &gt; low) &amp; (mu &lt; up):
            rem = 1
        else:
            rem = 0

        m[i, :] = [xbar, low, up, rem]

    inside = np.sum(m[:, 3])
    per = inside / rep
    desc = "There are " + str(inside) + " confidence intervals that contain "
           "the true mean (" + str(mu) + "), that is " + str(per) + " percent of the total CIs"

    return {"Matrix": m, "Decision": desc}
</code></pre>

<p>上述代碼讀起來很簡單，但是循環的時候就很慢了。下面針對上述代碼進行了改進，這多虧了 Python專家，看我上篇博文的<a href="http://alstatr.blogspot.com/2014/01/python-and-r-is-python-really-faster.html#disqus_thread">15條意見</a>吧。</p>

<pre><code>import numpy as np
import scipy.stats as ss

def case2(n = 10, mu = 3, sigma = np.sqrt(5), p = 0.025, rep = 100):
    scaled_crit = ss.norm.ppf(q = 1 - p) * (sigma / np.sqrt(n))
    norm = np.random.normal(loc = mu, scale = sigma, size = (rep, n))

    xbar = norm.mean(1)
    low = xbar - scaled_crit
    up = xbar + scaled_crit

    rem = (mu &gt; low) &amp; (mu &lt; up)
    m = np.c_[xbar, low, up, rem]

    inside = np.sum(m[:, 3])
    per = inside / rep
    desc = "There are " + str(inside) + " confidence intervals that contain "
           "the true mean (" + str(mu) + "), that is " + str(per) + " percent of the total CIs"
    return {"Matrix": m, "Decision": desc}
</code></pre>

<h3>更新</h3>

<p>那些對於本文ipython notebook版本感興趣的，請點擊<a href="http://nuttenscl.be/Python_Getting_Started_with_Data_Analysis.html">這裏</a>。這篇文章由<a href="https://twitter.com/NuttensC">Nuttens Claude</a>負責轉換成 ipython notebook 。</p>

<p>關於作者: <a href="http://python.jobbole.com/author/dengcy/">Den</a></p>

<p><a href="http://alstatr.blogspot.ca/2015/02/python-getting-started-with-data.html">Origin</a><br/>
<a href="http://python.jobbole.com/81133/">中文譯文</a></p>
]]></content>
  </entry>
  
</feed>
