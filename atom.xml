<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Themis_Sword's Blog]]></title>
  <link href="http://www.aprilzephyr.com/atom.xml" rel="self"/>
  <link href="http://www.aprilzephyr.com/"/>
  <updated>2015-05-12T17:52:14+08:00</updated>
  <id>http://www.aprilzephyr.com/</id>
  <author>
    <name><![CDATA[Themis_Sword]]></name>
    <email><![CDATA[licong0419@outlook.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Excerpt_Artificial Intelligence: A Modern Approach 3rd Edition(Russell &amp; Norvig)]]></title>
    <link href="http://www.aprilzephyr.com/blog/05122015/excerpt-artificial-intelligence-a-modern-approach-3rd-edition-russell-and-norvig/"/>
    <updated>2015-05-12T15:08:13+08:00</updated>
    <id>http://www.aprilzephyr.com/blog/05122015/excerpt-artificial-intelligence-a-modern-approach-3rd-edition-russell-and-norvig</id>
    <content type="html"><![CDATA[<h3>1. Introduction</h3>

<ul>
<li>Different people approach Al with different goals in mind, Two important questions to ask are: Are you concerned with thinking or behavior? Do you want to model humans or work from an ideal standard?</br></li>
<li>In this book, we adopt the view that intelligence is concerned mainly with rational action. Ideally, an intelligent agent takes the best possible action in a situation. We study the problem of building agents that are intelligent in this sense.</br></li>
<li>Philosophers (going back to 400 B.C.) made AI conceivable by considering the ideas that the mind is in some ways like a machine, that it operates on knowledge encoded in some internal language, and that thought can be used to choose what actions to take.</br></li>
<li>Mathematicians provided the tools to manipulate statements of logical certainty as well as uncertain, probabilistic statements. They also set the groundwork for understanding computation and reasoning about algorithms.</br></li>
<li>Economists formalized the problem of making decisions that maximize the expected outcome to the decision maker.<!--more--></br></li>
<li>Neuroscientists discovered some facts about how the brain works and the ways in which it is similar to and different from computers.</br></li>
<li>Psychologists adopted the idea that humans and animals can be considered information- processing machines. Linguists showed that language use fits into this model.</br></li>
<li>Computer engineers provided the ever-more-powerful machines that make AI applications possible.</br></li>
<li>Control theory deals with designing devices that act optimally on the basis of feedback from the environment. Initially, the mathematical tools of control theory were quite different from AI, but the fields are coming closer together.</br></li>
<li>The history of Al has had cycles of success, misplaced optimism. and resulting cutbacks in enthusiasm and funding. There have also been cycles of introducing new creative approaches and systematically refining the best ones.</br></li>
<li>AI has advanced more rapidly in the past decade because of greater use of the scientific method in experimenting with and comparing approaches.</br></li>
<li>Recent progress in understanding the theoretical basis for intelligence has gone hand in hand with improvements in the capabilities of real systems. The subfields of AI have become more integrated, and AI has found common ground with other disciplines.</br></li>
</ul>


<h3>2. Intelligent Agents</h3>

<ul>
<li>An agent is something that perceives and acts in an environment. The agent ftmction for an avail. specifics the action taken by the agent in response to any percept sequence.</li>
<li>The performance measure evaluates the behavior of the agent in an environment A rational agent acts so as to maximize the expected value of the performance measure, given the percept sequence it has seen so far.</li>
<li>A task environment specification includes the performance measure, the external environment, the actuators. and the sensors. In designing an agent, the first step must always be to specify the task environment as fully as possible.</li>
<li>Task environments vary along several significant dimensions. They can be fully or partially observable, single-agent or multiagent, deterministic or stochastic, episodic or sequential, static or dynamic, discrete or continuous, and known or unknown.</li>
<li>The agent program implements the agent function. There exists a variety of basic agent-program designs reflecting the kind of information made explicit and used in the decision process. The designs vary in efficiency, compactness, and flexibility. The appropriate design of the agent program depends on the nature of the environment.</li>
<li>Simple reflex agents respond directly to percepts, whereas model-based reflex agents maintain internal state to track aspects of the world that are not evident in the current percept. Goal-based agents act to achieve their goals, and utility-based agents try to maximize their own expected &ldquo;happiness.&rdquo;</li>
<li>All agents can improve their performance through learning.</li>
</ul>


<h3>3. Solving Problems by Searching</h3>

<ul>
<li>Before an agent can start searching for solutions, a goal must be identified and a well-defined problem must be formulated.</li>
<li>A problem consists of five parts: the initial state, a set of actions, a transition model describing the results of those actions, a goal test function, and a path cost function. The environment of the problem is represented by a state space. A path through the state space from the initial state to a goal state is asolution.</li>
<li>Search algorithms treat states and actions as atomic: they do not consider any internal structure they might possess.</li>
<li>A general TREE-SEARCH algorithm considers all possible paths to find a solution, whereas a GRAPH-SEARCH algorithm avoids consideration of redundant paths.</li>
<li>Search algorithms are judged on the basis of <strong>completeness, optimality, time complexity</strong>, and space complexity. Complexity depends on h, the branching factor in the state space, and d, the depth of the shallowest solution.</li>
<li>Uninformed search methods have access only to the problem definition. The basic algorithms are as follows:</br>*<em> <strong>Breadth-first</strong> search expands the shallowest nodes first; it is complete, optimal for unit step costs. but has exponential space complexity.</br>*</em> <strong>Uniform-cost</strong> search expands the node with lowest path cast, g(n), and is optimal for general step costs.</br>*<em> <strong>Depth-first</strong> search expands the deepest unexpanded node first. It is neither complete nor optimal, but has linear space complexity. Depth limited search adds a depth bound.</br>*</em> <strong>Iterative deepening</strong> search calls depth-first search with increasing depth limits until a goal is found. It is complete, optimal for unit step costs, has time complexity comparable to breadth-first search, and has linear space complexity.</br>** <strong>Bidirectional</strong> search can enormously reduce time complexity, but it is not always applicable and may require too much space.</br></li>
<li>Informed search methods may have access to a <strong>heuristic</strong> function h(n) that estimates the cost of a solution from n.  *<em> The generic <strong>best-first</strong> search algorithm selects a node for expansion according to an evaluation function.</br>*</em> <strong>Greedy best-first</strong> search expands nudes with minimal h(n). It is not optimal but is often efficient.</br>
*<em> <strong>A*</strong> search expands nodes with minimal f(n) = g(n) + h(n). A</em> is complete and optimal, provided that h(n) is admissible (for TREE-SEARCH) or consistent (for <strong>GRAPH-SEARCH</strong>). The space complexity of A<em> is still prohibitive.</br>*</em> <strong>RBFS</strong> (recursive best-first search) and <strong>SMA*</strong> (simplified memory-bounded A*) are robust, optimal search algorithms that use limited amounts of memory; given enough time, they can solve problems that A* cannot solve because it runs out of memory.</li>
<li>The performance of heuristic search algorithms depends on the quality of the heuristic function. One can sometimes construct good heuristics by relaxing the problem definition, by storing precomputed solution costs for subproblems in a pattern database, or by learning from experience with the problem class.</li>
</ul>


<h3>4. Beyond Classical Search</h3>

<ul>
<li>Local search methods such as <strong>hill climbing</strong> operate on complete-state formulations, keeping only a small number of nodes in memory. Several stochastic algorithms have been developed, including <strong>simulated annealing</strong>, which returns optimal solutions when given an appropriate cooling schedule.</li>
<li>Many local search methods apply also to problems in continuous spaces. <strong>Linear programming</strong> and <strong>convex optimization</strong> problem; obey certain restrictions on the shape of the state space and the nature of the objective function, and admit polynomial-time algorithms that are often extremely efficient in practice.</li>
<li>A <strong>genetic algorithm</strong> is a stochastic hill-climbing search in which a large population of states is maintained. New states are generated by mutation and by crossover, which combines pairs of states from the population.</li>
<li>In nondeterministic environments, agents can apply <strong>AND—OR</strong> search to generate Contingent plans that reach the goal regardless of which outcomcs occur during execution.</li>
<li>When the environment is partially observable, the <strong>belief state</strong> represents the set ofpossible states that the agent might be in.</li>
<li><strong>Standard search</strong> algorithms can be applied directly to belief-state space to solve <strong>sensorless problems</strong>, and belief-state AND—OR search can solve general partially observable problems. Incremental algorithms that construct solutions state by state within a belief state are often more efficient.</li>
<li><strong>Exploration problems</strong> arise when the agent has no idea about the states and actions of its environment. For safely explorable environments, <strong>online search</strong> agents can build a map and find a goal if one exists. Updating heuristic estimates from experience provides an effective method to escape from local minima.</li>
</ul>


<h3>5. Adversarial Search</h3>

<ul>
<li>A game can be defined by the initial state (how the board is set up), the legal actions in each state, the result of each action, a terminal test (which says when the game is over), and a utility function that applies to terminal states.</li>
<li>In two-player zero-sum games with perfect information, the minimax algorithm can select optimal moves by a depth-first enumeration of the game tree.</li>
<li>The alpha—beta search algorithm computes the same optimal move as minimax, but achieves much greater efficiency by eliminating subtrees that are provably irrelevant.</li>
<li>Usually, it is not feasible to consider the whole game tree (even with alpha—beta), so we need to cut the search off at some point and apply a heuristic <strong>evaluation function</strong> that estimates the utility of a state.</li>
<li>Many game programs precompute tables of best moves in the opening and endgame so that they can look up a move rather than search.</li>
<li>Games of chance can be handled by an extension to the minimax algorithm that evaluates a <strong>chance node</strong> by taking the average utility of all its children, weighted by the probability of each child.</li>
<li>Optimal play in games of <strong>imperfect information</strong>, such as Kriegspiel and bridge, requires reasoning about the current and future <strong>belief states</strong> of each player. A simple approximation can be obtained by averaging the value of an action over each possible configuration of missing information.</li>
<li>Programs have bested even champion human players at games such as chess, checkers, and Othello. Humans retain the edge in several games of imperfect information, such as poker, bridge, and Kriegspiel, and in games with very large branching factors and little good heuristic knowledge, such as Go.</li>
</ul>


<h3>6. Constraint Satisfaction Probllems</h3>

<ul>
<li><strong>Constraint satisfaction problems (CSPs)</strong> represent a state with a set of variable/value pairs and represent the conditions for a solution by a set of constraints on the variables. Many important real-world problems can be described as CSPs.</li>
<li>A number of inference techniques use the constraints to infer winch variable/value pairs are consistent and which are not. These include node, arc, path, and k-consistency.</li>
<li><strong>Backtracking search</strong>, a form of depth-first search, is commonly used for solving CSPs. Inference can be interwoven with search.</li>
<li>The <strong>minimtun-remaining-values</strong> and <strong>degree</strong> heuristics are domain-independent methods for deciding which variable to choose next in a backtracking search. The <strong>least-constraining-value</strong> heuristic helps in deciding which value to try first for a given variable. Backtracking occurs when no legal assignment can be found for a variable. <strong>Conflict-directed backjumping</strong> backtracks directly to the source of the problem.</li>
<li>Local search using the <strong>min-conflicts</strong> heuristic has also been applied to constraint satisfaction problems with great success.</li>
<li>The complexity of solving a CSP is strongly related to the structure of its constraint graph. Tree-structured problems can be solved in linear time. <strong>Cutset conditioning</strong> can reduce a general CSP to a tree-structured one and is quite efficient if a small cutset can be found. <strong>Tree decomposition</strong> techniques transform the CSP into a tree of subproblems and are efficient if the <strong>tree width</strong> of the constraint graph is small.</li>
</ul>


<h3>7. Logical Agents</h3>

<ul>
<li>Intelligent agents need knowledge about the world in order to reach good decisions.</li>
<li>Knowledge is contained in agents in the form of sentences in a <strong>knowledge representation language</strong> that are stored in a <strong>knowledge base</strong>.</li>
<li>A knowledge-based agent is composed of a knowledge base and an inference mechanism. It operates by storing sentences about the world in its knowledge base, using the inference mechanism to infer new sentences, and using these sentences to decide what action to take.</li>
<li>A representation language is defined by its <strong>syntax</strong>, which specifies the structure of sentences, and its <strong>semantics</strong>, which defines the truth of each sentence in each <strong>possible world</strong> or <strong>model</strong>.</li>
<li>The relationship of <strong>entailment</strong> between sentences is crucial to our understanding of reasoning. A sentence α entails another sentence β if it is true in all worlds where is is true. Equivalent definitions include the <strong>validity of</strong> the sentence α = β and the <strong>unsatisfiability</strong> of the sentence it α ¬ β.</li>
<li>Inference is the process of deriving new sentences from old ones. Sound inference algorithms derive only sentences that are entailed; complete algorithms derive all sentences that are entailed.</li>
<li><strong>Propositional logic</strong> is a simple language consisting of <strong>proposition symbols</strong> and <strong>logical connectives</strong>. It can handle propositions that are known true, known false, or completely unknown.</li>
<li>The set of possible models, given a fixed propositional vocabulary, is finite, so entailment can be checked by enumerating models. Efficient <strong>model-checking</strong> inference algorithms for propositional logic include backtracking and local search methods and can often solve large problems quickly.</li>
<li>Inference rules are patterns of sound inference that can be used to find proofs. The resolution rule yields a complete inference algorithm for knowledge bases that arc expressed in <strong>conjunctive normal form</strong>. <strong>Forward chaining</strong> and <strong>backward chaining</strong> are very natural reasoning algorithms for knowledge bases in <strong>Horn form</strong>.</li>
<li><strong>Local search</strong> methods such as WALKSAT can be used to find solutions. Such algo- rithms are sound but not complete.</li>
<li>Logical state estimation involves maintaining a logical sentence that describes the set of possible states consistent with the observation history. Each update step requires inference using the transition model of the environment, which is built from <strong>successor-state axioms</strong> that specify how each fluent changes.</li>
<li>Decisions within a logical agent can be made by SAT solving: finding possible models specifying future action sequences that reach the goal. This approach works only for fully observable or sensorless environments.</li>
<li>Propositional logic does not scale to environments of unbounded size because it lacks the expressive power to deal concisely with time, space, and universal patterns of relationships among objects.</li>
</ul>


<h3>8. First-Order Logic</h3>

<ul>
<li>Knowledge representation languages should be declarative, compositional, expressive,context independent, and unambiguous.</li>
<li>Logics differ in their <strong>ontological commitments</strong> and <strong>epistemological commitments</strong>. While propositional logic commits only to the existence of facts, firsi-order logic commits to the existence of objects and relations and thereby gains expressive power.</li>
<li>The syntax of first-order logic builds on that of propositional logic. It adds terms to represent objects, and has universal and existential quantifiers to construct assertions about all or some of the possible values of the quantified variables.</li>
<li>A possible world, or model, for first-order logic includes a set of objects and an <strong>interpretation</strong> that maps constant symbols to objects, predicate symbols to relations among objects, and function symbols to functions on objects.</li>
<li>An atomic sentence is true just when the relation named by the predicate holds between the objects named by the terms. <strong>Extended interpretations</strong>, which map quantifier variables to objects in the model, define the truth of quantified sentences.</li>
<li>Developing a knowledge base in first-order logic requires a careful process of analyzing the domain, choosing a vocabulary, and encoding the axioms required to support the desired inferences.</li>
</ul>


<h3>9. Inference in First-Order Logic</h3>

<ul>
<li>A first approach uses inference rules (universal instantiation and existential instan- tiation) to <strong>propositionalize</strong> the inference problem. Typically, this approach is slow, unless the domain is small.</li>
<li>The use of unification to identify appropriate substitutions for variables eliminates the instantiation step in first-order proofs, making the process more efficient in many cases.</li>
<li>A lifted version of Modus Ponens uses unification to provide a natural and powerful inference nile, generalized <strong>Modus Ponens</strong>. The <strong>forward-chaining</strong> and <strong>backward-chaining</strong> algorithms apply this rule to sets of definite clauses.</li>
<li>Generalized Modus Ponens is complete for definite clauses, although the entailment problem is <strong>semidecidable</strong>. For Datalog knowledge bases consisting of function-free definite clauses, entailment is decidable.</li>
<li>Forward chaining is used in deductive databases, where it can be combined with relational database operations. It is also used in production systems, which perform efficient updates with very large rule sets Forward chaining is complete for Datalog and runs in polynomial time.</li>
<li>Backward chaining is used in logic programming systems, which employ sophisticated compiler technology to provide very fast inference. Backward chaining suffers from redundant inferences and infinite loops; these can be alleviated by memoization.</li>
<li>Prolog, unlike first-order logic, uses a closed world with the unique names assumption and negation as failure. These make Prolog a more practical programming language, but bring it further from pure logic.</li>
<li>The generalized resolution inference rule provides a complete proof system for first-order logic, using knowledge bases in conjunctive normal form.</li>
<li>Several strategies exist for reducing the search space of a resolution system without compromising completeness. One of the most important issues is dealing with equality; we showed how <strong>demodulation</strong> and <strong>paramodulation</strong> can be used.</li>
<li>Efficient resolution-based theorem provers have been used to prove interesting mathematical theorems and to verify and synthesize software and hardware.</li>
</ul>


<h3>10. Classical Planning</h3>

<ul>
<li>Planning systems are problem-solving algorithms that operate on explicit propositional or relational representations of states and actions. These representations make possible the derivation of effective heuristics and the development of powerful and flexible algorithms for solving problems.</li>
<li>PDDL, the Planning Domain Definition Language, describes the initial and goal states as conjunctions of literals, and actions in terms of their preconditions and effects.</li>
<li>State-space search can operate in the forward direction (progression) or the backward direction (regression), Effective heuristics can be derived by subgoal independence assumptions and by various relaxations of the planning problem.</li>
<li>A planning graph can be constructed incrementally, starting from the initial state, Each layer contains a superset of all the literals or actions that could occur at that time step and encodes mutual exclusion (mutex) relations among literals or actions that cannot cooccur Planning graphs yield useful heuristics for state-space and partial-order planners and can be used directly in the GRAPHPLAN algorithm.</li>
<li>Other approaches include first-order deduction over situation calculus axioms; encoding a planning problem as a Boolean satisfiability problem or as a constraint satisfaction problem; and explicitly searching through the space of partially ordered plans.</li>
<li>Each of the major approaches to planning has its adherents, and there is as yet no con- sensus on which is best. Competition and cross-fertilization among the approaches have resulted in significant gains in efficiency for planning systems.</li>
</ul>


<h3>11. Planning and Acting in the Real World</h3>

<ul>
<li>Many actions consume <strong>resources</strong>, such as money, gas, or raw materials. It is convenient to treat these resources as numeric measures in a pool rather than try to reason about. say, each individual coin and bill in the world. Actions can generate and consume resources, and it is usually cheap and effective to check partial plans for satisfaction of resource constraints before attempting further refinements.</li>
<li>Time is one of the most important resources. It can be handled by specialized scheduling algorithms, or scheduling can be integrated with planning.</li>
<li><strong>Hierarchical task network (HTN)</strong> planning allows the agent to take advice from the domain designer in the form of high-level actions (HLAs) that can be implemented in various ways by lower-level action sequences. The effects of HLAs can be defined with <strong>angelic semantics</strong>, allowing provably correct high-level plans to be derived without consideration of lower-level implementations. HTN methods can create the very large plans required by many real-world applications.</li>
<li>Standard planning algorithms assume complete and correct information and deterministic, fully observable environments. Many domains violate this assumption.</li>
<li><strong>Contingent plans</strong> allow the agent to sense the world during execution to dccidc what branch of the plan to follow, hi some cases, sensorless or <strong>conformant planning</strong> can be used to construct a plan that works without the need for perception. Both conformant and contingent plans can be constructed by search in the space of <strong>belief states</strong>. Efficient representation or computation of belief states is a key problem.</li>
<li>An <strong>online planning agent</strong> uses execution monitoring and splices in repairs as needed to recover from unexpected situations, which can be due to nondeterministic actions, exogenous events, or incorrect models of the environment.</li>
<li><strong>Multiagent planning</strong> is necessary when there are other agents in the environment with which to cooperate or compete. Joint plans can be constructed, but must be augmented with some form of coordination if two agents are to agree on which joint plan to execute.</li>
<li>This chapter extends classic planning to cover nondeterministic environments (where outcomes of actions are uncertain), but it is not the last word on planning. Chapter 17 describes techniques for stochastic environments (in which outcomes of actions have probabilities associated with them): Markov decision processes, partially observable Markov decision processes, and game theory. In Chapter 21 we show that reinforcement learning allows an agent to learn how to behave from past successes and failures.</li>
</ul>


<h3>12. Knowledge Representation</h3>

<ul>
<li>Large-scale knowledge representation requites a general-purpose ontology to organize and tie together the various specific domains of knowledge.</li>
<li>A general-purpose ontology needs to cover a wide variety of knowledge and should be capable, in principle, of handling any domain.</li>
<li>Building a large, general-purpose ontology is a significant challenge that has yet to befully realized, although current frameworks seem to be quite robust.</li>
<li>We presented an <strong>upper ontology based on categories and the event calculus</strong>. We covered categories, subcategories, parts, structured objects, measurements, substances, events, time and space, change, and beliefs.</li>
<li>Natural kinds cannot be defined completely in logic, but properties of natural kinds can be represented.</li>
<li>Actions, events, and time can be represented either in situation calculus or in more expressive representations such as event calculus. Such representations enable an agentto construct plans by logical inference.</li>
<li>We presented a detailed analysis of the Internet shopping domain, exercising the generalontology and showing how the domain knowledge can be used by a shopping agent.</li>
<li>Special-purpose representation systems, such as <strong>semantic networks</strong> and <strong>description logics</strong>, have been devised to help in organizing a hierarchy of categories. <strong>Inheritance</strong> is an important form of inference, allowing the properties of objects to be deduced from their membership in categories.</li>
<li><strong>The closed-world assumption</strong>, as implemented in logic programs, provides a simple way to avoid having to specify lots of negative information. It is best interpreted as a default that can be overridden by additional information.</li>
<li><strong>Nonmonotonic logics</strong>, such as <strong>circumscription and default logic, are intended to capture</strong> default reasoning in general.</li>
<li><strong>Truth maintenance systems handle knowledge updates</strong> and revisions efficiently.</li>
</ul>


<h3>13. Quantifying Uncertainty</h3>

<ul>
<li>Uncertainty arises because of both laziness and ignorance. It is inescapable in complex,nondeterministic, or partially observable environments.</li>
<li>Probabilities express the agent&rsquo;s inability to reach a definite decision regarding the truth of a sentence. Probabilities summarize the agent&rsquo;s beliefs relative to the evidence.</li>
<li>Decision theory combines the agent&rsquo;s beliefs and desires, defining the best action as the one that maximizes expected utility.</li>
<li>Basic probability statements include prior probabilities and conditional probabilities over simple and complex propositions.</li>
<li>The axioms of probability constrain the possible assignments of probabilities to propositions. An agent that violates the axioms must behave irrationally in some cases.</li>
<li>The <strong>full joint probability distribution</strong> specifies the probability of each complete assignment of values to random variables. It is usually too large to create or use in its explicit form, but when it is available it can be used to answer queries simply by adding up entries for the possible worlds corresponding to the query propositions.</li>
<li><strong>Absolute independence</strong> between subsets of random variables allows the full joint distribution to be factored into smaller joint distributions, greatly reducing its complexity. Absolute independence seldom occurs in practice.</li>
<li><strong>Bayes&#8217; rule</strong> allows unknown probabilities to be computed from known conditional probabilities, usually in the causal direction. Applying Bayes&#8217; rule with many pieces of evidence runs into the same scaling problems as does the full joint distribution.</li>
<li>Conditional independence brought about by direct causal relationships in the domain might allow the full joint distribution to be factored into smaller, conditional distributions. The naive Bayes model assumes the conditional independence of all effect variables, given a single cause variable, and grows linearly with the number of effects.</li>
<li>A wumpus-world agent can calculate probabilities for unobserved aspects of the world, thereby improving on the decisions of a purely logical agent. Conditional independence makes these calculations tractable.</li>
</ul>


<h3>14. Probabilistic Reasoning</h3>

<ul>
<li>A Bayesian network is a directed acyclic graph whose nodes correspond to random variables; each node has a conditional distribution for the node, given its parents.</li>
<li>Bayesian networks provide a concise way to represent <strong>conditional independence relationships in the domain</strong>.</li>
<li>A Bayesian network specifies a full joint distribution; each joint entry is defined as the product of the corresponding entries in the local conditional distributions. A Bayesian network is often exponentially smaller than an explicitly enumerated joint distribution.</li>
<li>Many conditional distributions can be represented compactly by canonical families of distributions. <strong>Hybrid Bayesian networks</strong>, which include both discrete and continuous variables, use a variety of canonical distributions.</li>
<li>Inference in Bayesian networks means computing the probability distribution of a set of query variables, given a set of evidence variables. Exact inference algorithms, such as <strong>variable elimination</strong>, evaluate sums of products of conditional probabilities as efficiently as possible.</li>
<li>In <strong>polytrees</strong> (singly connected networks), exact inference takes time linear in the size of the network. In the general case, the problem is intractable.</li>
<li>Stochastic approximation techniques such as <strong>likelihood weighting* and </strong>Markov chainMonte Carlo** can give reasonable estimates of the true posterior probabilities in a network and can cope with much larger networks than can exact algorithms.</li>
<li>Probability theory can be combined with representational ideas from first-order logic to produce very powerful systems for reasoning under uncertainty, <strong>Relational probability models (RPMs)</strong> include representational restrictions that guarantee a well-defined probability distribution that can be expressed as an equivalent Bayesian network. <strong>Open universe probability models</strong> handle <strong>existence</strong> and <strong>identity uncertainty</strong>, defining probabilty distributions over the infinite space of first-order possible worlds.</li>
<li>Various alternative systems for reasoning under uncertainty have been suggested. Generally speaking, truth-functional systems are not well suited for such reasoning.</li>
</ul>


<h3>15. Probabilistic Reasoning over Time</h3>

<ul>
<li>The changing state of the world is handled by using a set of random variables to represent the state at each point in time.</li>
<li>Representations can be designed to satisfy the Markov property, so that the future is independent of the past given the present. Combined with the assumption that the process is stationary -— that is, the dynamics do not change over time &mdash; this greatly simplifies the representation.</li>
<li>A temporal probability model can he thought of as containing a transition model describing the state evolution and a sensor model describing the observation process.</li>
<li>The principal inference tasks in temporal models are filtering, prediction, smoothing, and computing the most likely explanation. Each of these can be achieved using simple, recursive algorithms whose rim time is linear in the length of the sequence.</li>
<li>Three families of temporal models were studied in more depth: hidden <strong>Markov models</strong>, Kalman filters, and dynamic Bayesian networks (which include the other two as special cases).</li>
<li>Unless special assumptions are made, as in Kalman filters, exact inference with many stare variables is intractahle. In practice, the particle filtering algorithm seems to he an effective approximation algorithm.</li>
<li>When trying to keep track of many objects, uncertainty arises as to which observations belong to which objects &mdash; the data association problem. The number of association hypotheses is typically intractably large, but MCMC and particle filtering algorithms for data association work well in practice.</li>
</ul>


<h3>16. Making Simple Decisions</h3>

<ul>
<li>Probability theory describes what an agent should believe on the basis of evidence, utility theory describes what an agent wants, and decision theory puts the two together to describe what an agent should do.</li>
<li>We can use decision theory to build a system that makes decisions by considering all possible actions and choosing the one that leads to the best expected outcome. Such a system is known as a rational agent.</li>
<li>Utility theory shows that an agent whose preferences between lotteries are consistent with a set of simple axioms can be described as possessing a utility function; further-more, the agent selects actions as if maximizing its expected utility.</li>
<li><strong>Multiattribute</strong> utility theory deals with utilities that depend on several distinct attributes of states. <strong>Stochastic dominance</strong> is a particularly useful technique for making unambiguous decisions, even without precise utility values for attributes.</li>
<li><strong>Decision networks</strong> provide a simple formalism for expressing and solving decision problems. They are a natural extension of Bayesian networks, containing decision and utility nodes in addition to chance nodes.</li>
<li>Sometimes, salving a problem involves finding more information before making a decision. The <strong>value of information</strong> is defined as the expected improvement in utility compared with making a decision without the information.</li>
<li>Expert systems that incorporate utility information have additional capabilities compared with pure inference systems. In addition to being able to make decisions, they can use the value of information to decide which questions to ask, if any; they can recommend contingency plans; and they can calculate the sensitivity of their decisions to small changes in probability and utility assessments.</li>
</ul>


<h3>17. Making Complex Decisions</h3>

<ul>
<li>Sequential decision problems in uncertain environments, also called <strong>Markov decision</strong> processes, or <strong>MDPs</strong>, are defined by a <strong>transition model</strong> specifying the probabilistic outcomes of actions and a <strong>reward function</strong> specifying the reward in each state.</li>
<li>The utility of a state sequence is the sum of all the rewards over the sequence, possibly discounted over time. The solution of an MDP is a policy that associates a decision with every stale that the agent might reach. An optimal policy maximizes the utility of the state sequences encountered when it is executed.</li>
<li>The utility of a state is the expected utility of the state sequences encountered when an optimal policy is executed, starting in that state. <strong>The value iteration</strong> algorithm for solving MDPs works by iteratively solving the equations relating the utility of each state to those of its neighbors.</li>
<li><strong>Policy iteration</strong> alternates between calculating the utilities of states under the current policy and improving the current policy with respect to the current utilities.</li>
<li>Partially observable MDPs, or POMDPs, are much more difficult to solve than are MDPs. They can be solved by conversion to an <strong>MOP</strong> in the continuous space of belief states; both value iteration and policy iteration algorithms have been devised. Optimal behavior in POMDPs includes information gathering to reduce uncertainty and therefore make better decisions in the future.</li>
<li>A decision-theoretic agent can be constructed for <strong>POMDP</strong> environments. The agent uses a <strong>dynamic decision network</strong> to represent the transition and sensor models, to update its belief state, and to project forward possible action sequences.</li>
<li><strong>Game theory</strong> describes rational behavior for agents in situations in which multiple agents interact simultaneously. Solutions of games are Nash equilibria &mdash; strategy profiles in which no agent has an incentive to deviate from the specified strategy.</li>
<li><strong>Mechanism design</strong> can be used to set the rules by which agents will interact, in order to maximize some global utility through the operation of individually rational agents. Sometimes, mechanisms exist that achieve this goal without requiring each agent to consider the choices made by other agents.</li>
</ul>


<h3>18. Learning From Examples</h3>

<ul>
<li>Learning takes many forms, depending on the nature of the agent, the component to be improved, and the available feedback.</li>
<li>If the available feedback provides the correct answer for example inputs, then the learning problem is called supervised learning. The task is to learn a function y = h(x). Learning a discrete-valued function is called classification; learning a continuous function is called regression.</li>
<li>Inductive learning involves finding a hypothesis that agrees well with the examples. <strong>Ockham&rsquo;s razor</strong> suggests choosing the simplest consistent hypothesis. The difficulty of this task depends on the chosen representation.</li>
<li>Decision trees can represent all Boolean fractions. The information-gain heuristic provides an efficient method for finding a simple, consistent decision tree.</li>
<li>The performance of at learning algorithm is measured by the learning curve, which shows the prediction accuracy on the test set as a function of the <strong>training-set</strong> size.</li>
<li>When there are multiple models to choose from, <strong>cross-validation</strong> can be used to select a model that will generalize well.</li>
<li>Sometimes not all errors are equal. A <strong>loss function</strong> tells us how bad each error is; the goal is then to minimize loss over a validation set.</li>
<li><strong>Computational learning theory</strong> analyzes the sample complexity and computational complexity of inductive learning. There is a tradeoff between the expressiveness of the hypothesis language and the ease of learning.</li>
<li><strong>Linear regression</strong> is a widely used model. The optimal parameters of a linear regression model can he found by gradient descent search, or computed exactly.</li>
<li>A linear classifier with a hard threshold &mdash; also known as a <strong>perceptron</strong> &mdash; can be trained by a simple weight update rule to fit data that are <strong>linearly separable</strong>. In other cases, the rule fails to converge.</li>
</ul>


<h3>19. Knowledge in Learning</h3>

<ul>
<li>The use of prior knowledge in learning leads to a picture of <strong>cumulative learning</strong>, in which learning agents improve their learning ability as they acquire more knowledge.</li>
<li>Prior knowledge helps learning by eliminating otherwise consistent hypotheses and by &ldquo;filling in&rdquo; the explanation of examples, thereby allowing for shorter hypotheses. These contributions often result in faster teaming from fewer examples.</li>
<li>Understanding the different logical roles played by prior knowledge, as expressed by entailment constraints, helps to define a variety of learning techniques.</li>
<li>Explanation-based learning (EBL) extracts general rules from single examples by explaining the examples and generalizing the explanation. It provides a deductive method for turning first-principles knowledge into useful, efficient, special purpose expertise.</li>
<li>Relevance-based learning (RBL) uses prior knowledge in the form of determinations to identify the relevant attributes, thereby generating a reduced hypothesis space and speeding up learning. RBL also allows deductive generalizations from single examples.</li>
<li>Knowledge-based inductive learning (KBIL) finds inductive hypotheses that explain sets of observations with the help of background knowledge.</li>
<li>Inductive logic programming (ILP) techniques perform KBIL on knowledge that is expressed in first-order logic. ILP methods can learn relational knowledge that is not expressible in attribute-based systems,</li>
<li>1LP can be done with a top-down approach of refining a very general rule or through a bottom-up approach of inverting the deductive process.</li>
<li>1LP methods naturally generate new predicates with which concise new theories can be expressed and show promise as general-purpose scientific theory formation systems.</li>
</ul>


<h3>20. Learning Probabilistic Models</h3>

<ul>
<li><strong>Bayesian learning</strong> methods formulate learning as a form of probabilistic inference, using the observations to update a prior distribution over hypotheses. This approach provides a good way to implement Ockham&rsquo;s razor, but quickly becomes intractable for complex hypothesis spaces.</li>
<li><strong>Maximum a posteriori (MAP)</strong> learning selects a single most likely hypothesis given the data. The hypothesis prior is still used and the method is often more tractable than full Bayesian learning.</li>
<li><strong>Maximum-likelihood learning</strong> simply selects the hypothesis that maximizes the likelihood of the data; it is equivalent to MAP learning with a uniform prior. In simple cases such as linear regression and fully observable Bayesian networks, maximum-likelihood solutions can be found easily in closed form. <strong>Naive Bayes learning</strong> is a particularly effective technique that scales well.</li>
<li>When some variables are hidden, local maximum likelihood solutions can be found using the EM algorithm. Applications include clustering using mixtures of Gaussians, learning Bayesian networks, and learning hidden Markov models.</li>
<li>Learning the structure of Bayesian networks is an example of <strong>model selection</strong>. This usually involves a discrete search in the space of structures. Some method is required for trading off model complexity against degree of fit.</li>
<li><strong>Nonparametric models</strong> represent a distribution using the collection of data points. Thus, the number of parameters grows with the training set. Nearest-neighbors methods look at the examples nearest to the point in question, whereas <strong>kernel methods</strong> form a distance-weighted combination of all the examples.</li>
</ul>


<h3>21. Reinforcement Learning</h3>

<ul>
<li>The overall agent design dictates the kind of information that must be learned. The three main designs we covered were the <strong>model-based</strong> design, using a model P and a utility function U; the model-free design, using an action-utility function Q; and the reflex design, using a policy r.</li>
<li>Utilities can be learned using three approaches:</br>*<em> <strong>Direct utility estimation</strong> uses the total observed reward-to-go for a given state as direct evidence for learning its utility.</br>
*</em> <strong>Adaptive dynamic programming (ADP)</strong> learns a model and a reward function from observations and then uses value or policy iteration to obtain the utilities or an optimal policy. ADP makes optimal use of the local constraints on utilities of states imposed through the neighborhood structure of the environment.</br>
** Temporal-difference (TD) methods update utility estimates to match those of successor states. They can be viewed as simple approximations to the ADP approach that can learn without requiring a transition model. Using a learned model to generate pseudoexperiences can, however, result in faster learning.</li>
<li>Action-utility functions, or Q-functions, can be learned by an ADP approach or a TD approach. With TD, Q-learning requires no model in either the learning or action-selection phase. This simplifies the learning problem but potentially restricts the ability to learn in complex environments, because the agent cannot simulate the results of possible courses of action.</li>
<li>When the learning agent is responsible for selecting actions while it learns, it must trade off the estimated value of those actions against the potential for learning useful new information. An exact solution of the exploration problem is infeasible, but some simple heuristics do a reasonable job.</li>
<li>In large state spaces, reinforcement learning algorithms must use an approximate functional representation in order to generalize over states. The temporal-difference signal can be used directly to update parameters in representations such as neural networks.</li>
<li>Policy-search methods operate directly on a representation of the policy, attempting to improve it based on observed performance. The variation in the performance in a stochastic domain is a serious problem; for simulated domains this can be overcome by fixing the randomness in advance.</li>
</ul>


<h3>22. Natural Language Processing</h3>

<ul>
<li>Probabilistic language models based on n-grams recover a surprising amount of information about as language. They can perform well on such diverse tasks as language identification, spelling correction, genre classification, and named-entity recognition.</li>
<li>These language models can have millions of features, so feature selection and preprocessing of the data to reduce noise is important.</li>
<li><strong>Text classification</strong> can be done with naive Bayes n-gram models or with any of the classification algorithms we have previously discussed. Classification can also be seen as a problem in data compression.</li>
<li><strong>Information retrieval</strong> systems use a very simple language model based on bags of words, yct still manage to perform well in tcrms of <strong>recall</strong> and precision on very large corpora of text. On Web corpora, link-analysis algorithms improve performance.</li>
<li><strong>Question answering</strong> can be handled by an approach based on information retrieval, for questions that have multiple answers in the corpus. When more answers are available in the corpus, we can use techniques that emphasize precision rather than recall.</li>
<li><strong>Information-extraction</strong> systems use a more complex model that includes limited notions of syntax and semantics in the form of templates. They can be built from finite-state automata, HMMs, or conditional random fields, and can be learned from examples.</li>
<li>In building a statistical language system, it is best to devise a model that can make good use of available <strong>data</strong>, even if the model seems overly simplistic.</li>
</ul>


<h3>23. Natural Language for Communication</h3>

<ul>
<li>Formal language theory and <strong>phrase structure</strong> grammars (and in particular, context. free grammar) are useful tools for dealing with some aspects of natural language. The probabilistic context-free grammar (PCFG) formalism is widely used.</li>
<li>Sentences in a context-free language can be parsed in O(n<sup>3</sup>) time by a <strong>chart parser</strong> such as the CYK algorithm, which requires grammar rules to be in <strong>Chomsky Normal Form</strong>.</li>
<li>A treebank can be used to learn a grammar. It is also possible to learn a grammar from an unparsed corpus of sentences, but this is less successful.</li>
<li>A <strong>lexicalized PCFG</strong> allows us to represent that some relationships between words are mare common than others.</li>
<li>It is convenient to augment a grammar to handle such problems as subject–verb agreement and pronoun case. Definite clause grammar (DCG) is a formalism that allows for augmentations. With DCG, parsing and semantic interpretation (and even generation) can be done using logical inference.</li>
<li>Semantic interpretation can also be handled by an augmented grammar.</li>
<li><strong>Ambiguity</strong> is a very important problem in natural language understanding; most sentences have many possible interpretations, but usually only one is appropriate. Disam-biguation relies on knowledge about the world, about the current situation, and aboutlanguage use.</li>
<li><strong>Machine translation</strong> systems have been implemented using a range of techniques, from full syntactic and semantic analysis to statistical techniques based on phrase frequencies. Currently the statistical models are most popular and most successful.</li>
<li><strong>Speech recognition</strong> systems are also primarily based on statistical principles. Speechsystems are popular and useful, albeit imperfect.
Together, machine translation and speech recognition are two of the big successes of natural language technology. One reason that the models perform well is that large corpora are available—both translation and speech are tasks that are performed in the wild&#8221; by people every day. In contrast, tasks like parsing sentences have been less successful, in part because no large corpora of parsed sentences are available in the wild&#8221; and in part because parsing is not useful in and of itself.</li>
</ul>


<h3>24. Perception</h3>

<ul>
<li>The process of image formation is well understood in its geometric and physical aspects. Given a description of a three-dimensional scene, we can easily produce a picture of it from some arbitrary camera position (the graphics problem). Inverting the process by going from an image to a description of the scene is more difficult.</li>
<li>To extract the visual information necessary for the tasks of manipulation ; navigation, and recognition, intermediate representations have to be constructed. Early vision image-processing algorithms extract primitive features from the image, such as edges and regions.</li>
<li>There are various cues in the image that enable one to obtain three-dimensional in- formation about the scene: motion, stereopsis, texture, shading, and contour analysis. Each of these cues relies en background assumptions about physical scenes to provide nearly unambiguous interpretations.</li>
<li>Object recognition in its full generality is a very hard problem. We discussed brightness-based and feature-based approaches. We also presented a simple algorithm for pose estimation. Other possibilities exist.</li>
</ul>


<h3>25. Robotics</h3>

<ul>
<li>Robots are equipped with sensors fur perceiving their environment and effectors with which they can assert physical forces on their environment. Most robots are either manipulators anchored at fixed locations or mobile robots that can move.</li>
<li>Robotic perception concerns itself with estimating decision-relevant quantities from sensor data. To do so, we need an internal representation and a method for updating this intemal representation over time. Common examples of hard perceptual problems include <strong>localization, mapping, and object recognition</strong>.</li>
<li><strong>Probabilistic filtering algorithms</strong> such as Kalman filters and particle filters arc useful for robot perception. These techniques maintain the belief state, a posterior distribution over state variables.</li>
<li>The planning of robot motion is usually done in <strong>configuration space</strong>, where each point specifies the location and orientation of the robot and its joint angles.</li>
<li>Configuration space search algorithms include <strong>cell decomposition</strong> techniques, which decompose the space of all configurations into finitely many cells, and <strong>skeletonization</strong> techniques, which project configuration spaces into lower-dimensional manifolds. The motion planning problem is then solved using search in these simpler structures.</li>
<li>A path found by a search algorithm can be executed by using the path as the reference trajectory for a <strong>PID controller</strong>. Controllers are necessary in robotics to accommodate small perturbations; path planning alone is usually insufficient.</li>
<li><strong>Potential field</strong> techniques navigate robots by potential functions, defined over the distance to obstacles and the goal location. Potential field techniques may get stuck in local minima but they can generate motion directly without the need for path planning.</li>
<li>Sometimes it is easier to specify a robot controller directly, rather than deriving a path from an explicit model of the environment. Such controllers can often be written as simple <strong>finite state machines</strong>.</li>
</ul>


<h3>26. Philosophical Foundations</h3>

<ul>
<li>Philosophers use the term weak AI for the hypothesis that machines could possibly behave intelligently. and strong AI for the hypothesis that such machines would count as having actual minds (as opposed to simulated minds).</li>
<li>Alan Turing rejected the question &ldquo;Can machines think&rdquo; and replaced it with a behavioral test. He anticipated many objections to the possibility of thinking machines. Few AI researchers pay attention to the Turing Test, preferring to concentrate on their systems&#8217; performance on practical tasks, rather than the ability to imitate humans.</li>
<li>There is general agreement in modem times that mental states are brain states.</li>
<li>Arguments for and against strong AI are inconclusive. Few mainstream AI researchers believe that anything significant hinges on the outcome of the debate.</li>
<li>Consciousness remains a mystery.</li>
<li>We identified six potential threats to society posed by AI and related technology. We concluded that some of the threats are either unlikely or differ little from threats posed by &ldquo;unintelligent&rdquo; technologies. One threat in particular is worthy of further consideration: that ultraintelligent machines might lead to a future that is very different from today &mdash; we may not like it, and at that paint we may not have a choice. Such considerations lead inevitably to the conclusion that we must weigh carefully, and soon, the possible consequences of AI research.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Excerpt_Machine Learning(Tom Mitchell)]]></title>
    <link href="http://www.aprilzephyr.com/blog/05122015/excerpt-machine-learning-tom-mitchell/"/>
    <updated>2015-05-12T10:40:37+08:00</updated>
    <id>http://www.aprilzephyr.com/blog/05122015/excerpt-machine-learning-tom-mitchell</id>
    <content type="html"><![CDATA[<h3>1. Introduction</h3>

<ul>
<li>Machine learning addresses the question of how to build computer programs that improve their performance at some task through experience.</li>
<li>Machine learning algorithms have proven to be of great value in a variety of application domains. They are especially useful in (a) data mining problems where large databases may contain valuable implicit regularities that can be discovered automatically (e.g., to analyze outcomes of medical treatments from patient databases or to learn general rules for credit worthiness from finalcial databases); (b) poorly understood domains where humans might not have the knowledge needed to develop effective algorithms (e.g., human face recognition from images); and &copy; domains where the program must dynamically adapt to changing conditions (e.g., controlling manufacturing processes under changing supply stocks or adapting to the changing reading interests of individuals).</li>
<li>Machine learning draws on idea from a diverse set of disciplines, including artifical intelligence, probability and statistics, computational complexity, information theory, psychology and neurobiology, control theory, and philosophy.</li>
<li>A well-defines learning problem approach involves a number of design choices, including choosing the type of training experience, the target function to be learned, a representation for this target function, and an algorithm for learning the target function from training examples.</li>
<li>Learning involves search: searching through a space of possible hypotheses to find the hypothesis that best fits the available training examples and other prior constraints or knowledge.<!--more--></li>
</ul>


<h3>2. Concept Learning and the General-to-Specific Ordering</h3>

<ul>
<li>Concept learning can be cast as a problem of searching through a large predefined space or potential hypotheses.</li>
<li>The general-to-specific partial ordering of hypotheses, which can be defined for any concept learning problem, provides a useful structure for organizing the search through the hypothesis space.</li>
<li>The Find-S algorithm utilizes this general-to-specific ordering, performing a specific-to-general search through the hypothesis space along one branch of the partial ordering, to find the most specific hypothesis consistent whith the training examples.</li>
<li>The Candidate-Elimination algorithm utilizes this general-to-specific ordering to compute the version space (the set of all hypotheses consistent with training data) by incrementally computing the sets of maximally specific (S) and maximally general (G) hypotheses.</li>
<li>Because the S and G sets delimit the entire set of hypotheses consistent with the data, they provide the learner with a description of its uncertainty regardin the exact identity of the target concept. This version space of alternative hypotheses can be examined to determine whether the learner has converged to the target concept, to determine when the training data are inconsistent, to generate informative quaries to further refine the version space, and to determine which unseen instances can be unambiguously classified based on the partially learned concept.</li>
<li>Version spaces and the Candidate-Elimination algorithm provide a useful conceptual framework for studying concept learning. However, this learning algorithm is not robust to noisy data or to situations in which the unknown target concept is not expressible in the provided hypothesis space.</li>
<li>Inductive learning algorithms are able to classify unseen examples only because of their mplicit inductive bias for selecting one cinsistent hypothesis over another. The bias concept can be found in the provided hypothesis space (c ∈ H). The output hypotheses and classifications of subsequent instances follow deductively from this assumption together with the observed training data.</li>
<li>If the hypothesis space is enriched to the point where there is a hypothesis corresponding to every possible subset of instances (the power set of the instances), this will remove any in ductive bias from the Candidate-Elimination algorithm, Unfortunately, this also removes the ability to classfy any instance beyond the observed training examples. An unbiased learner cannot make inductive leaps to classify unseen examples.</li>
</ul>


<h3>3. Decision Tree Learning</h3>

<ul>
<li>Decision tree learning provides a practical method for concept learning and for learning other discrete-valued functions. The ID3 family of algorithms infers decision trees by growing them from the root downward, greedily selecting the next best attribute for each new decision branch added to the tree.</li>
<li>ID3 searches a complete hypothesis space (i.e., the space of decision trees can represent any discrete-valued function defined over discrete-valued instances). It thereby avoids the major difficulty associated with approaches that consider only restricted sets of hypotheses: that the target function might not be present in the hypothesis space.</li>
<li>The inductiove bias implicit in ID3 includes a preference for smaller trees; that is, its search through the hypothesis space grows the tree only as large as needed in order to clasiy the available training examples.</li>
<li>Overfitting the training data is an important issue in decision tree learning. Because the training examples are only a sample of all possible instances, it is possible to add branches to the tree that improve performance on the training examples while decreasing performance on other instances outside this set. Methods for post-pruning the decision tree are therefore important to avoid overfitting in decision tree learning (and other inductive inference methods that employ a preference bias).</li>
<li>A large variety of extensions to the basic ID3 algorithm has been developed by different researchers. These include methods for post-pruning trees, handling real-valued attributes, accommodating training examples with missing attribute values, incrementally refining decision trees as new training examples become available, using attribute selection measures other than information gain, and considering costs associated with instance attibutes.</li>
</ul>


<h3>4. Artificial Neural Networks</h3>

<ul>
<li>Artificial neural network learning provides a practical method for learning real-valued and vector-valued functions over continuous and discrete-valued attributes, in a way that is robust to noise in the training data. The Backpropagation algorithm is the most common network learning method and has been successfully applied to a variety of learning tasks, such as handwriting recognition and robot control.</li>
<li>The hypothesis space considered by the Backpropagation algorithm is the space of all functions that can be represented by assigning weights to the given, fixed network of interconnected units. Feedforward networks containing three layers of units are able to approximate any function to arbitrary accuracy, given a sufficient (potentially very large) number of units in each layer. Even networks of practical size are capable of represening a rich choice for learning discrete and continuous functions whose general form is unknown in advance.</li>
<li>Backpropagation searches the space of possible hypotheses using gradient descent to iteratively reduce the error in the network fit to the training examples. Gradient descent converges to a local minimum in the training error with respect to the network weights. more generally, gradient descent is a potentially useful method for searching many contiously parameterized hypothesis spaces where the training error is a differentiable function of hypothesis paremeters.</li>
<li>One of the most intriguing properties of Backpropagation is its ability to invent new features that are not explicit in the input to the network. In particular, the internal (hidden) layers of multilayer networks learn to represent intermediate features that are useful for learning the target function and that are only implicit in the network inputs.</li>
<li>Overfitting the training data is an important issue in ANN learning. Overfitting results in networks that generalize poorly to new data despite excellent performance over the training data. Cross-validation methods can be used to estimate an approprite stopping point gradient descent search and thus to minimize the risk of overfitting.</li>
<li>Although Backpropagation is the most common ANN learning algorithm, many others have been proposed, including algorithms for more specialized tasks. For example, recurrent neural network methods train networks containing directed cycles, and algorithms such as Cascade Correlation alter the network structure as well as the network weights.</li>
</ul>


<h3>5. Evaluating Hypotheses</h3>

<ul>
<li>Statistical theory provides a basis for estimating the true error (error<sub>D</sub>(h)) of a hypothesis h, based on its observed error (error<sub>S</sub>(h)) over a sample S of data. For example, if h is a discrete-valued hypothesis and the data sample S contains n >= 30 examples drawn independently of h and of one another, then the N% confidence interval for (error<sub>D</sub>(h)) is approximately
<img src="http://latex.codecogs.com/gif.latex?error_{s}(h)\pm&space;z_{N}\sqrt{\frac{error_{s}(h)(1-error_{s}(h))}{n}}" title="error_{s}(h)\pm z_{N}\sqrt{\frac{error_{s}(h)(1-error_{s}(h))}{n}}" /></li>
<li>In general, the problem of estimating confidence intervals is approached by identifying the parameter to be estimated (e.g., error<sub>D</sub>(h)) and an estimator (e.g., error<sub>S</sub>(h)) for this quantity. Because the estimator is a random variable (e.g., error<sub>S</sub>(h) depends on the random sample S), it can be characterrized by the probability distribution that governs its value. Confidence intervals can then be calculated by determining the interval that contains the desired probability mass under this distribution.</li>
<li>One possible cause of errors in estimating hypothesis accuracy is estimation bias. If Y is an estimator for some oarameter p, the estimation bias of Y is the difference between p and the expected value of Y. For example, if S is the training data used to formulate hypothesis h, then error<sub>S</sub>(h) gives an optimistically biased estimate of the error error<sub>D</sub>(h).</li>
<li>A second cause of estimation error is variance in the estimate. Even with an unbiased estimator, the observed value of the estimator is likely to vary from one experiment to another. The variance σ<sup>2</sup> of the distribution governing the estimator characterizes how widely this estimate is likely to vary from the correct value. This variance decreases as the size of the data sample is increased.</li>
<li>Comparing the effectiveness of two learning algorithms is an estimation problem that is relatively easy when data and time are unlimited, but more difficult when these resourses are limited. One possible approach described in this chapter is to run the learning algorithms on different subsets of the available data, testing the learned hypotheses on the remaining data, then averaging the results of these experiments.</li>
<li>In most cases considered here, deriving confidence intervals invloves making a number of assumptions and approximations. For example, the above confidence interval for error<sub>D</sub>(h)involved approximating a Binominal distribution by a Normal distribution, approximating the variance of this distribution, and assuming instances are generated by a fixed, unchanging probability distribution. While intervals based on such approximations are oonly approximate confidence intevals, they nevertheless provide useful guidance for disigning and interpreting experimental results in machine learning.</li>
</ul>


<h3>6. Bayesian Learning</h3>

<ul>
<li>Bayesian methods providethe basis for probabilistic learning methods that accommodate (and require) knowledge about the prior probabilities of alternative hypotheses and about the probability of observing various data given the hypothesis. Bayesian methods allow assigning a posterior probability to each candidate hypothesis, based on these assumed priors and the observed data.</li>
<li>Bayesian methods can be used to determine the most probable hypothesis given the data &mdash; the maximum a posteriori (MAP) hypothesis. This is the optimal hypothesis in the sense that no other hypothesis is more likely.</li>
<li>The Bayes optimal classifier co,bines the predictions of all alternative hypotheses, weighted by their posterior probabilities, to calculate the most probable classification of each new instance.</li>
<li>The naive Bayes classifier is a Bayesian learning method that has been found to be useful in many practical applications. It is called &ldquo;naive&rdquo; because it incorporates the simplifying assumption that attibute values are conditionally independent, given the classification of the instance. When this assumption is met, the naive Bayes classifier outputs the MAP classification. Even when this asuumption is not met, as in the case of learning to clssify text, the naive Bayes classifier is often quite effective. Bayesian belief networks provide a more expressive representation for sets of conditional independence assumptions among subsets of the attributes.</li>
<li>The framework of Bayesian reasoning can provide a useful basis for analyzing certain learning methods that do not directly apply Bayes theorem. For example, under certain conditions it can be shown that minimizing the squared error when learning a real-world target function corresponds to computing the maxmum likelihood hypothesis.</li>
<li>The Minimum Description Length principle recommends choosing the hypothesis that minimizes the description length of the hypothesis plus the description length of the data given the hypothesis. Bayes theorem and baysic results from information theory can be used to provide a rationale for this principle.</li>
<li>In many practical learning tasks, some of the relevant instance variables may be unobservable. The EM algorithm provides a quite general approach to learning in the presence of unobservable variables. This algorithm begins with an arbitrary initial hypothesis. It then repeatedly calculates the expected values of the hidden variables (assuming the current hypothesis is correct), and then recalculates the maximum likelihood hypothesis (assuming the hidden variables have he expected values caldulated by the first step). This procedure converges to a local maximum likelihood hypothesis, along with estimated values for the hidden variables.</li>
</ul>


<h3>7. Computational Learning Theory</h3>

<ul>
<li>The probably approximately correct (PAC) model considers algorithms that learn target concepts from some concept class C, using examples drawn at random according to an unknown, but fixed, probability distribution. it requires that the learner probably (with probability at least [1 &ndash; δ]) learn a hypothesis that is approximately (within error ε) correct, given computational effort and training examples that frow only polynomially with 1/ε, 1/δ, the size of the instances, and the size of the target concept.</li>
<li>Within the setting of the PAC learning modelm any consistent learner using a finite hypothesis space H where C ⊆ H will, with probability (1 &ndash; δ), output a hypothesis within error ε of the target concept, after observing m randomly drawn training example, as long as<br/>
<img src="http://latex.codecogs.com/gif.latex?m&space;\geq&space;\frac{1}{\varepsilon&space;}(ln(1/\delta&space;)&plus;ln|H|)" title="m \geq \frac{1}{\varepsilon }(ln(1/\delta )+ln|H|)" /><br/>
This gives a bound on the number of training examples sufficient for successful learning under the PAC model.</li>
<li>One constraining assumption of the PAC learning model is that the learner knows in advance some restricted concept class C that contains the target concept to be learned. In contrast, the agnostic learning omdel considers the more general setting in which the learner makes no assumption about the class from which the target concept is drawn. Instead, the learner outputs the hypothesis from H that has the least error (possibly nonzero) over the training data. Under this less restrictive agnostic learning model, the learner is assured with probability (1 &ndash; δ)to output a hypothesis within error ε of the best possible hypothesis in H, fter observing m randomly drawn training examples, provided<br/>
<img src="http://latex.codecogs.com/gif.latex?m&space;\geq&space;\frac{1}{2\varepsilon^{2}&space;}(ln(1/\delta&space;)&plus;ln|H|)" title="m \geq \frac{1}{2\varepsilon^{2} }(ln(1/\delta )+ln|H|)" /></li>
<li>The number of training examples required for successful learning is strongly influenced by the complexity of the hypothesis space considered by the learner. One useful measure of the complexity of a hypothesis space H is its Vapnik-Chervonenkis dimension, VC(H). VC(H) is the size of the largest subset of instances that can be shattered (split in all possible ways by H.</li>
<li>An alternative upper bound on the number of training examples sufficient for successful learning under the PAC model, stated in terms of VC(H) is<br/>
<img src="http://latex.codecogs.com/gif.latex?m&space;\geq&space;\frac{1}{\varepsilon}(4log_{2}(2/\delta&space;)&plus;8VC(H)log_{2}(13/\varepsilon&space;))" title="m \geq \frac{1}{\varepsilon}(4log_{2}(2/\delta )+8VC(H)log_{2}(13/\varepsilon )))" /><br/>
A lower bound is<br/>
<img src="http://latex.codecogs.com/gif.latex?m&space;\geq&space;max[\frac{1}{\varepsilon&space;}log(1/\delta&space;),&space;\frac{VC(C)-1)}{32\varepsilon&space;}]" title="m \geq max[\frac{1}{\varepsilon }log(1/\delta ), \frac{VC(C)-1)}{32\varepsilon }]" /></li>
<li>An alternative learning model, called the mistake bound model, is used to analyze the number of training examples a learner will misclassify before is exactly learns the target concept. For rxample, the Halving algorithm will make at most [log<sub>2</sub>H] mistakes before exactly learning any target concept drawn from H. For an arbitrary concept class C, the best worst-case algorithm will make Opt &copy; mistakeIs, where<br/>
VC&copy; &lt;= Opt&copy; &lt;= log<sub>2</sub>(|C|)</li>
<li>The Weighted-Majority algorithm combines the weighted votes of multiple prediction algorithms to classify new instances. It learns weights for each of these prediction algorithms based on errors made over a sequence of examples. Interestingly, the number of mistakes made by Weighted-Majority be bounded in terms of the number of mistakes made by the best prediction algorithm in the pool.</li>
</ul>


<h3>8. Instance-Based Learning</h3>

<ul>
<li>Instance-based learning methods differ from other approaches to function ap- proximation because they delay processing of training examples until they must label a new query instance. As a result, they need not form an explicit hypothesis of the entire target function over the entire instance space, independent of the query instance. Instead, they may form a different local approximation to the target function for each query instance.</li>
<li>Advantages of instance-based methods include the ability to model complex target functions by a collection of less complex local approximations and the fact that information present in the training examples is never lost (because the examples themselves are stored explicitly). The main practical difficulties include efficiency of labeling new instances (all processing is done at query time rather than in advance), difficulties in determining an appropriate distance metric for retrieving &ldquo;related&rdquo; instances (especially when examples are represented by complex symbolic descriptions), and the negative impact of irrelevant features on the distance metric.</li>
<li>K-Nearest Neighbor an instance-based algorithm for approximating real-valued or discrete-valued target functions, assuming instances correspond to points in an N-dimensional Euclidean space. The target function value for a new query is estimated from the known values of the K nearest training examples.</li>
<li>Locally weighted regression methods are a generalization of K-Nearest Neighbor which an explicit local approximation to the target functionis constructed for each query instance. The local approximation to the target function may be based on a variety of functional forms such as constant, linear, or quadratic functions or on spatially localized kernel functions.</li>
<li>Radial basis function (RBF) networks are a type of artificial neural network constructed from spatially localized kernel functions. These can be seen as a blend of instance-based approaches (spatially localized influence of each kernel function) and neural network approaches (a global approximation to the target function is formed at training time rather than a local approximation at query time). Radial basis function networks have been used successfully in applications such as interpreting visual scenes, in which the assumption of spatially local influences is well-justified.* Case-based reasoning is an instance-based approach in which instances are represented by complex logical descriptions rather than points in a Euclidean space. Given these complex symbolic descriptions of instances, a rich variety of methods have been proposed for mapping from the training examples to target function values for new instances. Case-based reasoning methods have been used in applications such as modeling legal reasoning and for guiding searches in complex manufacturing and transportation planning problems.</li>
</ul>


<h3>9. Genetic Algorithm</h3>

<ul>
<li>Genetic algorithms (GAS) conduct a randomized, parallel, hill-climbing search for hypotheses that optimize a predefined fitness function.* The search performed by GAS is based on an analogy to biological evolution. A diverse population of competing hypotheses is maintained. At each iteration, the most fit members of the population are selected to produce new offspring that replace the least fit members of the population. Hypotheses are often encoded by strings that are combined by crossover operations, and subjected to random mutations.</li>
<li>GAs illustrate how learning can be viewed as a special case of optimization. In particular, the learning task is to find the optimal hypothesis, according to the predefined fitness function. This suggests that other optimization tech- niques such as simulated annealing can also be applied to machine learning problems.<em> GAs have most commonly been applied to optimization problems outside machine learning, such as design optimization problems. When applied to learning tasks, GAS are especially suited to tasks in which hypotheses are complex (e.g., sets of rules for robot control, or computer programs), and in which the objective to be optimized may be an indirect function of the hypothesis (e.g., that the set of acquired rules successfully controls a robot).</em> Genetic programming is a variant of genetic algorithms in which the hypotheses being manipulated are computer programs rather than bit strings. Operations such as crossover and mutation are generalized to apply to programs rather than bit strings. Genetic programming has been demonstrated to learn programs for tasks such as simulated robot control (Koza 1992) and recognizing objects in visual scenes (Teller and Veloso 1994).</li>
</ul>


<h3>10. Learning Sets of Rules</h3>

<ul>
<li>The sequential covering algorithm learns a disjunctive set of rules by first learning a single accurate rule, then removing the positive examples covered by this rule and iterating the process over the remaining training examples. It provides an efficient, greedy algorithm for learning rule sets, and an al- ternative to top-down decision tree learning algorithms such as ID3, which can be viewed as simultaneous, rather than sequential covering algorithms.<em> In the context of sequential covering algorithms, a variety of methods have been explored for learning a single rule. These methods vary in the search strategy they use for examining the space of possible rule preconditions. One popular approach, exemplifiedby the CN2 program, is to conduct a general-to-specific beam search, generating and testing progressively more specific rules until a sufficiently accurate rule is found. Alternative approaches search from specific to general hypotheses, use an example-driven search rather than generate and test, and employ different statistical measures of rule accuracy to guide the search.</em> Sets of first-order rules (i.e., rules containing variables) provide a highly expressive representation. For example, the programming language PROLOG represents general programs using collections of first-order Horn clauses. The problem of learning first-order Horn clauses is therefore often referred to as the problem of inductive logic programming.<em> One approach to learning sets of first-order rules is to extend the sequential covering algorithm of CN2 from propositional to first-order representations. This approach is exemplified by the FOIL program, which can learn sets of first-order rules, including simple recursive rule sets.</em> A second approach to learning first-order rules is based on the observation that induction is the inverse of deduction. In other words, the problem of induction is to find a hypothesis h that satisfies the constraint<br/>
<img src="http://latex.codecogs.com/gif.latex?(\veebar&space;\left&space;\langle&space;x_{i},f(x_{i})&space;\right&space;\rangle&space;\subseteq&space;D)&space;(B\wedge&space;h&space;\wedge&space;x_{i})\vdash&space;f(x_{i})" title="(\veebar \left \langle x_{i},f(x_{i}) \right \rangle \subseteq D) (B\wedge h \wedge x_{i})\vdash f(x_{i})" /><br/>
WhereB is general background information, x<sub>1</sub> &hellip; x<sub>n</sub> are descriptions of the instances in the training data D, and f(x<sub>1</sub>) &hellip; f(x<sub>n</sub>) are the target values of the training instances.</li>
<li>Following the view of induction as the inverse of deduction, some programs search for hypotheses by using operators that invert the well-known operators for deductive reasoning. For example, Cigol uses inverse resolution, an operation that is the inverse of the deductive resolution operator commonly used for mechanical theorem proving. Progol combines an inverse entailment strategy with a general-to-specific strategy for searching the hypothesis space.</li>
</ul>


<h3>11. Analytical Learning</h3>

<ul>
<li>In contrast to purely inductive learning methods that seek a hypothesis to fit the training data, purely analytical learning methods seek a hypothesis that fits the learner&rsquo;s prior knowledge and covers the training examples. Humans often make use of prior knowledge to guide the formation of new hypotheses. This chapter examines purely analytical learning methods. The next chapter examines combined inductive-analytical learning.<em> Explanation-based learning is a form of analytical learning in which the learner processes each novel training example by (1) explaining the observed target value for this example in terms of the domain theory, (2) analyzing this explanation to determine the general conditions under which the explanation holds, and (3) refining its hypothesis to incorporate these general conditions.</em> Prolog-Ebg isanexplanation-based learning algorithm that uses first-order Horn clauses to represent both its domain theory and its learned hypotheses. In Prolog-Ebg an explanation is a Prolog proof, and the hypothesis extracted from the explanation is the weakest preimage of this proof. As a result, the hypotheses output by Prolog-Ebg follow deductively from its domain theory.<em> Analytical learning methods such as Prolog-Ebg construct useful intermediate features as a side effect of analyzing individual training examples. This analytical approach to feature generation complements the statistically based generation of intermediate features (e.g., hidden unit features) in inductive methods such as Backpropagation.</em> Although Prolog-Ebg does not produce hypotheses that extend the deductive closure of its domain theory, other deductive learning procedures can. For example, a domain theory containing determination assertions (e.g., &ldquo;nationality determines language&rdquo;) can be used together with observed data to deductively infer hypotheses that go beyond the deductive closure of the domain theory.<em> One important class of problems for which a correct and complete domain theory can be found is the class of large state-space search problems. Systems such as Prodigy and Soar have demonstrated the utility of explanation-based learning methods for automatically acquiring effective search control knowledge that speeds up problem solving in subsequent cases.</em> Despite the apparent usefulness of explanation-based learning methods in humans, purely deductive implementations such as Prolog-Ebg suffer the disadvantage that the output hypothesis is only as correct as the domain theory. In the next chapter we examine approaches that combine inductive and analytical learning methods in order to learn effectively from imperfect domain theories and limited training data.</li>
</ul>


<h3>12. Combining Inductive and Analytical Learning</h3>

<ul>
<li>Approximate prior knowledge, or domain theories, are available in many practical learning problems. Purely inductive methods such as decision tree induction and neural network Backpropagation fail to utilize such domain theories, and therefore perform poorly when data is scarce. Purely analytical learning methods such as Prolog-Ebg utilize such domain theories, but produce incorrect hypotheses when given imperfect prior knowledge. Methods that blend inductive and analytical learning can gain the benefits of both approaches: reduced sample complexity and the ability to overrule incorrect prior knowledge.<em> One way to view algorithms for combining inductive and analytical learning is to consider how the domain theory affects the hypothesis space search. In this chapter we examined methods that use imperfect domain theories to (1) create the initial hypothesis in the search, (2) expand the set of search operators that generate revisions to the current hypothesis, and (3) alter the objective of the search.</em> A system that uses the domain theory to initialize the hypothesis is KBANN. This algorithm uses a domain theory encoded as propositional rules to analytically construct an artificial neural network that is equivalent to the domain theory. This network is then inductivelyrefined using the Backpropagation algorithm, to improve its performance over the training data. The result is a network biased by the original domain theory, whose weights are refined inductively based on the training data.<em> Tangentprop uses prior knowledge represented by desired derivatives of the target function. In some domains, such as image processing, this is a natural way to express prior knowledge. Tangentprop incorporates this knowledge by altering the objective function minimized by gradient descent search through the space of possible hypotheses.</em> EBNN uses the domain theory to alter the objective in searching the hypothesis space of possible weights for an artificial neural network. It uses a domain theory consisting of previously learned neural networks to perform a neural network analog to symbolic explanation-basedlearning. As in symbolic explanation-based learning, the domain theory is used to explain individual examples, yielding information about the relevance of different example features. With this neural network representation, however, information about relevance is expressed in the form of derivatives of the target function value with respect to instance features. The network hypothesis is trained using a variant of the Tangentprop algorithm, in which the error to be minimized includes both the error in network output values and the error in network derivatives obtained from explanations.<em> Focl uses the domain theory to expand the set of candidates considered at each step in the search. It uses an approximate domain theory represented by first order Horn clauses to learn a set of Horn clauses that approximate the target function. Focl employs a sequential covering algorithm, learning each Horn clause by a general-to-specific search. The domain theory is used to augment the set of next more specific candidate hypotheses considered at each step of this search. Candidate hypotheses are then evaluated based on their performance over the training data. In this way, Focl combines the greedy, general-to-specific inductive search strategy of Foil with the rule-chaining, analytical reasoning of analytical methods.</em> The question of how to best blend prior knowledge with new observations remains one of the key open questions in machine learning.</li>
</ul>


<h3>13. Reinforcement Learning</h3>

<ul>
<li>Reinforcement learning addresses the problem of learning control strategies for autonomous agents. It assumes that training information is available in the form of a real-valued reward signal given for each state-action transition. The goal of the agent is to learn an action policy that maximizes the total reward it will receive from any starting state.<em> The reinforcement learning algorithms addressed in this chapter fit a problem setting known as a Markov decision process. In Markov decision processes, the outcome of applying any action to any state depends only on this action and state (and not on preceding actions:or states). Markov decision processes cover a wide range of problems including many robot control, factory automation, and scheduling problems.</em> Q learning is one form of reinforcement learning in which the agent learns an evaluation function over states and actions. In particular, the evaluation function Q(s, a) is defined as the maximum expected, discounted, cumulative reward the agent can achieve by applying action a to state s. The Q learning algorithm has the advantage that it can be employed even when the learner has no prior knowledge of how its actions affect its environment.<em> Q learning can be proven to converge to the correct Q function under certain assumptions, when the learner&rsquo;s hypothesis Q<sup>s, a</sup>, is represented by a lookup table with a distinct entry for each &lt;s,a> pair. It can be shown to converge in both deterministic and nondeterministic MDPs. In practice, Q learning can require many thousands of training iterations to converge in even modest-sized problems.</em> Q learning is a member of a more general class of algorithms, called temporal difference algorithms. In general, temporal difference algorithms learn by iteratively reducing the discrepancies between the estimates produced by the agent at different times.* Reinforcement learning is closely related to dynamic programming approaches to Markov decision processes. The key difference is that historically these dynamic programming approaches have assumed that the agent possesses knowledge of the state transition function δ(s, a) and reward function r(s, a). In contrast, reinforcement learning algorithms such as Q learning typically assume the learner lacks such knowledge.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Data Science Resources]]></title>
    <link href="http://www.aprilzephyr.com/blog/05102015/data-science-resources/"/>
    <updated>2015-05-10T22:01:26+08:00</updated>
    <id>http://www.aprilzephyr.com/blog/05102015/data-science-resources</id>
    <content type="html"><![CDATA[<h3>Blogs</h3>

<p><a href="http://simplystatistics.org/">Simply Statistics</a>: Written by the Biostatistics professors at Johns Hopkins University who also run Coursera&rsquo;s <a href="https://www.coursera.org/specialization/jhudatascience/1">Data Science Specialization</a><br/>
<a href="http://blog.yhathq.com/">yhat&rsquo;s blog</a>: Beginner-friendly content, usually in Python<br/>
<a href="http://blog.kaggle.com/">No Free Hunch (Kaggle&rsquo;s blog)</a>: Mostly interviews with competition winners, or updates on their competitions<br/>
<a href="http://fastml.com/">FastML</a>: Various machine learning content, often with code<br/>
<a href="http://blog.echen.me/">Edwin Chen</a>: Infrequently updated, but long and thoughtful pieces<br/>
<a href="http://fivethirtyeight.com/">FiveThirtyEight</a>: Tons of timely data-related content<br/>
<a href="http://machinelearningmastery.com/blog/">Machine Learning Mastery</a>: Frequent posts on machine learning, very accessible<br/>
<a href="http://www.dataschool.io/">Data School</a>: Kevin Markham&rsquo;s blog! Beginner-focused, with reference guides and videos<br/>
<a href="http://mlwave.com/">MLWave</a>: Detailed posts on Kaggle competitions, by a Kaggle Master<br/>
<a href="http://101.datascience.community/">Data Science 101</a>: Short, frequent content about all aspects of data science<br/>
<a href="http://ml.posthaven.com/">ML in the Valley</a>: Thoughtful pieces by the Director of Analytics at Codecademy<!--more--></p>

<h3>Aggregators</h3>

<p><a href="http://www.datatau.com/">DataTau:</a> Like <a href="https://news.ycombinator.com/">Hacker News</a>, but for data<br/>
<a href="http://www.reddit.com/r/MachineLearning/">MachineLearning on reddit</a>: Very active subreddit<br/>
<a href="http://www.quora.com/Machine-Learning">Quora&rsquo;s Machine Learning section</a>: Lots of interesting Q&amp;A
<a href="https://www.quora.com/What-is-the-Data-Science-topic-FAQ">Quora&rsquo;s Data Science topic FAQ</a><br/>
<a href="http://www.kdnuggets.com/">KDnuggets</a>: Data mining news, jobs, classes and more</p>

<h3>DC Data Groups</h3>

<p><a href="http://www.datacommunitydc.org/">Data Community DC</a>: Coordinates six local data-related meetup groups<br/>
<a href="http://www.districtdatalabs.com/">District Data Labs</a>: Offers courses and other projects to local data scientists</p>

<h3>Online Classes</h3>

<p><a href="https://www.coursera.org/specialization/jhudatascience/1">Coursera&rsquo;s Data Science Specialization</a>: Nine courses (running every month) and a Capstone project, taught in R<br/>
<a href="http://online.stanford.edu/course/statistical-learning">Stanford&rsquo;s Statistical Learning</a>: By the authors of <a href="http://www-bcf.usc.edu/%7Egareth/ISL/">An Introduction to Statistical Learning</a> and <a href="http://statweb.stanford.edu/%7Etibs/ElemStatLearn/">Elements of Statistical Learning</a>, taught in R, highly recommended, running January through April 2015 (preview the <a href="http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/">lecture videos</a>)<br/>
<a href="https://www.coursera.org/course/ml">Coursera&rsquo;s Machine Learning (Andrew Ng)</a>: Andrew Ng&rsquo;s acclaimed course, taught in MATLAB/Octave (preview the <a href="https://class.coursera.org/ml-005/lecture">lecture videos</a>)<br/>
<a href="https://www.coursera.org/course/machlearning">Coursera&rsquo;s Machine Learning (Pedro Domingos)</a>: No upcoming sessions (preview the <a href="https://class.coursera.org/machlearning-001/lecture">lecture videos</a>)<br/>
<a href="http://work.caltech.edu/telecourse.html">Caltech&rsquo;s Learning from Data</a>: Widely praised, not language-specific<br/>
<a href="https://www.udacity.com/course/nd002">Udacity&rsquo;s Data Analyst Nanodegree</a>: Project-based curriculum using Python, R, MapReduce, MongoDB<br/>
<a href="https://www.coursera.org/specialization/datamining/20">Coursera&rsquo;s Data Mining Specialization</a>: Brand new specialization beginning February 2015<br/>
<a href="https://www.coursera.org/course/nlp">Coursera&rsquo;s Natural Language Processing</a>: No upcoming sessions, but <a href="https://class.coursera.org/nlp/lecture">lectures</a> and <a href="http://web.stanford.edu/%7Ejurafsky/NLPCourseraSlides.html">slides</a> are available<br/>
<a href="https://www.mysliderule.com/learning-paths/data-analysis">SlideRule&rsquo;s Data Analysis Learning Path</a>: Curated content from various online classes<br/>
<a href="https://www.udacity.com/course/cs271">Udacity&rsquo;s Intro to Artificial Intelligence</a>: Taught by Peter Norvig and Sebastian Thrun<br/>
<a href="https://www.coursera.org/course/neuralnets">Coursera&rsquo;s Neural Networks for Machine Learning</a>: Taught by Geoffrey Hinton, no upcoming sessions<br/>
<a href="http://www.statistics.com/data-science/">statistics.com</a>: Many online courses in data science<br/>
<a href="http://www.coursetalk.com/">CourseTalk</a>: Read reviews of online courses</p>

<h3>Online Content from Offline Classes</h3>

<p><a href="http://cs109.github.io/2014/">Harvard&rsquo;s CS109 Data Science</a>: Similar topics as General Assembly&rsquo;s course<br/>
<a href="http://www2.research.att.com/%7Evolinsky/DataMining/Columbia2011/Columbia2011.html">Columbia&rsquo;s Data Mining Class</a>: Excellent slides<br/>
<a href="http://www.cs171.org/2015/index.html">Harvard&rsquo;s CS171 Visualization</a>: Includes programming in D3<br/>
<a href="http://cs229.stanford.edu">Stanford&rsquo;s Machine Learning CS229</a></p>

<h3>Face-to-Face Educational Programs</h3>

<p><a href="http://yet-another-data-blog.blogspot.com/2014/04/data-science-bootcamp-landscape-full.html">Comparison of data science bootcamps</a>: Up-to-date list maintained by a Zipfian Academy graduate<br/>
<a href="http://www.skilledup.com/articles/list-data-science-bootcamps/">The Complete List of Data Science Bootcamps &amp; Fellowships</a><br/>
<a href="http://www.zipfianacademy.com/">Zipfian Academy</a>: Offers Data Science Immersive, Data Engineering Immersive, Master&rsquo;s in Big Data (San Francisco, but possibly expanding)<br/>
<a href="http://datascienceretreat.com/">Data Science Retreat</a>: Primarily uses R (Berlin)<br/>
<a href="http://www.thisismetis.com/data-science">Metis Data Science Bootcamp</a>: Newer bootcamp (New York)<br/>
<a href="http://www.persontyle.com/">Persontyle</a>: Various course offerings (based in London)<br/>
<a href="http://software-carpentry.org/">Software Carpentry</a>: Two-day workshops, primarily for researchers and hosted by universities (worldwide)<br/>
<a href="http://datascience.community/colleges">Colleges and Universities with Data Science Degrees</a></p>

<h3>Conferences</h3>

<p><a href="http://www.kdd.org/">Knowledge Discovery and Data Mining (KDD)</a>: Hosted by ACM<br/>
<a href="http://strataconf.com/">O&#8217;Reilly Strata + Hadoop World</a>: Big focus on &ldquo;big data&rdquo; (San Jose, London, New York)<br/>
<a href="http://pydata.org/">PyData</a>: For developers and users of Python data tools (worldwide)<br/>
<a href="https://us.pycon.org/">PyCon</a>: For developers and users of Python (Montreal in April 2015)</p>

<h3>Books</h3>

<p><a href="http://www-bcf.usc.edu/%7Egareth/ISL/">An Introduction to Statistical Learning with Applications in R</a><br/>
<a href="http://www-stat.stanford.edu/%7Etibs/ElemStatLearn/">Elements of Statistical Learning</a><br/>
<a href="http://www.greenteapress.com/thinkstats/">Think Stats</a><br/>
<a href="http://www.mmds.org/">Mining of Massive Datasets</a><br/>
<a href="http://www.pythonlearn.com/book.php">Python for Informatics</a><br/>
<a href="http://www.statsoft.com/Textbook">Statistics: Methods and Applications</a><br/>
<a href="http://shop.oreilly.com/product/0636920023784.do">Python for Data Analysis</a><br/>
<a href="http://www.amazon.com/gp/product/111866146X/">Data Smart: Using Data Science to Transform Information into Insight</a><br/>
<a href="http://www.amazon.com/Sams-Teach-Yourself-Minutes-Edition/dp/0672336073">Sams Teach Yourself SQL in 10 Minutes</a></p>

<h3>Other Resources</h3>

<p><a href="https://github.com/datasciencemasters/go">Open Source Data Science Masters</a>: Huge list of resources<br/>
<a href="https://trello.com/b/rbpEfMld/data-science">Data Science Trello Board</a>: Another list of resources<br/>
<a href="http://docs.python-guide.org/en/latest/">The Hitchhiker&rsquo;s Guide to Python</a>: Online guide to understanding Python and getting good at it<br/>
<a href="https://github.com/rasbt/python_reference">Python Reference</a>: Python tips, tutorials, and more<br/>
<a href="http://videolectures.net/Top/Computer_Science/">videolectures.net</a>: Tons of academic videos<br/>
<a href="http://www.metacademy.org/list">Metacademy</a>: Quick summary of many machine learning terms, with links to resources for learning more<br/>
<a href="https://github.com/rasbt/pattern_classification/blob/master/resources/data_glossary.md">Terms in data science defined in one paragraph</a></p>

<p><a href="https://github.com/justmarkham/DAT4/blob/master/resources.md">Origin</a><br/>
<a href="http://www.dataschool.io/resources/">Reference</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Python IAQ: Infrequently Answered Questions]]></title>
    <link href="http://www.aprilzephyr.com/blog/05052015/python-infrequently-answered-questions/"/>
    <updated>2015-05-05T13:02:30+08:00</updated>
    <id>http://www.aprilzephyr.com/blog/05052015/python-infrequently-answered-questions</id>
    <content type="html"><![CDATA[<h3>1 Q: 什麽是&#8221;少有回答的問題(Infrequently Answered Question)&ldquo; ?</h3>

<p>一個問題之所以很少有人回答，要麽是因為很少有人知道問題的答案，要麽是因為它涉及到一個晦澀而隱蔽的知識點(但可能是你關心的)。我過去認為是我在<a href="http://www.norvig.com/java-iaq.html">Java IAQ</a>中發明了這個詞組，但是它也出現在了以資料豐富而著稱的<a href="http://urbanlegends.about.com/library/weekly/aa082497.htm">About.com Urban Legends</a>網站上. 關於Python的FAQ有很多,但是Python的IAQ只有這一個。(&ldquo;少見問題列表&#8221;倒是有一些，其中一個是有諷刺意味的<a href="http://www.plethora.net/%7Eseebs/faqs/c-iaq.html">C</a>。)<!--more--></p>

<h3>2 Q: finally子句中的代碼每次都會被執行,對嗎?</h3>

<p>每次?應該說，幾乎每次。在try子句被執行後，無論是否出現異常，finally子句中的代碼都會被執行，即使調用了sys.exit. 不過如果程序沒有執行到finally子句的話，它就沒有辦法運行了。下面的代碼中，無論choice取何值，都會發生這樣的情況:</p>

<pre><code>try:
    if choice:
        while 1:
            pass
    else:
        print "Please pull the plug on your computer sometime soon..."
        time.sleep(60 * 60 * 24 * 365 * 10000)
finally:
    print "Finally ..."
</code></pre>

<h3>3 Q: 多態真是太棒了!無論一個列表(list)中的元素是什麽類型,我都可以用sort對它排序,對嗎?</h3>

<p>不對。考慮這種情況:</p>

<pre><code>&gt;&gt;&gt; x = [1, 1j]
&gt;&gt;&gt; x.sort()
Traceback (most recent call last):
  File "&lt;pyshell#13&gt;", line 1, in ?
    x.sort()
TypeError: cannot compare complex numbers using &lt;, &lt;=, &gt;, &gt;=
</code></pre>

<p>(1j是一個數，表示-1的平方根)問題在於:sort方法(在目前的實現中)使用__lt__方法來 比較元素的大小。而__lt__方法拒絕比較復數的大小(因為它們是不能排序的)。奇怪的是,complex.__lt__會毫不猶豫的比較復數與字符串，列表(list)和其他所有類型，除了復數。所以答案是,你可以對支持__lt__方法的對象序列(sequence)進行排序(當然如果將來實現變了，可能就是其它方法了)。</p>

<p>對於問題的地一部份，“多態真棒”，我同意。但是Python有時會讓使用多態變得困難，因為許多Python的類型(比如序列和數)的定義不太符合規則。</p>

<h3>4 Q: 在Python中我能寫++x和x++嗎?</h3>

<p>從語法上說，++x能， x++不能。但是從實際使用來說，別這樣做。這麽說什麽意思？<br/>
* 可以， ++x是合法的Python語法。不過如果你是一個C++或者Java程序員的話，它表示不是你想的那個意思。加號+是一個單目前綴操作符，所以++x被解析為+(+x),它表示的(至少對於數字來說)就是x。<br/>
* 不可以， x++本身就不是一個合法的表達式, 雖然在某些上下文時合法。比如， x++ -y被解析為x++(&ndash;(y)), 對於數字來說，等於x &ndash; y。當然，你可以創建一個類，讓++x有(很有限的)意義。比如可以讓這個類保存一個數字，然後使單目操作符+使它增加0.5(或者有0.5的概率增加1，如果你喜歡隨機化算法)，但是&hellip;<br/>
* 不可以，那樣真傻。最好還是用Python 2.0已經中加入的x += 1。
進一步的問題:為什麽Python不允許 x++？ 我相信原因與Python不允許在表達式中賦值一樣: Python想要清晰的區分語句和表達式。如果我覺得這兩者應該有所區別，那麽不允許++就是最好的決定。另一方面，函數語言的鼓吹者認為語句就應該是表達式。我跟我的丹麥老鄉，Bjarne Stroustrup，都這樣認為。他在The Design and Evolution of C++中說:“如果是從頭來設計一種語言的話，我會按照Algol68的方式，讓每條語句和聲明都是一個有返回值的表達式”。</p>

<h3>5 Q: 我能使用C++中對ostreams那樣的語法嗎，像這樣麽: count &lt;&lt; x &lt;&lt; y &hellip;?</h3>

<p>當然可以。如果你不喜歡寫&#8221;print x,y&#8221;，你可以試試這個：</p>

<pre><code>import sys

class ostream:
    def __init__(self, file):
        self.file = file

    def __lshift__(self, obj):
        self.file.write(str(obj));
        return self

cout = ostream(sys.stdout)
cerr = ostream(sys.stderr)
nl = '\n'
-----------------------------------
cout &lt;&lt; x &lt;&lt; " " &lt;&lt; y &lt;&lt; nl
</code></pre>

<p>(本文中所有的文件中的代碼都在橫線以上，使用這些代碼的例子在橫線以下。)這樣你就可以使用一種不同的語法了，但是它不能給你帶來一種新的輸出格式，它只是把Python中以有str的格式封裝了一層而已。這個做法很像Java裏面的toString()格式。C++使用的是一種迥異的格式：它沒有定義一組把對象轉換為字符串的規則，而定義了一種把對象打印到流的規則(也許是不完整的規則，因為很多C++程序仍然使用printf)。用流來實現會更加復雜，但是它的優勢在於如果你需要打印一個相當巨大的對象，就不用創建一個巨大的臨時對象來做這件事。</p>

<h3>6 Q: 如果我喜歡C++的printf呢?</h3>

<p>在Python中定義一個printf不是一個壞主意. 你可能認為printf(&ldquo;%d = %s&rdquo;, num, result)比print &ldquo;%d = %s&rdquo; % (num, result)更加自然, 因為那一對括號在更熟悉的位置(而且你不想要那個%)。更和況, 滿足這個需求輕而易舉:</p>

<pre><code>def printf(format, *args): print format % args,
</code></pre>

<p>即使是像這樣的一行代碼，也有幾個不同實現。首先，我必需要決定是否在結尾添加逗號。為了更像C++, 我決定加上(這就意味著如果你想在結尾換行，你需要自己在格式字符串的末尾添加)。其次，結尾處會打印一個空格。如果你不想要它，使用sys.stdout.write來代替print. 最後, 把一切都變得更像C好是一件好事嗎? 是，因為你需要一個打印函數(而不是一個打印語句)在只接受函數不接受語句的地方使用。比如，在lambda表達式中和map的第一個參數。事實上，這樣一個函數使用起來是很趁手的，你可能想要一個沒有格式化功能的:</p>

<pre><code>def prin(x): print x,
</code></pre>

<p>現在map(prin, seq)將打印seq中的每一個元素. 但是map(print, seq)是一個語法錯誤. 我曾經見過有些粗心大意的程序員(好吧, 沒錯, 我自己就是. 但是我知道我自己很粗心 )認為把這兩個函數合二為一是個好主意, 像這樣:</p>

<pre><code>def printf(format, *args): print str(format) % args,  
</code></pre>

<p>這樣 printf(42)， printf(&lsquo;A multi-line\n message&rsquo;)和 printf(&lsquo;%4.2f&rsquo;, 42)都能工作。但是當你用了pring(&lsquo;100% guaranteed&rsquo;)或者是其他任何含有%字符卻並不是一個格式化指令時，&#8221;好主意&#8221;就會變成&#8221;我想啥呢?&ldquo;。如果你真的實現了這麽一個printf，它需要這樣的註釋:</p>

<pre><code>def printf(format, *args): 
    """使用第一個參數作為格式字符串來格式化args, 然後打印. 
    如果format不是字符串, 將被str轉換成字符串. 如果x可能含
    有%和反斜線字符, 你必須使用printf('%s', x)來代替 printf(x).
    """ 
  print str(format) % args,
</code></pre>

<h3>7 Q: 關於字典(Dictionary)，有沒有更好的語法? 我使用的鍵(key)都是標識符.</h3>

<p>有!用一對引號來包括鍵的確是一件麻煩的事情，尤其當鍵是一個很長的字符串時. 起初我認為Python中加入特別的語法是有幫助的，用{a=1, b=2}來代替現在必需的{&lsquo;a&rsquo;:1, &lsquo;b&rsquo;:2}。在Python 2.3中，你可以用的語法是dict(a=1, b=2, c=3, dee=4)，這和我的想法一樣好。在Python 2.3以前，我使用一個只有一行的函數def Dict(**dict): return dict</p>

<p>一個讀者指出，對於散列Perl也有類似的特殊符號: 在Perl中對於散列文本，你可以寫(&ldquo;a&rdquo;, 1, &ldquo;b&rdquo;, 2)或者(a=>1, b=>2)。這是事實，但不是事實的全部。&#8221;man perlop&#8221;說&#8221;=>符號最多只是逗號操作符的同意詞&hellip;&ldquo;而且事實上當a和b是barewords時，你可以寫(a, 1, b, 2)。但是，就像Dag Asheim指出的，如果你打開strict，你將會從這個寫法中得到一個錯誤。你必須要麽使用字符串，要麽使用=>操作符。最後，Larry Wall已經申明，&#8221;Perl 6中將不會有bareword&#8221;。(關於perl的這以部分，我的翻譯可能有很大問題，因為我根本不會Perl!&mdash;譯註)</p>

<h3>8 Q: 那麽，對象有沒有類似的簡便辦法呢?</h3>

<p>的確是有的。如果你想要創建一個對象來把數據保存在不同的域中，下面的代碼就可以做到:</p>

<pre><code>class Struct:
    def __init__(self, **entries):      self.__dict__.update(entries)
&gt;&gt;&gt; globals = Struct(answer=42, linelen = 80,   font='courier')
&gt;&gt;&gt; globals.answer
42
&gt;&gt;&gt; globals.answer = 'plastics'
&gt;&gt;&gt; vars(globals)
{'answer': 'plastics', 'font': 'courier', 'linelen': 80}
</code></pre>

<p>從本質上說，我們在這裏做的是創建一個匿名類。好吧，我知道globals的類是 Struct，但是因為我們在它裏面添加了slots，就像是創建了一個新的，未命名的類(這和lambda創建匿名函數是很像的)。我討厭再給Struct添加什麽了，因為它現在很簡潔，不過如果你添加下面的方法，就可以漂亮打印出它的每個結構。</p>

<pre><code>def __repr__(self):
    args = ['%s=%s' % (k, repr(v)) for (k,v) in     vars(self).items()]
    return 'Struct(%s)' % ', '.join(args)
&gt;&gt;&gt; globals
------------------------------------------------
Struct(answer='plastics', font='courier', linelen=80)
</code></pre>

<h3>9 Q: 這樣創建新對象是很方便，但是要更新時怎麽辦呢?</h3>

<p>是這樣的，字典是有一個update方法的，所以當d是一個字典時，你可以用d.update(dict(a=100, b=200))。但是對象沒有對應的方法，所以你只能用obj.a = 100;obj.b = 200。或者你可以定義一個函數update(x, a=100, b=200)來更新x，無論它是字典還是對象都可以:</p>

<pre><code>import types

def update(x, **entries):
    if type(x) == types.DictType: x.update(entries)
    else: x.__dict__.update(entries)
    return x
</code></pre>

<p>把它用於構造函數特別漂亮:</p>

<pre><code>def __init__(self, a, b, c, d=42, e=None, f=()):
    update(self, a=a, b=b, c=c, d=d, e=e, f=f) 
</code></pre>

<h3>10 Q: 我能創建一個默認值為0或者[]的或者別的什麽的字典麽?</h3>

<p>如果你常常要對某個東西計數，咱們會有同感: count[x] ＋＝ 1比被迫用的count[x] = count.get(x, 0) + 1要優美許多。在Python 2.2以後，繼承內建的dict類可以輕松的搞定這個。我把它叫做我的DefaultDict。註意copy.deepcopy的使用: 有了它，就不會讓dict裏面的每個key都使用同一個[]作為默認值(雖然拷貝0浪費了一點時間，不過如果你使用更新和訪問比初始化更頻繁的話，還算可以接受):</p>

<pre><code>class DefaultDict(dict):
"""Dictionary with a default value for unknown keys."""
    def __init__(self, default):
        self.default = default

   def __getitem__(self, key):
        if key in self: return self.get(key)
        return self.setdefault(key, copy.deepcopy(self.default))
--------------------------------------
&gt;&gt;&gt; d = DefaultDict(0)
&gt;&gt;&gt; d['hello'] += 1
&gt;&gt;&gt; d
{'hello': 1}
&gt;&gt;&gt; d2 = DefaultDict([])
&gt;&gt;&gt; d2[1].append('hello')
&gt;&gt;&gt; d2[2].append('world')
&gt;&gt;&gt; d2[1].append('there')
&gt;&gt;&gt; d2
{1: ['hello', 'there'], 2: ['world']}

def bigrams(words):
    "Counts of word pairs, in a dict of dicts."
    d = DefaultDict(DefaultDict(0))
    for (w1, w2) in zip([None] + words, words + [None]):
        d[w1][w2] += 1
    return d

&gt;&gt;&gt; bigrams('i am what i am'.split())
{None: {'i': 1}, 'i': {'am': 2}, 'what': {'i': 1},  'am': {None: 1, 'what': 1}}
</code></pre>

<p>值得註意的是，如果沒有DefaultDict，bigram例子程序中的d[w1][w2] += 1就大概應該象這樣:</p>

<pre><code>d.setdefault(w1,{}).setdefault(w2, 0); d[w1][w2] += 1
</code></pre>

<h3>11 Q: 嘿，你能用0.0007KB或者更少的代碼做一個矩陣變換麽?</h3>

<p>我還以為你永遠不會問呢. 如果你用序列組成的序列來表示矩陣的話，用zip就可以搞定了:</p>

<pre><code>&gt;&gt;&gt; m = [(1,2,3), (4,5,6)] 
&gt;&gt;&gt; zip(*m) 
[(1, 4), (2, 5), (3, 6)]
</code></pre>

<p>要想理解它，你需要知道f(*m)就像於apply(f,m)。你問的是一個古老的Lisp問題，在Python中它的等價答案是map(None, *m)，但是用Chih-Chung Chang建議的zip版代碼會更短小。你可能認為這些代碼唯一的用處就是在Letterman的Stupid Programmer&#8217;sTricks(David Michael Letterman, 美國晚間脫口秀主持人，他主持的一個著名節目是Stupid Pet Tricks——譯註)中露臉，但是有一天我遇到了這個問題:有一個數據庫行的列表，每一行中都是排序過的值的列表。找出每一列中不重復的值，組成一個列表。我的答案是：</p>

<pre><code>possible_values = map(unique, zip(*db))  
</code></pre>

<h3>12 Q: 用f(*m)的技巧很酷. 有沒有同樣的語法可以用在方法調用上, 比如x.f(*y)?</h3>

<p>這個問題暴露一個錯誤的概念。根本就沒有方法調用的語法！Python語法中，有函數調用的，也有從對象中取得域的，也有綁定方法的。把這三者結合起來，就讓x.f(y)看起來像一塊單獨的語法，而事實上，它等價於(x.f)(y)，後者又等價於(getattr(x, &lsquo;f&rsquo;))(y)。我猜你可能不相信我，來看:</p>

<pre><code>class X:
    def f(self, y): return 2 * y
    --------------------------------------
&gt;&gt;&gt; x = X()
&gt;&gt;&gt; x.f
&lt;bound method X.f of &lt;__main__.X instance at 0x009C7DB0&gt;&gt;
&gt;&gt;&gt; y = 21
&gt;&gt;&gt; x.f(y)
42
&gt;&gt;&gt; (x.f)(y)
42
&gt;&gt;&gt; (getattr(x, 'f'))(y)
42
&gt;&gt;&gt; xf = x.f
&gt;&gt;&gt; xf(y)
42
&gt;&gt;&gt; map(x.f, range(5))
[0, 2, 4, 6, 8]
</code></pre>

<p>所以這個問題的答案是:你可以在方法調用中使用*y或**y(或者其他任何你可以放在函數調用中的)，因為方法調用就是函數調用。</p>

<h3>13 Q: 你能用用0行代碼實現Python的抽象類嗎? 4行呢?</h3>

<p>Java中有一個abstract關鍵詞。你可以用它來定義一個只能繼承不能被實例化的抽象類，該類中所有的抽象方法都需要你來實現。很少有人知道在Python中，你可以用幾乎一樣的方式使用abstract。不同的是，當你想要調用一個沒有實現的方式時，你得到的是一個運行時錯誤而不是編譯錯誤。比較下面的代碼:</p>

<pre><code>## Python
class MyAbstractClass:
    def method1(self): abstract

class MyClass(MyAbstractClass): 
    pass
    --------------------------------------
&gt;&gt;&gt; MyClass().method1()
Traceback (most recent call last):
    ...
NameError: name 'abstract' is not defined
</code></pre>

<p>==============================================</p>

<pre><code>    /* Java */
public abstract class MyAbstractClass {
    public abstract void method1();
}

class MyClass extends MyAbstractClass {}
----------------------------------------------
% javac MyAbstractClass
MyAbstractClass.java:5: 
  class MyClass must be declared abstract. 
  It does not define void method1() from class MyAbstractClass.
</code></pre>

<p>別花太多時間在Python語言參考手冊裏面尋找abstract關鍵字，它根本就不在那裏。我把它加入了Python語言中，並且最美妙的是，它的實現用了0行代碼! 當你調用methord1，你會得到一個NameError錯誤，因為不存在abstract變量。(你也許會說這是欺騙，如果有人定義一個變量叫做abstract它就沒有效果了) 但是如果代碼中依賴的一個變量被人重定義的話，任何程序都難逃錯誤的命運。這裏唯一的區別就是我們依賴的是沒有定義的變量。</p>

<p>如果你願意寫abstract()替代abstract，那麽你可以定義一個函數拋出一個更有意義的NotImplementedError以取代NameError。(同樣，如果有人重定義abstract為零參數函數以外的任何東西，你還是會得到一個錯誤信息。)為了讓abstract的錯誤信息看起來舒服一點，只需去函數調用棧(stack frame)中看看誰是這個討厭的調用者:</p>

<pre><code>def abstract():
    import inspect
    caller = inspect.getouterframes(inspect.currentframe())[1][3]
    raise NotImplementedError(caller + ' must be implemented in subclass')
    ----------------------------------------------
&gt;&gt;&gt; MyDerivedClass().method1()
Traceback (most recent call last):
    ...
NotImplementedError: method1 must be implemented in subclass
</code></pre>

<h3>14 Q: 在Python中我怎麽實現枚舉類型呢?</h3>

<p>這個問題沒有一個答案，因為在Python中有好幾個答案，取決於你對枚舉的期望。如果你只是想有幾個變量，每個都有不同的整數值，你可以這樣:</p>

<pre><code>red, green, blue = range(3)
</code></pre>

<p>缺點是當你想在左邊添加一個新的變量，需要同時增加右邊的整數。不過這不算太壞，因為當你忘記的時候Python會拋出一個錯誤。如果你把枚舉隔離在類中可能更幹凈一點:</p>

<pre><code>class Colors:
    red, green, blue = range(3)
</code></pre>

<p>現在Colors.red會得到0, 並且dir(Colors)可能也能派上用場(雖然你還需要忽略__doc__和__module__兩項). 如果你想完全控制每個枚舉變量的值, 可以使用好幾個問題以前的Struct函數, 就像下面:</p>

<pre><code>Enum = Struct
Colors = Enum(red=0, green=100, blue=200)
</code></pre>

<p>盡管這些簡單的辦法通常已經夠了，可有人還想要更多。在 <a href="http://www.python.org/doc/essays/metaclasses/Enum.py">python.org</a>，<a href="http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/67107">ASPN</a>和<a href="http://www.faqts.com/knowledge_base/view.phtml/aid/4415">faqts</a>上都有枚舉類型的實現。下面是我的版本，它(幾乎)涵蓋所有人的需要，並且仍然保持合理的簡潔(一共44行，其中有22行代碼):</p>

<pre><code>class Enum:

    """創建一個可的枚舉類型, 然後給他添加變量/值對. 構造函數
    和.ints(names)方法接受變量名的列表並且將連續的整數賦予他們. 方法.strs(names)將每個變量名賦給它自己(就是說變量'v'有值'v'). 方法.vals(a=99, b=200) 讓你可以給任何變量賦任何值. "變量名列表"也可以是一個字符串, 它將被.split()分開. 方法.end()返回最大整數值加1,比如: opcode = Enum("add sub load store").vals(illegal=255)."""

    def __init__(self, names=[]): self.ints(names)

    def set(self, var, val):
    """Set var to the value val in the enum."""
        if var in vars(self).keys(): raise AttributeError("duplicate var in enum")
        if val in vars(self).values(): raise ValueError("duplicate value in enum")
        vars(self)[var] = val
        return self

    def strs(self, names):
    """Set each of the names to itself (as a string) in the enum."""
        for var in self._parse(names): self.set(var, var)
        return self

    def ints(self, names):
    """Set each of the names to the next highest int in the enum."""
        for var in self._parse(names): self.set(var, self.end())
        return self

    def vals(self, **entries):
    """Set each of var=val pairs in the enum."""
        for (var, val) in entries.items(): self.set(var, val)
        return self

    def end(self):
    """One more than the largest int value in the enum, or 0 if none."""
        try: return max([x for x in vars(self).values() if type(x)==type(0)]) + 1
        except ValueError: return 0

    def _parse(self, names):
    ### If names is a string, parse it as a list of names.
        if type(names) == type(""): return names.split()
        else: return names
</code></pre>

<p>下面是使用它的例子:</p>

<pre><code>&gt;&gt;&gt; opcodes = Enum("add sub load store").vals(illegal=255)
&gt;&gt;&gt; opcodes.add
  0
&gt;&gt;&gt; opcodes.illegal
  255
&gt;&gt;&gt; opcodes.end()
  256
&gt;&gt;&gt; dir(opcodes)
  ['add', 'illegal', 'load', 'store', 'sub']
&gt;&gt;&gt; vars(opcodes)
  {'store': 3, 'sub': 1, 'add': 0, 'illegal': 255, 'load': 2}
&gt;&gt;&gt; vars(opcodes).values()
  [3, 1, 0, 255, 2]
</code></pre>

<p>註意這些方法都是層疊(cascaded)的，在構造函數後你可以把.strs， .ints和.vals組合在一行代碼中。還要註意的dir和vals輔助使用，它們不會被任何東西幹擾, 除了你定義的變量。為了遍歷所有的枚舉值，你可以使用for x in vars(opcodes).values()。還有就是，如果你願意，可以使用非整數值來賦給枚舉變量。使用.strs和.vals方法就行了。最後，註意重復變量名和值都是一種錯誤。有時你可能想有一個重復的值(比如為了創建別名)。你可以刪掉拋出ValueError的那行，或者像這樣用:vars(opcodes)[&lsquo;first_op&rsquo;] = 0。這裏我最不喜歡的是很有可能把vals和value搞混。也許我可以給vals想一個更好的名字。</p>

<h3>15 Q: 為什麽Python中沒有&#8221;集合(Set)&ldquo;類型?</h3>

<p>當這個問題第一個發布在這裏的時候還沒有, 程序員們通常用字典來代替它. 但是在Python 2.4中有一個很好的內建<a href="http://docs.python.org/lib/types-set.html">set類型</a>。</p>

<h3>16 Q: 我能用布爾類型嗎?</h3>

<p>當這個問題第一次發布在這裏時，Python中還沒有布爾類型。現在嘛，Python 2.3以後都內建有一個<a href="http://docs.python.org/lib/node31.html">bool類型</a>。</p>

<h3>17 Q: Python中有能與(test?result:alternative)等價的操作嗎?</h3>

<p>Java和C++都有三目運算符(test?result:alternative)。Python一直拒絕它，但在將來的Python 2.5中，將允許(result if test else alternative)形式的表達式。這樣的結果是破壞了Python中表達式和語句清楚的區別，不過它是對許多人要求的妥協。</p>

<p>在Python 2.5到來前，你怎麽辦?這裏有幾個選擇:</p>

<ol>
<li>. 你可以試試[alternaticve, result][test]. 註意如果alternative和result中有遞歸調用或者昂貴的操作的話, 這個方法不太好, 因為它們兩個都會被求值. 如果test可以返回一個非布爾值, 那就下面這個</li>
<li>. [result, alternative][not test]. 這兩個的可讀性都很好.</li>
<li>. test and result or alternative 有人很習慣這樣，有人卻覺得它令人糊塗. 它只在能確認result非假後使用.</li>
<li>. (test and [result] or [alternative])[0] 避免了上面那個限制.</li>
<li>. [lambda: result, lambda: alternative][not not test]()擺脫了上面所有的限制(除了可讀性), 但別跟人家說是我告訴你這樣做的. 你甚至可以把它封裝在一個函數裏面. 公認的命名規範是, 對於模仿關鍵詞的變量, 在後面跟一個下劃線. 所以我們有:</li>
<li>. if_(test, result, lambda: alternative)
這裏我們定義</li>
</ol>


<hr />

<pre><code>def if_(test, result, alternative=None):
"If test is true, 'do' result, else alternative. 'Do' means call if callable."
    if test:
    if callable(result): result = result()
    return result
else:
    if callable(alternative): alternative = alternative()
    return alternative
--------------------------------------------------
&gt;&gt;&gt; fact = lambda n: if_(n &lt;= 1, 1, lambda: n *     fact(n-1))
&gt;&gt;&gt; fact(6)
720
</code></pre>

<ol>
<li>. 現在假定你因為某種原因, 與&#8221;if(test, &hellip;&ldquo;的語法相比, 就是更喜歡&#8221;if(test) &hellip;&rdquo;(並且, 你從來不想擺脫alternative那個部分). 你可以試試這個:</li>
</ol>


<hr />

<pre><code>def _if(test):
    return lambda alternative: \
               lambda result: \
                   [delay(result), delay(alternative)][not not test]()

def delay(f):
    if callable(f): return f
    else: return lambda: f
&gt;&gt;&gt; fact = lambda n: _if (n &lt;= 1) (1) (lambda: n *  fact(n-1))
&gt;&gt;&gt; fact(100)
93326215443944152681699238856266700490715968264381621468592963895217599993229915608941463976156518286253697920827223758251185210916864000000000000000000000000L
</code></pre>

<p>If u cn rd ths, u cn gt a jb in fncnl prg (if thr wr any)。(這個就不翻了吧:) )</p>

<h3>18 Q: 還有其他主要類型是Python缺少的嗎?</h3>

<p>關於Python，有一件很爽的事情就是你可以使用數字，字符串，列表，和字典(現在還有集合和布爾)就能走很遠。但是還有幾個主要類型是缺少的. 對我來說，最重要的是一個可變的字符串。一次又一次的使用str ＋＝ x 是很慢的，而維護字符組成的列表(或者子字符串的列表)意味著你放棄了一些很棒的字符串函數。一個可能的解決是array.array(&lsquo;c&rsquo;)。另一個是UserString.MutableString，盡管它本來的目的是用於教學而不是實踐。第三個是mmap模塊, 第四是cStringIO. 這些方法都不完美，不過加在一起也提供了足夠的選擇。最後，我發現我經常需要一個某種順序的隊列。標準庫中有一個<a href="http://www.python.org/doc/current/lib/module-Queue.html">Queue module</a>，但它是專用於線程的隊列。因為這裏有太多選項了，所以我就不為了實現一個標準隊列的去遊說了。不過呢，我將提供我實現的幾種隊列，FIFO，LIFO和優先隊列:</p>

<pre><code>"""
This module provides three types of queues, with these constructors:
  Stack([items])  -- Create a Last In First Out queue, implemented as a list
  Queue([items])  -- Create a First In First Out queue
  PriorityQueue([items]) -- Create a queue where minimum item (by &lt;) is first
Here [items] is an optional list of initial items; if omitted, queue is empty.
Each type supports the following methods and functions:
  len(q)          -- number of items in q (also q.__len__())
  q.append(item)  -- add an item to the queue
  q.extend(items) -- add each of the items to the queue
  q.pop()         -- remove and return the "first" item from the queue
"""

def Stack(items=None):
    "A stack, or last-in-first-out queue, is    implemented as a list."
    return items or []

class Queue:
    "A first-in-first-out queue."
    def __init__(self, items=None): self.start = 0;     self.A = items or []
    def __len__(self):                return    len(self.A) - self.start
    def append(self, item):             self.A.append(item)
    def extend(self, items):            self.A.extend(items)

    def pop(self):
        A = self.A
        item = A[self.start]
        self.start += 1
        if self.start &gt; 100 and self.start &gt; len(A)/2:
            del A[:self.start]
            self.start = 0
        return item

class PriorityQueue:
    "A queue in which the minimum element (as determined by cmp) is first."
    def __init__(self, items=None, cmp=operator.lt):
          self.A = []; self.cmp = cmp;
          if items: self.extend(items)

    def __len__(self): return len(self.A)

    def append(self, item):
        A, cmp = self.A, self.cmp
        A.append(item)
        i = len(A) - 1
        while i &gt; 0 and cmp(item, A[i//2]):
            A[i], i = A[i//2], i//2
        A[i] = item

    def extend(self, items):
        for item in items: self.append(item)

    def pop(self):
        A = self.A
        if len(A) == 1: return A.pop()
        e = A[0]
        A[0] = A.pop()
        self.heapify(0)
        return e

    def heapify(self, i):
        "Assumes A is an array whose left and right children are heaps,"
        "move A[i] into the correct position.  See CLR&amp;S p. 130"
        A, cmp = self.A, self.cmp
        left, right, N = 2*i + 1, 2*i + 2, len(A)-1
        if left &lt;= N and cmp(A[left], A[i]):
            smallest = left
        else:
            smallest = i
        if right &lt;= N and cmp(A[right], A[smallest]):
            smallest = right
        if smallest != i:
            A[i], A[smallest] = A[smallest], A[i]
            self.heapify(smallest)
</code></pre>

<p>註意一個技巧&#8221;items or []&ldquo;，下面這樣做是非常錯誤的</p>

<pre><code>def Stack(items=[]): return items
</code></pre>

<p>這是想說明默認值是一個空的列表。如果我們這樣作了，那麽不同的堆棧將會共享一個列表。通過使默認值為None(一個有效輸入之外的false值)，我們可以安排每個實例得到它自己的新列表。可能拒絕使用這個技巧的理由，在下面例子中，一個用戶這樣用</p>

<pre><code>s = Stack(items)
</code></pre>

<p>他可能覺得之後的s和items應該是相同的。但這是只會在發生在當items非空的時候。我認為這樣的反對理由是不太嚴重的，因為這裏並沒有什麽明確的承諾。(事實上，一個用戶也可能期望items保持不變，這只在item為空時候成立)。</p>

<h3>19 Q: 在Python裏面怎麽實現Singleton模式?</h3>

<p>我假定你的意思是：你希望一個類只可以被實例化一次，然後當你再次實例化時拋出一個異常。我知道的最簡單的辦法是定義一個函數施行這個想法，然後在你的類構造函數裏面調用這個函數:</p>

<pre><code>def singleton(object, instantiated=[]):
    "Raise an exception if an object of this class has been instantiated before."
    assert object.__class__ not in instantiated, \
        "%s is a Singleton class but is already instantiated" % object.__class__
    instantiated.append(object.__class__)

class YourClass:
    "A singleton class to do something ..."
    def __init__(self, args):
        singleton(self)
        ...
</code></pre>

<p>你也可以跟metaclass打交道，這樣你可以寫出class YourClass(Singletion)，但是為什麽自找麻煩呢?在&#8221;四人幫&#8221;把理論帶給我們以前，&#8221;singleton&#8221;(沒有那個公式化的名字)只是一個簡單的想法，剛好與一行簡單代碼相配，而不是一套信仰.</p>

<h3>20 Q: 沒有&#8221;news&#8221;是好消息嗎?</h3>

<p>我假設你的意思是Python沒有new關鍵字。的確是的。在C++中，new用來標記堆的分配而不是棧的。這時，這個關鍵字是有用的。在Java中，所有的對象都是在堆上分配的，所以new沒有真正的意義。它只是作為一個區別構造函數和其他靜態方法的提醒。但是這個區別可能對Java弊大於利，因為它是低層次的，它強迫實現代碼過早決定那些真正應該延後的東西。我想Python作出了正確的選擇，保持構造函數和一個普通函數調用使用相同的語法。</p>

<p>比如說，在有bool類出現之前，我們曾經想實現一個。為了跟內建的有所區別的，我們就叫它Bool。假設我們想實現這樣的想法:Bool類型只有一個true和一個false對象。一個辦法是把類名從Bool改為_Bool(這樣它不會被導出)，然後定義一個函數Bool:</p>

<pre><code>def Bool(val):
    if val: return true
    else: return false

true, false = _Bool(1), _Bool(0)
</code></pre>

<p>這就讓函數Bool變成_Bool對象的一個工廠(誠然是一個小得少見的工廠)。要點在於調用Bool(1)的程序員不應該知道或者關心返回的對象是一個新的還是回收的(至少對於不可變對象是這樣)。Python語法允許隱藏這個區別，但是Java語法不行。</p>

<p>在一些著作中這裏有點混淆。有些人使用術語&#8221;Singleton Pattern&#8221;稱呼這樣的工廠，因為這裏對構造函數的每個不同的參數有一個單獨的對象。和大多數人一樣，我贊同前一個問題中我下的定義。這個模式也可以封裝一個類型。我們可以叫它&#8221;CachedFactory&#8221;。這個想法來源於當你寫下</p>

<pre><code>class Bool:
    ... ## see here for Bool's definition

Bool = CachedFactory(Bool)
</code></pre>

<p>然後當你第一次調用Bool(1)，參數列表(1,)，得到原來的Bool類的代理。但是任何後續的對Bool(1)調用將返回第一個對象，它是被保存在緩存中：</p>

<pre><code>class CachedFactory:
    def __init__(self, klass):
        self.cache = {}
        self.klass = klass

    def __call__(self, *args):
        if self.cache.has_key(args):
            return self.cache[args]
        else:
            object = self.cache[args] = self.klass(*args)
            return object
</code></pre>

<p>需要註意的一件事情是，類和構造函數沒有任何其余的東西。這個模式將適用於所有可調用的對象。當擴展到普通的函數，它被稱作&#8221;Memoization Pattern&#8221;。實現代碼是一樣的，只是名字變了:</p>

<pre><code>class Memoize:
    def __init__(self, fn):
        self.cache = {}
        self.fn = fn

    def __call__(self, *args):
        if self.cache.has_key(args):
            return self.cache[args]
        else:
            object = self.cache[args] = self.fn(*args)
            return object
</code></pre>

<p>現在你可以寫下fact = Memoize(fact)，現在階乘運算的時間復雜度是分攤到每次調用的O(1)，而不是O(n)。</p>

<h3>21 Q: 我能有一個像shell裏面一樣的歷史記錄嗎?</h3>

<p>能。如果你要是這個麽?</p>

<pre><code>&gt;&gt;&gt; from shellhistory import h
h[2] &gt;&gt;&gt; 7*8
56
h[3] &gt;&gt;&gt; 9*9
81
h[4] &gt;&gt;&gt; h[2]
56
h[5] &gt;&gt;&gt; 'hello' + ' world'
'hello world'
h[6] &gt;&gt;&gt; h
[None, 9, 56, 81, 56, 'hello world']
h[7] &gt;&gt;&gt; h[5] * 2
'hello worldhello world'
h[8] &gt;&gt;&gt;  h[7] is _ is h[-1]
1
</code></pre>

<p>這是怎辦到的?變量sys.ps1是系統提示符，默認值是字符串&#8217;>>>&lsquo;，但是你可以設置成其它任何東西。如果你設置了一個非字符串對象，這個對象的__str__方法將被調用。所以我們將創建這麽一個對象，它的字符串方法把最近的結果(變量_)添加到一個叫h(代表history)的列表中, 然後返回一個包含列表長度，接著是&rsquo;>>>&lsquo;的提示字符串。至少原來計劃是這樣。結果是(在IDLE 2.2的Windows實現中)，sys.ps1.__str__被調用了三次，而不是提示符被打印前的一次。別問我為什麽。為了解決這個問題，只有當_不是歷史列表中最後一個元素時，我才加入它。而且我也不自討麻煩的把None加入歷史列表中了，因為它不會被Python的交互循環顯示。我還排除了向h自己中添加h，因為這樣的環形結構可以能會帶來打印和比較時的麻煩。另一個復雜因素是Python解釋器實際上是嘗試打印&rsquo;\n&#8217; + sys.ps1，(它本來應該單獨的打印&#8217;\n&#8217;，或者打印&#8217;\n&#8217; + str(sys.ps1))這就意味著sys.ps1也需要一個__radd__方法. 最後，如果Python session中(或者是在.python啟動文件中)一開始的輸入是導入我的第一版模塊，它將會失敗。在檢查了一番之後，我發現這是因為直到第一個表達式被求值以後，變量_才被綁定。所以我捕獲了_未綁定的異常。然後就有:</p>

<pre><code>import sys

h = [None]

class Prompt:
    "Create a prompt that stores results (i.e. _) in the array h."
    def __init__(self, str='h[%d] &gt;&gt;&gt; '):
        self.str = str;

    def __str__(self):
        try:
            if _ not in [h[-1], None, h]: h.append(_);
        except NameError:
            pass
        return self.str % len(h);

    def __radd__(self, other):
        return str(other) + str(self)

sys.ps1 = Prompt()
</code></pre>

<h3>22 Q: 怎麽得到我的函數的執行時間?</h3>

<p>下面是一個簡單的答案:</p>

<pre><code>def timer(fn, *args):
    "Time the application of fn to args. Return (result, seconds)."
    import time
    start = time.clock()
    return fn(*args), time.clock() - start
&gt;&gt;&gt;timer(max, range(1e6))
(999999, 0.4921875)
</code></pre>

<p>在我的utils module裏還有一個更復雜的答案。</p>

<h3>23 Q: 我的.python啟動文件是什麽樣子的?</h3>

<p>現在它是看起來像這樣，但是它已經改變了很多了:</p>

<pre><code>from __future__ import nested_scopes
import sys, os, string, time
from utils import *

################ Interactive Prompt and Debugging ################

try:
    import readline
except ImportError:
    print "Module readline not available."
else:
    import rlcompleter
    readline.parse_and_bind("tab: complete")

h = [None]

class Prompt:
    def __init__(self, str='h[%d] &gt;&gt;&gt; '):
        self.str = str;

    def __str__(self):
        try:
            if _ not in [h[-1], None, h]: h.append(_);
        except NameError:
           pass
        return self.str % len(h);

  def __radd__(self, other):
        return str(other) + str(self)


if os.environ.get('TERM') in [ 'xterm', 'vt100' ]:
    sys.ps1 = Prompt('\001\033[0:1;31m\002h[%d] &gt;&gt;&gt; \001\033[0m\002')
else:
    sys.ps1 = Prompt()
sys.ps2 = ''
</code></pre>

<p><a href="mailto:%20peter@norvig.com">Peter Norvig</a><br/>
<a href="http://norvig.com/python-iaq.html">Origin</a><br/>
<a href="http://pythonic.zoomquiet.io/data/20071017193806/index.html#11">中文翻譯</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[機器學習&amp;深度學習資料]]></title>
    <link href="http://www.aprilzephyr.com/blog/04302015/ji-qi-xue-xi-and-shen-du-xue-xi-zi-liao/"/>
    <updated>2015-04-30T15:31:50+08:00</updated>
    <id>http://www.aprilzephyr.com/blog/04302015/ji-qi-xue-xi-and-shen-du-xue-xi-zi-liao</id>
    <content type="html"><![CDATA[<p><strong><a href="http://ml.memect.com">機器學習日報__好東西傳送門</a></strong><!--more--></p>

<p>*<a href="http://www.erogol.com/brief-history-machine-learning/">《Brief History of Machine Learning》</a><br/>
介紹:這是一篇介紹機器學習歷史的文章，介紹很全面，從感知機、神經網絡、決策樹、SVM、Adaboost到隨機森林、Deep Learning.</p>

<p>*<a href="http://www.idsia.ch/%7Ejuergen/DeepLearning15May2014.pdf">《Deep Learning in Neural Networks: An Overview》</a><br/>
介紹:這是瑞士人工智能實驗室Jurgen Schmidhuber寫的最新版本《神經網絡與深度學習綜述》本綜述的特點是以時間排序，從1940年開始講起，到60-80年代，80-90年代，一直講到2000年後及最近幾年的進展。涵蓋了deep learning裏各種tricks，引用非常全面.</p>

<p>*<a href="http://machinelearningmastery.com/a-gentle-introduction-to-scikit-learn-a-python-machine-learning-library/">《A Gentle Introduction to Scikit-Learn: A Python Machine Learning Library》</a><br/>
介紹:這是一份python機器學習庫,如果您是一位python工程師而且想深入的學習機器學習.那麽這篇文章或許能夠幫助到你.</p>

<p>*<a href="http://machinelearningmastery.com/how-to-layout-and-manage-your-machine-learning-project/">《How to Layout and Manage Your Machine Learning Project》</a><br/>
介紹:這一篇介紹如果設計和管理屬於你自己的機器學習項目的文章，裏面提供了管理模版、數據管理與實踐方法.</p>

<p>*<a href="https://medium.com/code-poet/80ea3ec3c471">《Machine Learning is Fun!》</a><br/>
介紹:如果你還不知道什麽是機器學習，或則是剛剛學習感覺到很枯燥乏味。那麽推薦一讀。這篇文章已經被翻譯成中文,如果有興趣可以移步<a href="http://blog.jobbole.com/67616/">http://blog.jobbole.com/67616/</a></p>

<p>*<a href="http://cran.r-project.org/doc/contrib/Liu-R-refcard.pdf">《R語言參考卡片》</a><br/>
介紹:R語言是機器學習的主要語言,有很多的朋友想學習R語言，但是總是忘記一些函數與關鍵字的含義。那麽這篇文章或許能夠幫助到你</p>

<p>*<a href="http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/">《Choosing a Machine Learning Classifier》</a><br/>
介紹:我該如何選擇機器學習算法，這篇文章比較直觀的比較了Naive Bayes，Logistic Regression，SVM，決策樹等方法的優劣，另外討論了樣本大小、Feature與Model權衡等問題。此外還有<a href="http://www.52ml.net/15063.html">已經翻譯了的版本。</a></p>

<p>*<a href="http://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks">《An Introduction to Deep Learning: From Perceptrons to Deep Networks》</a><br/>
介紹：深度學習概述：從感知機到深度網絡，作者對於例子的選擇、理論的介紹都很到位，由淺入深。<a href="http://www.cnblogs.com/xiaowanyer/p/3701944.html">翻譯版本</a></p>

<p>*<a href="http://vdisk.weibo.com/s/ayG13we2vxyKl">《The LION Way: Machine Learning plus Intelligent Optimization》</a><br/>
介紹:&lt;機器學習與優化>這是一本機器學習的小冊子, 短短300多頁道盡機器學習的方方面面. 圖文並茂, 生動易懂, 沒有一坨坨公式的煩惱. 適合新手入門打基礎, 也適合老手溫故而知新. 比起MLAPP/PRML等大部頭, 也許這本你更需要!具體內容<a href="http://intelligent-optimization.org/LIONbook/">推薦閱讀</a></p>

<p>*<a href="http://1.guzili.sinaapp.com/?p=174">《深度學習與統計學習理論》</a><br/>
介紹:作者是來自百度，不過他本人已經在2014年4月份申請離職了。但是這篇文章很不錯如果你不知道深度學習與支持向量機/統計學習理論有什麽聯系？那麽應該立即看看這篇文章.</p>

<p>*<a href="http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-fall-2010/readings/MIT6_042JF10_notes.pdf">《計算機科學中的數學》</a><br/>
介紹:這本書是由谷歌公司和MIT共同出品的計算機科學中的數學：<a href="https://github.com/ty4z2008/Qix/blob/master/Mathematics%20for%20Computer%20Science">Mathematics for Computer Science</a>，Eric Lehman et al 2013 。分為5大部分：1）證明，歸納。2）結構，數論，圖。3）計數，求和，生成函數。4）概率，隨機行走。5）遞歸。等等</p>

<p>*<a href="http://research.microsoft.com/en-US/people/kannan/book-no-solutions-aug-21-2014.pdf">《信息時代的計算機科學理論(Foundations of Data Science)》</a><br/>
介紹：信息時代的計算機科學理論,目前國內有紙質書購買，<a href="https://itunes.apple.com/us/book/introduction-to-data-science/id529088127">iTunes購買</a></p>

<p>*<a href="http://vdisk.weibo.com/s/ayG13we2vx5qg">《Data Science with R》</a><br/>
介紹:這是一本由雪城大學新編的第二版《數據科學入門》教材：偏實用型，淺顯易懂，適合想學習R語言的同學選讀。</p>

<p>*<a href="http://www.informit.com/articles/article.aspx?p=2213858">《Twenty Questions for Donald Knuth》</a><br/>
介紹:這並不是一篇文檔或書籍。這是篇向圖靈獎得主Donald Knuth提問記錄稿： 近日， Charles Leiserson, Al Aho, Jon Bentley等大神向Knuth提出了20個問題，內容包括TAOCP，P/NP問題，圖靈機，邏輯，以及為什麽大神不用電郵等等。</p>

<p>*<a href="http://arxiv.org/pdf/1402.4304v2.pdf">《Automatic Construction and Natural-Language Description of Nonparametric Regression Models》</a><br/>
介紹：不會統計怎麽辦？不知道如何選擇合適的統計模型怎麽辦？那這篇文章你的好好讀一讀了麻省理工Joshua B. Tenenbaum和劍橋Zoubin Ghahramani合作，寫了一篇關於automatic statistician的文章。可以自動選擇回歸模型類別，還能自動寫報告&hellip;</p>

<p>*<a href="http://openreview.net/venue/iclr2014">《ICLR 2014論文集》</a><br/>
介紹:對深度學習和representation learning最新進展有興趣的同學可以了解一下</p>

<p>*<a href="http://www-nlp.stanford.edu/IR-book/">《Introduction to Information Retrieval》</a><br/>
介紹：這是一本信息檢索相關的書籍，是由斯坦福Manning與谷歌副總裁Raghavan等合著的Introduction to Information Retrieval一直是北美最受歡迎的信息檢索教材之一。最近作者增加了該課程的幻燈片和作業。<a href="http://www-nlp.stanford.edu/IR-book/information-retrieval.html">IR相關資源</a></p>

<p>*<a href="http://www.denizyuret.com/2014/02/machine-learning-in-5-pictures.html">《Machine learning in 10 pictures》</a><br/>
介紹:Deniz Yuret用10張漂亮的圖來解釋機器學習重要概念：1. Bias/Variance Tradeoff 2. Overfitting 3. Bayesian / Occam&rsquo;s razor 4. Feature combination 5. Irrelevant feature 6. Basis function 7. Discriminative / Generative 8. Loss function 9. Least squares 10. Sparsity.很清晰</p>

<p>*<a href="http://webscope.sandbox.yahoo.com/catalog.php?datatype=l">《雅虎研究院的數據集匯總》</a><br/>
介紹：雅虎研究院的數據集匯總： 包括語言類數據，圖與社交類數據，評分與分類數據，計算廣告學數據，圖像數據，競賽數據，以及系統類的數據。</p>

<p>*<a href="http://www-bcf.usc.edu/%7Egareth/ISL/">《An Introduction to Statistical Learning with Applications in R》</a><br/>
介紹：這是一本斯坦福統計學著名教授Trevor Hastie和Robert Tibshirani的新書，並且在2014年一月已經<a href="https://class.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/about">開課</a></p>

<p>*<a href="http://machinelearningmastery.com/best-machine-learning-resources-for-getting-started/">Best Machine Learning Resources for Getting Started</a><br/>
介紹：機器學習最佳入門學習資料匯總是專為機器學習初學者推薦的優質學習資源，幫助初學者快速入門。而且這篇文章的介紹已經被翻譯成<a href="http://article.yeeyan.org/view/22139/410514">中文版</a>。如果你不怎麽熟悉，那麽我建議你先看一看中文的介紹。</p>

<p>*<a href="http://blog.sina.com.cn/s/blog_bda0d2f10101fpp4.html">My deep learning reading list</a><br/>
介紹:主要是順著Bengio的PAMI review的文章找出來的。包括幾本綜述文章，將近100篇論文，各位山頭們的Presentation。全部都可以在google上找到。</p>

<p>*<a href="http://www.morganclaypool.com/doi/abs/10.2200/S00266ED1V01Y201005HLT008?journalCode=hlt">Cross-Language Information Retrieval</a><br/>
介紹：這是一本書籍，主要介紹的是跨語言信息檢索方面的知識。理論很多</p>

<p>*<a href="http://www.ibm.com/developerworks/cn/web/1103_zhaoct_recommstudy1/index.html?ca=drs-">探索推薦引擎內部的秘密，第 1 部分: 推薦引擎初探</a><br/>
<a href="http://www.ibm.com/developerworks/cn/web/1103_zhaoct_recommstudy2/index.html?ca=drs-">探索推薦引擎內部的秘密，第 2 部分: 深度推薦引擎相關算法 &ndash; 協同過濾</a><br/>
<a href="http://www.ibm.com/developerworks/cn/web/1103_zhaoct_recommstudy3/index.html?ca=drs-">探索推薦引擎內部的秘密，第 3 部分: 深度推薦引擎相關算法 &ndash; 聚類</a><br/>
介紹:本文共有三個系列，作者是來自IBM的工程師。它主要介紹了推薦引擎相關算法，並幫助讀者高效的實現這些算法。</p>

<p>*<a href="http://mimno.infosci.cornell.edu/b/articles/ml-learn/">《Advice for students of machine learning》</a><br/>
介紹：康奈爾大學信息科學系助理教授David Mimno寫的《對機器學習初學者的一點建議》， 寫的挺實際，強調實踐與理論結合，最後還引用了馮 • 諾依曼的名言: &ldquo;Young man, in mathematics you don&rsquo;t understand things. You just get used to them.&rdquo;</p>

<p>*<a href="http://web.stanford.edu/group/pdplab/pdphandbook/">分布式並行處理的數據</a><br/>
介紹：這是一本關於分布式並行處理的數據《Explorations in Parallel Distributed Processing: A Handbook of Models, Programs, and Exercises》,作者是斯坦福的James L. McClelland。著重介紹了各種神級網絡算法的分布式實現,做Distributed Deep Learning 的童鞋可以參考下</p>

<p>*<a href="http://blogs.technet.com/b/machinelearning/archive/2014/07/01/what-is-machine-learning.aspx">《“機器學習”是什麽？》</a><br/>
介紹:【“機器學習”是什麽？】John Platt是微軟研究院傑出科學家，17年來他一直在機器學習領域耕耘。近年來機器學習變得炙手可熱，Platt和同事們遂決定開設<a href="http://blogs.technet.com/b/machinelearning/">博客</a>，向公眾介紹機器學習的研究進展。機器學習是什麽，被應用在哪裏？來看Platt的<a href="http://blogs.technet.com/b/machinelearning/archive/2014/07/01/what-is-machine-learning.aspx">這篇博文</a></p>

<p>*<a href="http://icml.cc/2014/index/article/15.htm">《2014年國際機器學習大會ICML 2014 論文》</a><br/>
介紹：2014年國際機器學習大會（ICML）已經於6月21-26日在國家會議中心隆重舉辦。本次大會由微軟亞洲研究院和清華大學聯手主辦，是這個有著30多年歷史並享譽世界的機器學習領域的盛會首次來到中國，已成功吸引海內外1200多位學者的報名參與。幹貨很多，值得深入學習下</p>

<p>*<a href="http://blogs.technet.com/b/machinelearning/archive/2014/07/11/machine-learning-for-industry-a-case-study.aspx">《Machine Learning for Industry: A Case Study》</a><br/>
介紹：這篇文章主要是以Learning to Rank為例說明企業界機器學習的具體應用，RankNet對NDCG之類不敏感，加入NDCG因素後變成了LambdaRank，同樣的思想從神經網絡改為應用到Boosted Tree模型就成就了LambdaMART。<a href="http://research.microsoft.com/en-us/people/cburges/?WT.mc_id=Blog_MachLearn_General_DI">Chirs Burges</a>，微軟的機器學習大神，Yahoo 2010 Learning to Rank Challenge第一名得主，排序模型方面有RankNet，LambdaRank，LambdaMART，尤其以LambdaMART最為突出，代表論文為： <a href="http://research.microsoft.com/en-us/um/people/cburges/tech_reports/msr-tr-2010-82.pdf">From RankNet to LambdaRank to LambdaMART: An Overview</a> 此外，Burges還有很多有名的代表作，比如：<a href="http://research.microsoft.com/pubs/67119/svmtutorial.pdf">A Tutorial on Support Vector Machines for Pattern Recognition</a>,<a href="http://research.microsoft.com/en-us/um/people/cburges/tech_reports/tr-2004-56.pdf">Some Notes on Applied Mathematics for Machine Learning</a></p>

<p>*<a href="http://meta-guide.com/software-meta-guide/100-best-github-deep-learning/">100 Best GitHub: Deep Learning</a><br/>
介紹:100 Best GitHub: Deep Learning</p>

<p>*<a href="http://www.52ml.net/12019.html">《UFLDL-斯坦福大學Andrew Ng教授“Deep Learning”教程》</a><br/>
介紹:本教程將闡述無監督特征學習和深度學習的主要觀點。通過學習，你也將實現多個功能學習/深度學習算法，能看到它們為你工作，並學習如何應用/適應這些想法到新問題上。本教程假定機器學習的基本知識（特別是熟悉的監督學習，邏輯回歸，梯度下降的想法），如果你不熟悉這些想法，我們建議你去這裏<a href="http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=MachineLearning">機器學習課程</a>，並先完成第II，III，IV章（到邏輯回歸）。此外這關於這套教程的源代碼在github上面已經有python版本了 <a href="https://github.com/jatinshah/ufldl_tutorial">UFLDL Tutorial Code</a></p>

<p>*<a href="http://research.microsoft.com/pubs/217165/ICASSP_DeepTextLearning_v07.pdf">《Deep Learning for Natural Language Processing and Related Applications》</a><br/>
介紹:這份文檔來自微軟研究院,精髓很多。如果需要完全理解，需要一定的機器學習基礎。不過有些地方會讓人眼前一亮,毛塞頓開。</p>

<p>*<a href="https://colah.github.io/posts/2014-07-Understanding-Convolutions/">Understanding Convolutions</a><br/>
介紹:這是一篇介紹圖像卷積運算的文章，講的已經算比較詳細的了</p>

<p>*<a href="http://mlss2014.com/">《Machine Learning Summer School》</a><br/>
介紹：<a href="https://www.youtube.com/user/smolix">每天請一個大牛來講座，主要涉及機器學習，大數據分析，並行計算以及人腦研究</a></p>

<p>*<a href="https://github.com/josephmisiti/awesome-machine-learning">《Awesome Machine Learning》</a><br/>
介紹：一個超級完整的機器學習開源庫總結，如果你認為這個碉堡了，那後面這個列表會更讓你驚訝：【Awesome Awesomeness】,國內已經有熱心的朋友進行了翻譯<a href="http://blog.jobbole.com/73806/">中文介紹</a>，<a href="https://github.com/josephmisiti/awesome-machine-learning/blob/master/books.md">機器學習數據挖掘免費電子書</a></p>

<p>*<a href="http://see.stanford.edu/see/lecturelist.aspx?coll=63480b48-8819-4efd-8412-263f1a472f5a">斯坦福《自然語言處理》課程視頻</a><br/>
介紹:ACL候任主席、斯坦福大學計算機系Chris Manning教授的《自然語言處理》課程所有視頻已經可以在斯坦福公開課網站上觀看了（如Chrome不行，可用IE觀看） 作業與測驗也可以下載。</p>

<p>*<a href="http://freemind.pluskid.org/machine-learning/deep-learning-and-shallow-learning/">《Deep Learning and Shallow Learning》</a><br/>
介紹:對比 Deep Learning 和 Shallow Learning 的好文，來著浙大畢業、MIT 讀博的 Chiyuan Zhang 的博客。</p>

<p>*<a href="http://benanne.github.io/2014/08/05/spotify-cnns.html">《Recommending music on Spotify with deep learning》</a><br/>
介紹:利用卷積神經網絡做音樂推薦。</p>

<p>*<a href="http://neuralnetworksanddeeplearning.com/index.html">《Neural Networks and Deep Learning》</a><br/>
介紹：神經網絡的免費在線書，已經寫了三章了，還有<a href="https://github.com/mnielsen/neural-networks-and-deep-learning">對應的開源代碼</a>，愛好者的福音。</p>

<p>*<a href="http://machinelearningmastery.com/java-machine-learning/">《Java Machine Learning》</a><br/>
介紹：Java機器學習相關平臺和開源的機器學習庫，按照大數據、NLP、計算機視覺和Deep Learning分類進行了整理。看起來挺全的，Java愛好者值得收藏。</p>

<p>*<a href="http://www.oschina.net/translate/6-tips-for-writing-better-code">《Machine Learning Theory: An Introductory Primer》</a><br/>
介紹：機器學習最基本的入門文章，適合零基礎者</p>

<p>*<a href="http://www.ctocio.com/hotnews/15919.html">《機器學習常見算法分類匯總》</a><br/>
介紹：機器學習的算法很多。很多時候困惑人們都是，很多算法是一類算法，而有些算法又是從其他算法中延伸出來的。這裏，我們從兩個方面來給大家介紹，第一個方面是學習的方式，第二個方面是算法的類似性。</p>

<p>*<a href="http://suanfazu.com/discussion/68/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87survey%E5%90%88%E9%9B%86">《機器學習經典論文/survey合集》</a><br/>
介紹：看題目你已經知道了是什麽內容,沒錯。裏面有很多經典的機器學習論文值得仔細與反復的閱讀。</p>

<p>*<a href="http://work.caltech.edu/library/">《機器學習視頻庫》</a><br/>
介紹：視頻由加州理工學院（Caltech）出品。需要英語底子。</p>

<p>*<a href="http://suanfazu.com/discussion/109/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BB%8F%E5%85%B8%E4%B9%A6%E7%B1%8D">機器學習經典書籍</a><br/>
介紹：總結了機器學習的經典書籍，包括數學基礎和算法理論的書籍，可做為入門參考書單。</p>

<p>*<a href="http://efytimes.com/e1/fullnews.asp?edid=121516">16 Free eBooks On Machine Learning</a><br/>
介紹:16本機器學習的電子書，可以下載下來在pad，手機上面任意時刻去閱讀。不多我建議你看完一本再下載一本。</p>

<p>*<a href="http://www.erogol.com/large-set-machine-learning-resources-beginners-mavens/">《A Large set of Machine Learning Resources for Beginners to Mavens》</a><br/>
介紹:標題很大，從新手到專家。不過看完上面所有資料。肯定是專家了</p>

<p>*<a href="http://article.yeeyan.org/view/22139/410514">機器學習最佳入門學習資料匯總</a><br/>
介紹：入門的書真的很多，而且我已經幫你找齊了。</p>

<p>*<a href="http://users.soe.ucsc.edu/%7Eniejiazhong/slides/chandra.pdf">Sibyl</a><br/>
介紹：Sibyl 是一個監督式機器學習系統，用來解決預測方面的問題，比如 YouTube 的視頻推薦。</p>

<p>*<a href="http://www.iro.umontreal.ca/%7Ebengioy/dlbook/">《Deep Learning》</a><br/>
介紹：Yoshua Bengio, Ian Goodfellow, Aaron Courville著</p>

<p>*<a href="http://www.slideshare.net/ssuser9cc1bd/piji-li-dltm">《Neural Network &amp; Text Mining》</a><br/>
介紹:關於(Deep) Neural Networks在 NLP 和 Text Mining 方面一些paper的總結</p>

<p>*<a href="http://www.cnblogs.com/lxy2017/p/3927226.html">《前景目標檢測1（總結）》</a><br/>
介紹:計算機視覺入門之前景目標檢測1（總結）</p>

<p>*<a href="http://www.52ml.net/17004.html">《行人檢測》</a><br/>
介紹:計算機視覺入門之行人檢測</p>

<p>*<a href="http://www.kdnuggets.com/2014/08/deep-learning-important-resources-learning-understanding.html">《Deep Learning – important resources for learning and understanding》</a><br/>
介紹:Important resources for learning and understanding . Is awesome</p>

<p>*<a href="http://www.toptal.com/machine-learning/machine-learning-theory-an-introductory-primer">《Machine Learning Theory: An Introductory Primer》</a><br/>
介紹:這又是一篇機器學習初學者的入門文章。值得一讀</p>

<p>*<a href="http://neuralnetworksanddeeplearning.com/">《Neural Networks and Deep Learning》</a><br/>
介紹:在線Neural Networks and Deep Learning電子書</p>

<p>*<a href="http://www.52nlp.cn/python-%E7%BD%91%E9%A1%B5%E7%88%AC%E8%99%AB-%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86-%E7%A7%91%E5%AD%A6%E8%AE%A1%E7%AE%97-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98">《Python 網頁爬蟲 &amp; 文本處理 &amp; 科學計算 &amp; 機器學習 &amp; 數據挖掘兵器譜》</a><br/>
介紹:python的17個關於機器學習的工具</p>

<p>*<a href="http://www.flickering.cn/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/2014/06/%E7%A5%9E%E5%A5%87%E7%9A%84%E4%BC%BD%E7%8E%9B%E5%87%BD%E6%95%B0%E4%B8%8A/">《神奇的伽瑪函數(上)》</a><br/>
<a href="http://www.flickering.cn/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/2014/06/%E7%A5%9E%E5%A5%87%E7%9A%84%E4%BC%BD%E7%8E%9B%E5%87%BD%E6%95%B0%E4%B8%8A/">神奇的伽瑪函數(下)</a></p>

<p>*<a href="http://cxwangyi.github.io/2014/01/20/distributed-machine-learning/">《分布式機器學習的故事》</a><br/>
介紹:作者王益目前是騰訊廣告算法總監，王益博士畢業後在google任研究。這篇文章王益博士7年來從谷歌到騰訊對於分布機器學習的所見所聞。值得細讀</p>

<p>*<a href="http://metacademy.org/roadmaps/cjrd/level-up-your-ml">《機器學習提升之道（Level-Up Your Machine Learning）》</a><br/>
介紹:把機器學習提升的級別分為0~4級，每級需要學習的教材和掌握的知識。這樣，給機器學習者提供一個上進的路線圖，以免走彎路。另外，整個網站都是關於機器學習的，資源很豐富。</p>

<p>*<a href="http://www.mlsurveys.com/">Machine Learning Surveys</a><br/>
介紹:機器學習各個方向綜述的網站</p>

<p>*<a href="http://deeplearning.net/reading-list/">Deep Learning Reading list</a><br/>
介紹:深度學習閱資源列表</p>

<p>*<a href="http://research.microsoft.com/pubs/219984/DeepLearningBook_RefsByLastFirstNames.pdf">《Deep Learning: Methods and Applications》</a><br/>
介紹：這是一本來自微的研究員 li Peng和Dong Yu所著的關於深度學習的方法和應用的電子書</p>

<p>*<a href="http://pan.baidu.com/s/1pJ0ok7T">《Machine Learning Summer School 2014》</a><br/>
介紹:2014年七月CMU舉辦的機器學習夏季課剛剛結束 有近50小時的視頻、十多個PDF版幻燈片，覆蓋 深度學習，貝葉斯，分布式機器學習，伸縮性 等熱點話題。所有13名講師都是牛人：包括大牛Tom Mitchell （他的［機器學習］是名校的常用教材），還有CMU李沐 .（1080P高清喲）</p>

<p>*<a href="http://users.soe.ucsc.edu/%7Eniejiazhong/slides/chandra.pdf">《Sibyl: 來自Google的大規模機器學習系統》</a><br/>
介紹:在今年的IEEE/IFIP可靠系統和網絡（DSN）國際會議上，Google軟件工程師Tushar Chandra做了一個關於Sibyl系統的主題演講。 Sibyl是一個監督式機器學習系統，用來解決預測方面的問題，比如YouTube的視頻推薦。詳情請閱讀<a href="http://www.infoq.com/cn/news/2014/07/google-sibyl">google sibyl</a></p>

<p>*<a href="http://googleresearch.blogspot.com/2014/09/building-deeper-understanding-of-images.html">《Building a deeper understanding of images》</a><br/>
介紹:谷歌研究院的Christian Szegedy在谷歌研究院的博客上簡要地介紹了他們今年參加ImageNet取得好成績的GoogLeNet系統.是關於圖像處理的。</p>

<p>*<a href="https://github.com/memect/hao/blob/master/awesome/bayesian-network-python.md">《Bayesian network 與python概率編程實戰入門》</a><br/>
介紹:貝葉斯學習。如果不是很清可看看<a href="http://www.infoq.com/cn/news/2014/07/programming-language-bayes">概率編程語言與貝葉斯方法實踐</a></p>

<p>*<a href="http://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan/">《AMA: Michael I Jordan》</a><br/>
介紹:網友問伯克利機器學習大牛、美國雙料院士Michael I. Jordan：&#8221;如果你有10億美金，你怎麽花？Jordan: &ldquo;我會用這10億美金建造一個NASA級別的自然語言處理研究項目。&rdquo;</p>

<p>*<a href="http://www.cnblogs.com/tornadomeet/p/3395593.html">《機器學習&amp;數據挖掘筆記_16（常見面試之機器學習算法思想簡單梳理）》</a><br/>
介紹:常見面試之機器學習算法思想簡單梳理,此外作者還有一些其他的<a href="http://www.cnblogs.com/tornadomeet/tag/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">機器學習與數據挖掘文章</a>和<a href="http://www.cnblogs.com/tornadomeet/tag/Deep%E3%80%80Learning/">深度學習文章</a>,不僅是理論還有源碼。</p>

<p>*<a href="http://www.kdnuggets.com/2014/09/most-viewed-web-mining-lectures-videolectures.html">《文本與數據挖掘視頻匯總》</a><br/>
介紹：Videolectures上最受歡迎的25個文本與數據挖掘視頻匯總</p>

<p>*<a href="http://timdettmers.wordpress.com/2014/08/14/which-gpu-for-deep-learning/">《怎麽選擇深度學習的GPUs》</a><br/>
介紹:<a href="http://t.cn/RhpuD1G">在Kaggle上經常取得不錯成績的Tim Dettmers介紹了他自己是怎麽選擇深度學習的GPUs, 以及個人如何構建深度學習的GPU集群:</a></p>

<p>*<a href="http://www.infoq.com/cn/news/2014/09/depth-model">《對話機器學習大神Michael Jordan：深度模型》</a><br/>
介紹:對話機器學習大神Michael Jordan</p>

<p>*<a href="http://blog.sina.com.cn/s/blog_46d0a3930101fswl.html">《Deep Learning 和 Knowledge Graph 引爆大數據革命》</a><br/>
介紹:還有<a href="http://blog.sina.com.cn/s/blog_46d0a3930101gs5h.html">２，３部分</a></p>

<p>*<a href="http://blog.sina.com.cn/s/blog_46d0a3930101h6nf.html">《Deep Learning 教程翻譯》</a><br/>
介紹:是Stanford 教授 Andrew Ng 的 Deep Learning 教程，國內的機器學習愛好者很熱心的把這個教程翻譯成了中文。如果你英語不好，可以看看這個</p>

<p>*<a href="http://markus.com/deep-learning-101/">《Deep Learning 101》</a><br/>
介紹:因為近兩年來，深度學習在媒體界被炒作很厲害（就像大數據）。其實很多人都還不知道什麽是深度學習。這篇文章由淺入深。告訴你深度學究竟是什麽！</p>

<p>*<a href="http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial">《UFLDL Tutorial》</a><br/>
介紹:這是斯坦福大學做的一免費課程（很勉強），這個可以給你在深度學習的路上給你一個學習的思路。裏面提到了一些基本的算法。而且告訴你如何去應用到實際環境中。<a href="http://ufldl.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B">中文版</a></p>

<p>*<a href="http://deeplearning.cs.toronto.edu/">《Toronto Deep Learning Demos》</a><br/>
介紹:這是多倫多大學做的一個深度學習用來識別圖片標簽／圖轉文字的demo。是一個實際應用案例。有源碼</p>

<p>*<a href="http://metacademy.org/roadmaps/rgrosse/deep_learning">《Deep learning from the bottom up》</a><br/>
介紹:機器學習模型，閱讀這個內容需要有一定的基礎。</p>

<p>*<a href="http://cran.r-project.org/web/views/">《R工具包的分類匯總》</a><br/>
介紹: (CRAN Task Views, 34種常見任務,每個任務又各自分類列舉若幹常用相關工具包) 例如: 機器學習，自然語言處理，時間序列分析，空間信息分析，多重變量分析，計量經濟學，心理統計學，社會學統計，化學計量學，環境科學，藥物代謝動力學 等</p>

<p>*<a href="http://www.ctocio.com/hotnews/15919.html">《機器學習常見算法分類匯總》</a><br/>
介紹: 機器學習無疑是當前數據分析領域的一個熱點內容。很多人在平時的工作中都或多或少會用到機器學習的算法。本文為您總結一下常見的機器學習算法，以供您在工作和學習中參考.</p>

<p>*<a href="http://blog.csdn.net/zouxy09/article/details/8775360">《Deep Learning（深度學習）學習筆記整理系列》</a><br/>
介紹: 很多幹貨，而且作者還總結了好幾個系列。另外還作者還了一個<a href="http://blog.csdn.net/zouxy09/article/details/14222605">文章導航</a>.非常的感謝作者總結。</p>

<p>*<a href="http://research.microsoft.com/apps/video/default.aspx?id=206976&amp;l=i">《Tutorials Session A &ndash; Deep Learning for Computer Vision》</a><br/>
介紹:傳送理由：Rob Fergus的用深度學習做計算機是覺的NIPS 2013教程。有mp4, mp3, pdf各種<a href="http://msrvideo.vo.msecnd.net/rmcvideos/206976/dl/206976.pdf">下載</a>他是紐約大學教授，目前也在Facebook工作，他2014年的8篇<a href="http://cs.nyu.edu/%7Efergus/pmwiki/pmwiki.php?n=PmWiki.Publications">論文</a></p>

<p>*<a href="https://github.com/xpqiu/fnlp/">《FudanNLP》</a><br/>
介紹:FudanNLP，這是一個復旦大學計算機學院開發的開源中文自然語言處理（NLP）工具包 Fudan NLP裏包含中文分詞、關鍵詞抽取、命名實體識別、詞性標註、時間詞抽取、語法分析等功能，對搜索引擎 文本分析等極為有價值。</p>

<p>*<a href="http://engineering.linkedin.com/large-scale-machine-learning/open-sourcing-ml-ease">《Open Sourcing ml-ease》</a><br/>
介紹:LinkedIn 開源的機器學習工具包,支持單機, Hadoop cluster，和 Spark cluster 重點是 logistic regression 算法</p>

<p>*<a href="http://ztl2004.github.io/MachineLearningWeekly/index.html">《機器學習周刊》</a><br/>
介紹:對於英語不好，但又很想學習機器學習的朋友。是一個大的福利。機器學習周刊目前主要提供中文版，還是面向廣大國內愛好者，內容涉及機器學習、數據挖掘、並行系統、圖像識別、人工智能、機器人等等。謝謝作者</p>

<p>*<a href="http://v.163.com/special/opencourse/daishu.html">《線性代數》</a><br/>
介紹：《線性代數》是《機器學習》的重要數學先導課程。其實《線代》這門課講得淺顯易懂特別不容易，如果一上來就講逆序數及羅列行列式性質，很容易讓學生失去學習的興趣。我個人推薦的最佳《線性代數》課程是麻省理工Gilbert Strang教授的課程。 <a href="http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/">課程主頁</a></p>

<p>*<a href="http://blog.andreamostosi.name/big-data/">《Big-data》</a><br/>
介紹:大數據數據處理資源、工具不完備列表，從框架、分布式編程、分布式文件系統、鍵值數據模型、圖數據模型、數據可視化、列存儲、機器學習等。很贊的資源匯總。</p>

<p>*<a href="http://yahoolabs.tumblr.com/post/97839313996/machine-learning-for-smart-dummies">《machine learning for smart dummies》</a><br/>
介紹:雅虎邀請了一名來自本古裏安大學的訪問學者，制作了一套關於機器學習的系列視頻課程。本課程共分為7期，詳細講解了有關SVM, boosting, nearest neighbors, decision trees 等常規機器學習算法的理論基礎知識。</p>

<p>*<a href="http://arxiv.org/abs/1409.7770">《Entanglement-Based Quantum Machine Learning》</a><br/>
介紹:應對大數據時代，量子機器學習的第一個實驗<a href="http://arxiv-web3.library.cornell.edu/pdf/1409.7770.pdf">paper下載</a></p>

<p>*<a href="http://www.wired.com/2014/01/how-to-hack-okcupid/all/">《How a Math Genius Hacked OkCupid to Find True Love》</a><br/>
介紹:Wired雜誌報道了UCLA數學博士Chris McKinlay （圖1）通過大數據手段+機器學習方法破解婚戀網站配對算法找到真愛的故事,通過Python腳本控制著12個賬號，下載了婚戀網站2萬女用戶的600萬問題答案，對他們進行了統計抽樣及聚類分析（圖2，3），最後終於收獲了真愛。科技改變命運！</p>

<p>*<a href="https://www.edx.org/course/mitx/mitx-6-832x-underactuated-robotics-3511">《Underactuated Robotics》</a><br/>
介紹:MIT的Underactuated Robotics於 2014年10月1日開課，該課屬於MIT研究生級別的課程，對機器人和非線性動力系統感興趣的朋友不妨可以挑戰一下這門課程！</p>

<p>*<a href="http://yanbohappy.sinaapp.com/?p=498">《mllib實踐經驗(1)》</a><br/>
介紹:mllib實踐經驗分享</p>

<p>*<a href="http://www.seobythesea.com/2014/09/google-turns-deep-learning-classification-fight-web-spam/">《Google Turns To Deep Learning Classification To Fight Web Spam》</a><br/>
介紹:Google用Deep Learning做的antispam(反垃圾郵件)</p>

<p>*<a href="https://github.com/memect/hao/blob/master/awesome/nlp.md">NLP常用信息資源</a><br/>
介紹:NLP常用信息資源</p>

<p>*<a href="https://github.com/soulmachine/machine-learning-cheat-sheet">《機器學習速查表》</a><br/>
介紹:機器學習速查表</p>

<p>*<a href="http://arnetminer.org/conferencebestpapers">《Best Papers vs. Top Cited Papers in Computer Science》</a><br/>
介紹：從1996年開始在計算機科學的論文中被引用次數最多的論文</p>

<p>*<a href="http://mmcheng.net/zh/itam/">《InfiniTAM: 基於深度圖像的體數據集成框架》</a><br/>
介紹：把今年的一個ACM Trans. on Graphics (TOG)論文中的代碼整理為一個開源的算法框架，共享出來了。歡迎大家使用。可以實時的采集3D數據、重建出三維模型。Online learning，GPU Random forest，GPU CRF也會後續公開。</p>

<p>*<a href="http://karpathy.github.io/neuralnets/">《Hacker&rsquo;s guide to Neural Networks》</a><br/>
介紹：【神經網絡黑客指南】現在，最火莫過於深度學習（Deep Learning），怎樣更好學習它？可以讓你在瀏覽器中，跑起深度學習效果的超酷開源項目convnetjs作者karpathy告訴你，最佳技巧是，當你開始寫代碼，一切將變得清晰。他剛發布了一本圖書，不斷在線更新</p>

<p>*<a href="http://machinelearningmastery.com/building-a-production-machine-learning-infrastructure/">《Building a Production Machine Learning Infrastructure》</a><br/>
介紹：前Google廣告系統工程師Josh Wills 講述工業界和學術界機器學習的異同,大實話</p>

<p>*<a href="http://neo4j.com/blog/deep-learning-sentiment-analysis-movie-reviews-using-neo4j/">《Deep Learning Sentiment Analysis for Movie Reviews using Neo4j》</a><br/>
介紹：使用<a href="http://www.neo4j.org/">Neo4j</a>做電影評論的情感分析。</p>

<p>*<a href="http://memkite.com/deep-learning-bibliography/">《DeepLearning.University – An Annotated Deep Learning Bibliography》</a><br/>
介紹：不僅是資料，而且還對有些資料做了註釋。</p>

<p>*<a href="http://www.datarobot.com/blog/a-primer-on-deep-learning/">《A primer on deeping learning》</a><br/>
介紹：深度學習入門的初級讀本</p>

<p>*<a href="https://news.ycombinator.com/item?id=8379571">《Machine learning is teaching us the secret to teaching 》</a><br/>
介紹：機器學習教會了我們什麽？</p>

<p>*<a href="http://scikit-learn.org/stable/documentation.html">《scikit-learn：用於機器學習的Python模塊</a><br/>
介紹：scikit-learn是在SciPy基礎上構建的用於機器學習的Python模塊。</p>

<p>*<a href="http://www.infoq.com/cn/news/2014/10/interview-michael-jordan">《對話機器學習大神Michael Jordan：解析領域中各類模型》</a><br/>
介紹：喬丹教授（Michael I. Jordan）教授是機器學習領域神經網絡的大牛，他對深度學習、神經網絡有著很濃厚的興趣。因此，很多提問的問題中包含了機器學習領域的各類模型，喬丹教授對此一一做了解釋和展望。</p>

<p>*<a href="http://www.redblobgames.com/pathfinding/a-star/introduction.html">《A*搜索算法的可視化短教程》</a><br/>
介紹：A*搜索是人工智能基本算法，用於高效地搜索圖中兩點的最佳路徑, 核心是 g(n)+h(n): g(n)是從起點到頂點n的實際代價，h(n)是頂點n到目標頂點的估算代價。<a href="https://github.com/memect/hao/issues/256">合集</a></p>

<p>*<a href="http://code.csdn.net/news/2822123">《基於雲的自然語言處理開源項目FudanNLP》</a><br/>
介紹：本項目利用了Microsoft Azure，可以在幾分種內完成NLP on Azure Website的部署，立即開始對FNLP各種特性的試用，或者以REST API的形式調用FNLP的語言分析功能</p>

<p>*<a href="http://www.youku.com/playlist_show/id_22935176.html">《吳立德《概率主題模型&amp;數據科學基礎》》</a><br/>
介紹：現任復旦大學首席教授、計算機軟件博士生導師。計算機科學研究所副所長.內部課程</p>

<p>*<a href="http://ml.memect.com/article/machine-learning-guide.html">機器學習入門資源不完全匯總</a><br/>
介紹：好東西的幹貨真的很多</p>

<p>*<a href="http://memkite.com/deep-learning-bibliography/">收集從2014年開始深度學習文獻</a><br/>
介紹：從硬件、圖像到健康、生物、大數據、生物信息再到量子計算等，Amund Tveit等維護了一個DeepLearning.University小項目：收集從2014年開始深度學習文獻，相信可以作為深度學習的起點,<a href="https://github.com/memkite/DeepLearningBibliography">github</a></p>

<p>*<a href="http://emnlp2014.org/papers/pdf/EMNLP2014148.pdf">EMNLP上兩篇關於股票趨勢的應用論文</a><br/>
介紹：EMNLP上兩篇關於<a href="http://emnlp2014.org/papers/pdf/EMNLP2014148.pdf">stock trend</a>用到了deep model組織特征； <a href="http://emnlp2014.org/papers/pdf/EMNLP2014120.pdf">Exploiting Social Relations and Sentiment for Stock Prediction</a>用到了stock network。</p>

<p>*<a href="http://deeplearning.net/tutorial/deeplearning.pdf">《Bengio組（蒙特利爾大學LISA組）深度學習教程》</a><br/>
介紹：作者是深度學習一線大牛Bengio組寫的教程，算法深入顯出，還有實現代碼，一步步展開。</p>

<p>*<a href="http://arxiv.org/pdf/1410.5401v1.pdf">《學習算法的Neural Turing Machine》</a><br/>
介紹：許多傳統的機器學習任務都是在學習function，不過谷歌目前有開始學習算法的趨勢。谷歌另外的這篇學習Python程序的<a href="http://arxiv.org/pdf/1410.4615v1.pdf">Learning to Execute</a>也有相似之處</p>

<p>*<a href="http://www.morganclaypool.com/doi/abs/10.2200/S00607ED2V01Y201410HLT026">《Learning to Rank for Information Retrieval and Natural Language Processing》</a><br/>
介紹：作者是華為技術有限公司，諾亞方舟實驗室，首席科學家的李航博士寫的關於信息檢索與自然語言處理的文章</p>

<p>*<a href="http://www.aclweb.org/anthology/D11-1147">《Rumor has it: Identifying Misinformation in Microblogs》</a><br/>
介紹：利用機用器學習在謠言的判別上的應用,此外還有兩個。一個是識別垃圾與虛假信息的<a href="http://digital.cs.usu.edu/%7Ekyumin/tutorial/www-tutorial.pdf">paper</a>.還有一個是<a href="http://www.datatang.com/news/details_1319.htm">網絡輿情及其分析技術</a></p>

<p>*R機器學習實踐](<a href="http://study.163.com/course/introduction/854064.htm">http://study.163.com/course/introduction/854064.htm</a>)<br/>
介紹：該課程是網易公開課的收費課程，不貴，超級便宜。主要適合於對利用R語言進行機器學習，數據挖掘感興趣的人。</p>

<p>*<a href="http://ifeve.com/bigdataanalyticsbeyondhadoop_evolutionofmlrealizaton/">《大數據分析：機器學習算法實現的演化》</a><br/>
介紹：本章中作者總結了三代機器學習算法實現的演化：第一代非分布式的， 第二代工具如Mahout和Rapidminer實現基於Hadoop的擴展，第三代如Spark和Storm實現了實時和叠代數據處理。<a href="http://ifeve.com/wp-content/uploads/2014/05/big-data-analytics-beyond-hadoop.pdf">BIG DATA ANALYTICS BEYOND HADOOP</a></p>

<p>*<a href="http://book.douban.com/subject/5921462/">《圖像處理，分析與機器視覺》</a><br/>
介紹：講計算機視覺的四部奇書（應該叫經典吧）之一，另外三本是Hartley的《多圖幾何》、Gonzalez的《數字圖像處理》、Rafael C.Gonzalez / Richard E.Woods的<a href="http://book.douban.com/subject/1106342/">《數字圖像處理》</a></p>

<p>*<a href="http://pan.baidu.com/s/1sjFeLTN">《LinkedIn最新的推薦系統文章Browsemaps》</a><br/>
介紹：裏面基本沒涉及到具體算法，但作者介紹了CF在LinkedIn的很多應用，以及他們在做推薦過程中獲得的一些經驗。最後一條經驗是應該監控log數據的質量，因為推薦的質量很依賴數據的質量！</p>

<p>*<a href="http://blog.sina.com.cn/s/blog_574a437f01019poo.html">《初學者如何查閱自然語言處理（NLP）領域學術資料》</a><br/>
介紹：初學者如何查閱自然語言處理（NLP）領域學術資料</p>

<p>*<a href="http://www.open-electronics.org/raspberry-pi-and-the-camera-pi-module-face-recognition-tutorial/">《樹莓派的人臉識別教程》</a><br/>
介紹：用樹莓派和相機模塊進行人臉識別</p>

<p>*<a href="http://www.hangli-hl.com/uploads/3/1/6/8/3168008/short_text_conversation_mla.pdf">《利用深度學習與大數據構建對話系統》</a><br/>
介紹：如何利用深度學習與大數據構建對話系統</p>

<p>*<a href="http://lear.inrialpes.fr/people/mairal/resources/pdf/review_sparse_arxiv.pdf">《經典論文Leo Breiman：Statistical Modeling: The Two Cultures》</a><br/>
介紹：Francis Bach合作的有關稀疏建模的新綜述(書)：Sparse Modeling for Image and Vision Processing，內容涉及Sparsity, Dictionary Learning, PCA, Matrix Factorization等理論，以及在圖像和視覺上的應用，而且第一部分關於Why does the l1-norm induce sparsity的解釋也很不錯。</p>

<p>*<a href="http://www.umiacs.umd.edu/%7Ehal/docs/daume04rkhs.pdf">《Reproducing Kernel Hilbert Space》</a><br/>
介紹：RKHS是機器學習中重要的概念，其在large margin分類器上的應用也是廣為熟知的。如果沒有較好的數學基礎，直接理解RKHS可能會不易。本文從基本運算空間講到Banach和Hilbert空間，深入淺出，一共才12頁。</p>

<p>*<a href="http://karpathy.github.io/neuralnets/">《Hacker&rsquo;s guide to Neural Networks》</a><br/>
介紹：許多同學對於機器學習及深度學習的困惑在於，數學方面已經大致理解了，但是動起手來卻不知道如何下手寫代碼。斯坦福深度學習博士Andrej Karpathy寫了一篇實戰版本的深度學習及機器學習教程，手把手教你用Javascript寫神經網絡和SVM.</p>

<p>*<a href="http://blog.csdn.net/pandalibaba/article/details/17409395">《【語料庫】語料庫資源匯總》</a><br/>
介紹：【語料庫】語料庫資源匯總</p>

<p>*<a href="http://blog.jobbole.com/60809/">《機器學習算法之旅》</a><br/>
介紹：本文會過一遍最流行的機器學習算法，大致了解哪些方法可用，很有幫助。</p>

<p>*<a href="http://www.csee.wvu.edu/%7Exinl/source.html">《Reproducible Research in Computational Science》</a><br/>
介紹：這個裏面有很多關於機器學習、信號處理、計算機視覺、深入學習、神經網絡等領域的大量源代碼（或可執行代碼）及相關論文。科研寫論文的好資源</p>

<p>*<a href="http://cilvr.nyu.edu/doku.php?id=deeplearning:slides:start">NYU 2014年的深度學習課程資料</a><br/>
介紹：NYU 2014年的深度學習課程資料，有視頻</p>

<p>*<a href="https://github.com/memect/hao/blob/master/awesome/computer-vision-dataset.md">《計算機視覺數據集不完全匯總》</a><br/>
介紹：計算機視覺數據集不完全匯總</p>

<p>*<a href="http://mloss.org/software/">《Machine Learning Open Source Software》</a><br/>
介紹：機器學習開源軟件</p>

<p>*<a href="http://www.csie.ntu.edu.tw/%7Ecjlin/libsvm/">《LIBSVM》</a><br/>
介紹：A Library for Support Vector Machines</p>

<p>*<a href="http://www.support-vector-machines.org/index.html">《Support Vector Machines》</a><br/>
介紹：<a href="https://github.com/ty4z2008/Qix/blob/master/files.cnblogs.com/tekson/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B9%8B%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95.doc">數據挖掘十大經典算法</a>之一</p>

<p>*<a href="http://meta-guide.com/software-meta-guide/100-best-github-deep-learning/">《100 Best GitHub: Deep Learning》</a><br/>
介紹：github上面100個非常棒的項目</p>

<p>*<a href="http://archive.ics.uci.edu/ml">《加州大學歐文分校(UCI)機器學習數據集倉庫》</a><br/>
介紹：當前加州大學歐文分校為機器學習社區維護著306個數據集。<a href="http://archive.ics.uci.edu/ml/datasets.html">查詢數據集</a></p>

<p>*<a href="http://cs.stanford.edu/people/karpathy/">Andrej Karpathy個人主頁</a><br/>
介紹：Andrej Karpathy 是斯坦福大學Li Fei-Fei的博士生，使用機器學習在圖像、視頻語義分析領域取得了科研和工程上的突破，發的文章不多，但每個都很紮實，在每一個問題上都做到了state-of-art.</p>

<p>*<a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html">《Andrej Karpathy的深度強化學習演示》</a><br/>
介紹：Andrej Karpathy的深度強化學習演示，<a href="http://arxiv.org/pdf/1312.5602v1.pdf">論文在這裏</a></p>

<p>*<a href="http://www.52nlp.cn/cikm-competition-topdata">《CIKM數據挖掘競賽奪冠算法-陳運文》</a><br/>
介紹：CIKM Cup(或者稱為CIKM Competition)是ACM CIKM舉辦的國際數據挖掘競賽的名稱。</p>

<p>*<a href="http://www.cs.toronto.edu/%7Ehinton/">Geoffrey E. Hinton</a><br/>
介紹：傑弗裏·埃弗裏斯特·辛頓 FRS是一位英國出生的計算機學家和心理學家，以其在神經網絡方面的貢獻聞名。辛頓是反向傳播算法和對比散度算法的發明人之一，也是深度學習的積極推動者.</p>

<p>*<a href="http://cikm2014.fudan.edu.cn/cikm2014/Tpl/Public/slides/CIKM14_tutorial_slides_6.pdf">《自然語言處理的深度學習理論與實際》</a><br/>
介紹：微軟研究院深度學習技術中心在CIKM2014 上關於《自然語言處理的深度學習理論與實際》教學講座的幻燈片</p>

<p>*<a href="http://eugenezhulenev.com/blog/2014/11/14/stock-price-prediction-with-big-data-and-machine-learning/">《用大數據和機器學習做股票價格預測》</a><br/>
介紹： 本文基於&lt;支持向量機的高頻限價訂單的動態建模>采用了 Apache Spark和Spark MLLib從紐約股票交易所的訂單日誌數據構建價格運動預測模型。(股票有風險，投資謹慎)<a href="https://github.com/ezhulenev/orderbook-dynamics">GitHub源代碼托管地址.</a></p>

<p>*<a href="http://dataunion.org/?p=2011">《關於機器學習的若幹理論問題》</a><br/>
介紹：徐宗本 院士將於熱愛機器學習的小夥伴一起探討有關於機器學習的幾個理論性問題，並給出一些有意義的結論。最後通過一些實例來說明這些理論問題的物理意義和實際應用價值。</p>

<p>*<a href="http://vdisk.weibo.com/s/D2szyg_bBVM0">《深度學習在自然語言處理的應用》</a><br/>
介紹：作者還著有《這就是搜索引擎：核心技術詳解》一書，主要是介紹應用層的東西</p>

<p>*<a href="http://www.cs.ubc.ca/%7Enando/340-2012/index.php">《Undergraduate machine learning at UBC》</a><br/>
介紹：機器學習課程</p>

<p>*<a href="http://blog.sina.com.cn/s/blog_6ae183910101h4jr.html">《人臉識別必讀的N篇文章》</a><br/>
介紹：人臉識別必讀文章推薦</p>

<p>*<a href="http://semocean.com/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E6%96%87%E7%8C%AE%E5%8F%8A%E8%B5%84%E6%96%99/">《推薦系統經典論文文獻及業界應用》</a><br/>
介紹：推薦系統經典論文文獻</p>

<p>*<a href="http://ocw.sjtu.edu.cn/G2S/OCW/cn/CourseDetails.htm?Id=398">《統計機器學習》</a><br/>
介紹：統計學習是關於計算機基於數據構建的概率統計模型並運用模型對數據進行預測和分析的一門科學，統計學習也成為統計機器學習。課程來自上海交通大學</p>

<p>*<a href="http://ocw.sjtu.edu.cn/G2S/OCW/cn/CourseDetails.htm?Id=397">《機器學習導論》</a><br/>
介紹：機器學習的目標是對計算機編程，以便使用樣本數據或以往的經驗來解決給定的問題.</p>

<p>*<a href="http://deeplearning.net/software_links/">人工智能和機器學習領域有趣的開源項目</a><br/>
介紹：<a href="http://code.csdn.net/news/2822818">部分中文列表</a></p>

<p>*<a href="http://blog.csdn.net/suipingsp/article/details/41645779">《機器學習經典算法詳解及Python實現&mdash;基於SMO的SVM分類器》</a><br/>
介紹:此外作者還有一篇<a href="http://blog.csdn.net/suipingsp/article/details/41722435">元算法、AdaBoost　python實現文章</a></p>

<p>*<a href="http://aria42.com/blog/2014/12/understanding-lbfgs/">《Numerical Optimization: Understanding L-BFGS》</a><br/>
介紹:加州伯克利大學博士Aria Haghighi寫了一篇超贊的數值優化博文，從牛頓法講到擬牛頓法，再講到BFGS以及L-BFGS, 圖文並茂，還有偽代碼。強烈推薦。</p>

<p>*<a href="http://www.goldencui.org/2014/12/02/%E7%AE%80%E6%98%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0%EF%BC%88%E4%B8%80%EF%BC%89/">《簡明深度學習方法概述（一）》</a><br/>
<a href="http://www.goldencui.org/2014/12/06/%E7%AE%80%E6%98%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0%EF%BC%88%E4%BA%8C%EF%BC%89/">《簡明深度學習方法概述（二）》</a></p>

<p>*<a href="http://www.johndcook.com/blog/r_language_for_programmers/">《R language for programmers》</a><br/>
介紹:Ｒ語言程序員私人定制版</p>

<p>*<a href="http://www.cheyun.com/content/news/4051">《谷歌地圖解密：大數據與機器學習的結合》</a><br/>
介紹:谷歌地圖解密</p>

<p>*<a href="http://blog.csdn.net/u012690204/article/details/41853731">《空間數據挖掘常用方法》</a><br/>
介紹:空間數據挖掘常用方法</p>

<p>*<a href="https://www.kaggle.com/c/word2vec-nlp-tutorial">《Use Google&rsquo;s Word2Vec for movie reviews》</a><br/>
介紹:Kaggle新比賽 ”When bag of words meets bags of popcorn“ aka ”邊學邊用word2vec和deep learning做NLP“ 裏面全套教程教一步一步用python和gensim包的word2vec模型，並在實際比賽裏面比調參數和清數據。 如果已裝過gensim不要忘升級</p>

<p>*<a href="http://pynlpir.readthedocs.org/en/latest/">《PyNLPIR》</a><br/>
介紹:PyNLPIR提供了NLPIR/ICTCLAS漢語分詞的Python接口,此外<a href="http://zhon.readthedocs.org/en/latest/">Zhon</a>提供了常用漢字常量，如CJK字符和偏旁，中文標點，拼音，和漢字正則表達式（如找到文本中的繁體字）</p>

<p>*<a href="http://www.technologyreview.com/view/533496/why-neural-networks-look-set-to-thrash-the-best-human-go-players-for-the-first-time/">《深度卷積神經網絡下圍棋》</a><br/>
介紹:這文章說把最近模型識別上的突破應用到圍棋軟件上，打16萬張職業棋譜訓練模型識別功能。想法不錯。訓練後目前能做到不用計算，只看棋盤就給出下一步，大約10級棋力。但這篇文章太過樂觀，說什麽人類的最後一塊堡壘馬上就要跨掉了。話說得太早。不過，如果與別的軟件結合應該還有潛力可挖。@萬精油墨綠</p>

<p>*<a href="http://mrtz.org/blog/the-nips-experiment/">《NIPS審稿實驗》</a><br/>
介紹:UT Austin教授Eric Price關於今年NIPS審稿實驗的詳細分析,他表示，根據這次實驗的結果，如果今年NIPS重新審稿的話，會有一半的論文被拒。</p>

<p>*<a href="http://www.kdnuggets.com/2014/12/top-kdnuggets-2014-analytics-big-data-science-stories.html">《2014年最佳的大數據，數據科學文章》</a><br/>
介紹:KDNuggets分別總結了2014年14個閱讀最多以及分享最多的文章。我們從中可以看到多個主題——深度學習，數據科學家職業，教育和薪酬，學習數據科學的工具比如R和Python以及大眾投票的最受歡迎的數據科學和數據挖掘語言</p>

<p>*<a href="http://blog.csdn.net/suipingsp/article/details/42101139">《機器學習經典算法詳解及Python實現&mdash;線性回歸（Linear Regression）算法》</a><br/>
介紹:Python實現線性回歸,作者還有其他很棒的文章推薦可以看看</p>

<p>*<a href="http://download.csdn.net/album/detail/1367/1/1">《2014中國大數據技術大會33位核心專家演講PDF》</a><br/>
介紹：2014中國大數據技術大會33位核心專家演講PDF下載</p>

<p>*<a href="http://arxiv.org/abs/1412.5335">《使用RNN和Paragraph Vector做情感分析》</a><br/>
介紹：這是T. Mikolov &amp; Y. Bengio最新論文Ensemble of Generative and Discriminative Techniques for Sentiment Analysis of Movie Reviews ，使用RNN和PV在情感分析效果不錯，<a href="https://github.com/mesnilgr/iclr15">項目代碼</a>公布在github(目前是空的)。這意味著Paragraph Vector終於揭開面紗了嘛。</p>

<p>*<a href="http://pan.baidu.com/s/1o6I9S18">《NLPIR/ICTCLAS2015分詞系統大會上的技術演講 》</a><br/>
介紹:NLPIR/ICTCLAS2015分詞系統發布與用戶交流大會上的演講，請更多朋友檢閱新版分詞吧。</p>

<p>*<a href="https://medium.com/code-poet/80ea3ec3c471">《Machine Learning is Fun!》</a><br/>
介紹:Convex Neural Networks 解決維數災難</p>

<p>*<a href="http://dataunion.org/?p=5395">《CNN的反向求導及練習》</a><br/>
介紹:介紹CNN參數在使用bp算法時該怎麽訓練，畢竟CNN中有卷積層和下采樣層，雖然和MLP的bp算法本質上相同，但形式上還是有些區別的，很顯然在完成CNN反向傳播前了解bp算法是必須的。此外作者也做了一個<a href="http://www.cnblogs.com/tornadomeet/archive/2012/05/24/2515980.html">資源集:機器學習，深度學習，視覺，數學等</a></p>

<p>*<a href="https://github.com/cloudflare/ahocorasick">《正則表達式優化成Trie樹 》</a><br/>
介紹:如果要在一篇文章中匹配十萬個關鍵詞怎麽辦？<a href="https://github.com/cloudflare/ahocorasick">Aho-Corasick</a>算法利用添加了返回邊的Trie樹，能夠在線性時間內完成匹配。 但如果匹配十萬個正則表達式呢 ？ 這時候可以用到把多個正則優化成Trie樹的方法，如日本人寫的<a href="http://search.cpan.org/%7Edankogai/Regexp-Trie-0.02/">Regexp::Trie</a></p>

<p>*<a href="http://jmozah.github.io/links/">《Deep learning Reading List》</a><br/>
介紹:深度學習閱讀清單</p>

<p>*<a href="http://caffe.berkeleyvision.org/">Caffe</a><br/>
介紹:Caffe是一個開源的深度學習框架，作者目前在google工作，作者主頁<a href="http://daggerfs.com/index.html">Yangqing Jia (賈揚清)</a></p>

<p>*<a href="https://github.com/BVLC/caffe/blob/master/models/bvlc_googlenet/readme.md">《GoogLeNet深度學習模型的Caffe復現》</a><br/>
介紹:2014 ImageNet冠軍GoogLeNet深度學習模型的Caffe復現模型,<a href="http://arxiv.org/abs/1409.4842">GoogleNet論文</a>.</p>

<p>*<a href="https://github.com/jbarrow/LambdaNet">《LambdaNet，Haskell實現的開源人工神經網絡庫》</a><br/>
介紹:LambdaNetLambdaNet是由Haskell實現的一個開源的人工神經網絡庫，它抽象了網絡創建、訓練並使用了高階函數。該庫還提供了一組預定義函數，用戶可以采取多種方式組合這些函數來操作現實世界數據。</p>

<p>*<a href="http://wenku.baidu.com/course/view/49e8b8f67c1cfad6195fa705">《百度余凱&amp;張潼機器學習視頻》</a><br/>
介紹:如果你從事互聯網搜索，在線廣告，用戶行為分析，圖像識別，自然語言理解，或者生物信息學，智能機器人，金融預測，那麽這門核心課程你必須深入了解。</p>

<p>*<a href="http://v.youku.com/v_show/id_XODQzNDM4MDg0.html">楊強在TEDxNanjing談智能的起源</a><br/>
介紹:&ldquo;人工智能研究分許多流派。其中之一以IBM為代表，認為只要有高性能計算就可得到智能，他們的‘深藍’擊敗了世界象棋冠軍；另一流派認為智能來自動物本能；還有個很強的流派認為只要找來專家，把他們的思維用邏輯一條條寫下，放到計算機裏就行……&rdquo; 楊強在TEDxNanjing談智能的起源</p>

<p>*<a href="http://www.machinelearning.org/proceedings/icml2006/047_Connectionist_Tempor.pdf">《深度RNN/LSTM用於結構化學習 0)序列標註Connectionist Temporal ClassificationICML06》</a><br/>
介紹:1)機器翻譯<a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">Sequence to Sequence NIPS14</a> 2)成分句法<a href="http://arxiv.org/pdf/1412.7449v1.pdf">GRAMMAR AS FOREIGN LANGUAGE</a></p>

<p>*<a href="http://techblog.youdao.com/?p=915">《Deep Learning實戰之word2vec》</a><br/>
介紹:網易有道的三位工程師寫的word2vec的解析文檔，從基本的詞向量/統計語言模型->NNLM->Log-Linear/Log-Bilinear->層次化Log-Bilinear，到CBOW和Skip-gram模型，再到word2vec的各種tricks，公式推導與代碼，基本上是網上關於word2vec資料的大合集，對word2vec感興趣的朋友可以看看</p>

<p>*<a href="http://mloss.org/software/">《Machine learning open source software》</a><br/>
介紹:機器學習開源軟件,收錄了各種機器學習的各種編程語言學術與商業的開源軟件．與此類似的還有很多例如:<a href="http://www.dmoz.org/Computers/Artificial_Intelligence/Machine_Learning/Software/">DMOZ &ndash; Computers: Artificial Intelligence: Machine Learning: Software</a>, <a href="http://www.csie.ntu.edu.tw/%7Ecjlin/libsvm/">LIBSVM &mdash; A Library for Support Vector Machines</a>,　<a href="http://www.cs.waikato.ac.nz/ml/weka/">Weka 3: Data Mining Software in Java</a>,　<a href="http://scikit-learn.org/stable/">scikit-learn:Machine Learning in Python</a>,　<a href="https://github.com/ty4z2008/Qix/blob/master/www.nltk.org">Natural Language Toolkit:NLTK</a>,　<a href="http://mallet.cs.umass.edu/">MAchine Learning for LanguagE Toolkit</a>,　<a href="http://orange.biolab.si/">Data Mining &ndash; Fruitful and Fun</a>,　<a href="http://opencv.willowgarage.com/wiki/">Open Source Computer Vision Library</a></p>

<p>*<a href="http://www.guokr.com/post/512037/">《機器學習入門者學習指南》</a><br/>
介紹:作者是計算機研二(寫文章的時候，現在是2015年了應該快要畢業了)，專業方向自然語言處理．這是一點他的經驗之談．對於入門的朋友或許會有幫助</p>

<p>*<a href="http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/">《A Tour of Machine Learning Algorithms》</a><br/>
介紹:這是一篇關於機器學習算法分類的文章，非常好</p>

<p>*<a href="http://ml.memect.com/download/2014.zip">2014年的《機器學習日報》大合集</a><br/>
介紹:機器學習日報裏面推薦很多內容，在這裏有一部分的優秀內容就是來自機器學習日報．</p>

<p>*<a href="http://blog.csdn.net/abcjennifer/article/details/42493493">《Image classification with deep learning常用模型》</a><br/>
介紹:這是一篇關於圖像分類在深度學習中的文章</p>

<p>*<a href="http://research.microsoft.com/en-us/people/deng/">《自動語音識別：深度學習方法》</a><br/>
介紹:作者與Bengio的兄弟Samy 09年合編《自動語音識別：核方法》 3）李開復1989年《自動語音識別》專著，其博導、94年圖靈獎得主Raj Reddy作序</p>

<p>*<a href="http://blog.csdn.net/heiyeshuwu/article/details/42554903">《NLP中的中文分詞技術》</a><br/>
介紹: 作者是360電商技術組成員,這是一篇NLP在中文分詞中的應用</p>

<p>*<a href="http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/">《Using convolutional neural nets to detect facial keypoints tutorial》</a><br/>
介紹: 使用deep learning的人臉關鍵點檢測，此外還有一篇<a href="https://www.kaggle.com/c/facial-keypoints-detection/details/deep-learning-tutorial">AWS部署教程</a></p>

<p>*<a href="http://www.amazon.cn/Advanced-Structured-Prediction-Nowozin-Sebastian/dp/0262028379">《書籍推薦:Advanced Structured Prediction》</a><br/>
介紹: 由Sebastian Nowozin等人編纂MIT出版的新書<a href="http://t.cn/RZxipKG">《Advanced Structured Prediction》</a>，匯集了結構化預測領域諸多牛文，涉及CV、NLP等領域，值得一讀。網上公開的幾章草稿:<a href="http://www2.informatik.hu-berlin.de/%7Ekloftmar/publications/strucBook.pdf">一</a>,<a href="http://mlg.eng.cam.ac.uk/yutian/Publications/ChenGelfandWelling14-HerdingBookChapter.pdf">二</a>,<a href="http://web.engr.oregonstate.edu/%7Esinisa/research/publications/StructPredictionChapter14.pdf">三</a>,<a href="http://ttic.uchicago.edu/%7Emeshi/papers/smoothCD_chapter.pdf">四</a>,<a href="http://www.cs.ox.ac.uk/Stanislav.Zivny/homepage/publications/zwp14mit-draft.pdf">五</a></p>

<p>*<a href="http://arxiv.org/pdf/1501.01571v1.pdf">《An Introduction to Matrix Concentration Inequalities》</a><br/>
介紹: Tropp把數學家用高深裝逼的數學語言寫的矩陣概率不等式用初等的方法寫出來，是非常好的手冊，領域內的paper各種證明都在用裏面的結果。雖說是初等的，但還是非常的難</p>

<p>*<a href="https://agenda.weforum.org/2014/12/the-free-big-data-sources-you-should-know/">《The free big data sources you should know》</a><br/>
介紹: 不容錯過的免費大數據集，有些已經是耳熟能詳，有些可能還是第一次聽說，內容跨越文本、數據、多媒體等，讓他們伴你開始數據科學之旅吧，具體包括：Data.gov、US Census Bureau、European Union Open Data Portal、Data.gov.uk等</p>

<p>*<a href="http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html">《A Brief Overview of Deep Learning》</a><br/>
介紹: 谷歌科學家、Hinton親傳弟子Ilya Sutskever的深度學習綜述及實際建議</p>

<p>*<a href="http://nikhilbuduma.com/2015/01/11/a-deep-dive-into-recurrent-neural-networks/">《A Deep Dive into Recurrent Neural Nets》</a><br/>
介紹: 非常好的討論遞歸神經網絡的文章，覆蓋了RNN的概念、原理、訓練及優化等各個方面內容，強烈推薦！本文作者Nikhil Buduma還有一篇<a href="http://nikhilbuduma.com/2014/12/29/deep-learning-in-a-nutshell/">Deep Learning in a Nutshell</a>值得推薦</p>

<p>*<a href="http://qianjiye.de/2014/11/machine-learning-resources/">機器學習：學習資源</a><br/>
介紹:裏面融合了很多的資源，例如競賽，在線課程，demo，數據整合等。有分類</p>

<p>*<a href="https://www.otexts.org/book/sfml">《Statistical foundations of machine learning》</a><br/>
介紹:《機器學習的統計基礎》在線版，該手冊希望在理論與實踐之間找到平衡點，各主要內容都伴有實際例子及數據，書中的例子程序都是用R語言編寫的。</p>

<p>*<a href="http://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks">《A Deep Learning Tutorial: From Perceptrons to Deep Networks》</a><br/>
介紹:IVAN VASILEV寫的深度學習導引：從淺層感知機到深度網絡。高可讀</p>

<p>*<a href="http://futureoflife.org/static/data/documents/research_priorities.pdf">《Research priorities for robust and beneficial artificial intelligence》</a><br/>
介紹:魯棒及有益的人工智能優先研究計劃：一封公開信,目前已經有Stuart Russell, Tom Dietterich, Eric Horvitz, Yann LeCun, Peter Norvig, Tom Mitchell, Geoffrey Hinton, Elon Musk等人簽署<a href="http://futureoflife.org/misc/open_letter">The Future of Life Institute (FLI)</a>.這封信的背景是最近霍金和Elon Musk提醒人們註意AI的潛在威脅。公開信的內容是AI科學家們站在造福社會的角度，展望人工智能的未來發展方向，提出開發AI系統的Verification，Validity, Security, Control四點要求，以及需要註意的社會問題。畢竟當前AI在經濟領域，法律，以及道德領域相關研究較少。其實還有一部美劇<a href="http://tv.sohu.com/20120925/n353925789.shtml">《疑犯追蹤》</a>,介紹了AI的演進從一開始的自我學習，過濾，圖像識別，語音識別等判斷危險，到第四季的時候出現了機器通過學習成長之後想控制世界的狀態。說到這裏推薦收看。</p>

<p>*<a href="http://metacademy.org/">《Metacademy》</a><br/>
介紹:裏面根據詞條提供了許多資源，還有相關知識結構，路線圖，用時長短等。號稱是”機器學習“搜索引擎</p>

<p>*<a href="https://research.facebook.com/blog/879898285375829/fair-open-sources-deep-learning-modules-for-torch/">《FAIR open sources deep-learning modules for Torch》</a><br/>
介紹:Facebook人工智能研究院（FAIR）開源了一系列軟件庫，以幫助開發者建立更大、更快的深度學習模型。開放的軟件庫在 Facebook 被稱作模塊。用它們替代機器學習領域常用的開發環境 Torch 中的默認模塊，可以在更短的時間內訓練更大規模的神經網絡模型。</p>

<p>*<a href="http://www.cnblogs.com/ello/archive/2012/04/28/2475419.html">《淺析人臉檢測之Haar分類器方法》</a><br/>
介紹:本文雖然是寫於2012年，但是這篇文章完全是作者的經驗之作。</p>

<p>*<a href="http://www.ituring.com.cn/article/55994">《如何成為一位數據科學家》</a><br/>
介紹:本文是對《機器學習實戰》作者Peter Harrington做的一個訪談。包含了書中部分的疑問解答和一點個人學習建議</p>

<p>*<a href="http://www.metacademy.org/roadmaps/rgrosse/deep_learning">《Deep learning from the bottom up》</a><br/>
介紹:非常好的深度學習概述，對幾種流行的深度學習模型都進行了介紹和討論</p>

<p>*<a href="http://onepager.togaware.com/TextMiningO.pdf">《Hands-On Data Science with R Text Mining》</a><br/>
介紹:主要是講述了利用R語言進行數據挖掘</p>

<p>*<a href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/">《Understanding Convolutions》</a><br/>
介紹:幫你理解卷積神經網絡，講解很清晰，此外還有兩篇<a href="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/">Conv Nets: A Modular Perspective</a>，<a href="http://colah.github.io/posts/2014-12-Groups-Convolution/">Groups &amp; Group Convolutions</a>. 作者的其他的關於神經網絡文章也很棒</p>

<p>*<a href="http://www.iro.umontreal.ca/%7Epift6266/H10/notes/deepintro.html#introduction-to-deep-learning-algorithms">《Introduction to Deep Learning Algorithms》</a><br/>
介紹:Deep Learning算法介紹，裏面介紹了06年3篇讓deep learning崛起的論文</p>

<p>*<a href="http://www.iro.umontreal.ca/%7Ebengioy/papers/ftml_book.pdf">《Learning Deep Architectures for AI》</a><br/>
介紹:一本學習人工智能的書籍，作者是Yoshua Bengio，<a href="http://www.infoq.com/cn/articles/ask-yoshua-bengio">相關國內報道</a></p>

<p>*<a href="http://www.cs.toronto.edu/%7Ehinton/">Geoffrey E. Hinton個人主頁</a><br/>
介紹:Geoffrey Hinton是Deep Learning的大牛，他的主頁放了一些介紹性文章和課件值得學習</p>

<p>*<a href="http://omega.albany.edu:8008/JaynesBook.html">《PROBABILITY THEORY: THE LOGIC OF SCIENCE》</a><br/>
介紹:概率論：數理邏輯書籍</p>

<p>*<a href="https://github.com/h2oai/h2o">《H2O》</a><br/>
介紹:一個用來快速的統計，機器學習並且對於數據量大的數學庫</p>

<p>*<a href="http://www.iclr.cc/doku.php?id=iclr2015:main">《ICLR 2015會議的arXiv稿件合集》</a><br/>
介紹:在這裏你可以看到最近深度學習有什麽新動向。</p>

<p>*<a href="http://www-nlp.stanford.edu/IR-book/">《Introduction to Information Retrieval》</a><br/>
介紹:此書在信息檢索領域家喻戶曉， 除提供該書的免費電子版外，還提供一個<a href="http://www-nlp.stanford.edu/IR-book/information-retrieval.html">IR資源列表</a>，收錄了信息檢索、網絡信息檢索、搜索引擎實現等方面相關的圖書、研究中心、相關課程、子領域、會議、期刊等等，堪稱全集，值得收藏</p>

<p>*<a href="http://yosinski.com/mlss12/MLSS-2012-Amari-Information-Geometry/">《Information Geometry and its Applications to Machine Learning》</a><br/>
介紹:信息幾何學及其在機器學習中的應用</p>

<p>*<a href="http://computationallegalstudies.com/2015/01/legal-analytics-introduction-course-professors-daniel-martin-katz-michael-j-bommarito/">《Legal Analytics – Introduction to the Course》</a><br/>
介紹:課程《法律分析》介紹幻燈片。用機器學習解決法律相關分析和預測問題，相關的法律應用包括預測編碼、早期案例評估、案件整體情況的預測，定價和工作人員預測，司法行為預測等。法律領域大家可能都比較陌生，不妨了解下。</p>

<p>*<a href="https://github.com/yanxionglu/text_pdf">《文本上的算法》</a><br/>
介紹: 文中提到了最優，模型，最大熵等等理論，此外還有應用篇。推薦系統可以說是一本不錯的閱讀稿，關於模型還推薦一篇<a href="http://blog.sina.com.cn/s/blog_6742eecd0100iqcv.html">Generative Model 與 Discriminative Model</a></p>

<p>*<a href="https://github.com/karpathy/neuraltalk">《NeuralTalk》</a><br/>
介紹: NeuralTalk is a Python+numpy project for learning Multimodal Recurrent Neural Networks that describe images with sentences.NeuralTalk是一個Python的從圖像生成自然語言描述的工具。它實現了Google (Vinyals等，卷積神經網絡CNN + 長短期記憶LSTM) 和斯坦福 (Karpathy and Fei-Fei， CNN + 遞歸神經網絡RNN)的算法。NeuralTalk自帶了一個訓練好的動物模型，你可以拿獅子大象的照片來試試看</p>

<p>*<a href="https://www.paypal-engineering.com/2015/01/12/deep-learning-on-hadoop-2-0-2/">《Deep Learning on Hadoop 2.0》</a><br/>
介紹:本文主要介紹了在Hadoop2.0上使用深度學習,文章來自paypal</p>

<p>*<a href="http://arxiv.org/abs/1206.5533">《Practical recommendations for gradient-based training of deep architectures》</a><br/>
介紹:用基於梯度下降的方法訓練深度框架的實踐推薦指導,作者是<a href="http://www.iro.umontreal.ca/%7Ebengioy/yoshua_en/research.html">Yoshua Bengio</a></p>

<p>*<a href="http://machinelearningmastery.com/machine-learning-statistical-causal-methods/">《Machine Learning With Statistical And Causal Methods》</a><br/>
介紹: 用統計和因果方法做機器學習（視頻報告）</p>

<p>*<a href="https://www.youtube.com/playlist?list=PLD0F06AA0D2E8FFBA">《Machine Learning Course 160’》</a><br/>
介紹: 一個講機器學習的Youtube視頻教程。160集。系統程度跟書可比擬。</p>

<p>*<a href="http://www.cnblogs.com/LeftNotEasy/archive/2010/12/05/mathmatic_in_machine_learning_1_regression_and_gradient_descent.html">《回歸(regression)、梯度下降(gradient descent)》</a><br/>
介紹: 機器學習中的數學，作者的研究方向是機器學習，並行計算如果你還想了解一點其他的可以看看他<a href="http://www.cnblogs.com/LeftNotEasy/archive/2011/05/02/recommended-blogspots.html">博客</a>的其他文章</p>

<p>*<a href="http://tech.meituan.com/mt-recommend-practice.html">《美團推薦算法實踐》</a><br/>
介紹: 美團推薦算法實踐，從框架，應用，策略，查詢等分析</p>

<p>*<a href="http://arxiv.org/abs/1412.1632">《Deep Learning for Answer Sentence Selection》</a><br/>
介紹: 深度學習用於問答系統答案句的選取</p>

<p>*<a href="http://www.iro.umontreal.ca/%7Elisa/pointeurs/WWW2014.pdf">《Learning Semantic Representations Using Convolutional Neural Networks for Web Search》</a><br/>
介紹: CNN用於WEB搜索，深度學習在文本計算中的應用</p>

<p>*<a href="https://github.com/caesar0301/awesome-public-datasets">《Awesome Public Datasets》</a><br/>
介紹: Awesome系列中的公開數據集</p>

<p>*<a href="http://www.academics.io/">《Search Engine &amp; Community》</a><br/>
介紹: 一個學術搜索引擎</p>

<p>*<a href="http://honnibal.github.io/spaCy/">《spaCy》</a><br/>
介紹: 用Python和Cython寫的工業級自然語言處理庫，號稱是速度最快的NLP庫，快的原因一是用Cython寫的，二是用了個很巧妙的hash技術，加速系統的瓶頸，NLP中稀松特征的存取</p>

<p>*<a href="http://fr.slideshare.net/MrChrisJohnson/collaborative-filtering-with-spark">《Collaborative Filtering with Spark》</a><br/>
介紹: <a href="http://www.fields.utoronto.ca/video-archive/event/323/2014">Fields</a>是個數學研究中心,上面的這份ppt是來自Fields舉辦的活動中Russ Salakhutdinov帶來的《大規模機器學習》分享</p>

<p>*<a href="http://www.7300days.com/index.php/stds/topic/list/id/27/name/Topic%20modeling">《Topic modeling 的經典論文》</a><br/>
介紹: Topic modeling 的經典論文,標註了關鍵點</p>

<p>*<a href="http://arxiv.org/abs/1412.6564">《Move Evaluation in Go Using Deep Convolutional Neural Networks》</a><br/>
介紹: 多倫多大學與Google合作的新論文，深度學習也可以用來下圍棋，據說能達到六段水平</p>

<p>*<a href="http://ztl2004.github.io/MachineLearningWeekly/issue2.html">《機器學習周刊第二期》</a>
介紹: 新聞，paper,課程，book，system,CES,Roboot，此外還推薦一個<a href="http://blog.newitfarmer.com/ai/deep-learning/15302/repost-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%B8%8E%E7%BB%BC%E8%BF%B0%E8%B5%84%E6%96%99">深度學習入門與綜述資料</a></p>

<p>*<a href="http://www.bigdata-madesimple.com/learning-more-like-a-human-18-free-ebooks-on-machine-learning/">《Learning more like a human: 18 free eBooks on Machine Learning》</a><br/>
介紹: 18 free eBooks on Machine Learning</p>

<p>*<a href="http://www.hangli-hl.com/">《Recommend :Hang Li Home》</a><br/>
介紹:Chief scientist of Noah&rsquo;s Ark Lab of Huawei Technologies.He worked at the Research Laboratories of NEC Corporation during 1990 and 2001 and Microsoft Research Asia during 2001 and 2012. <a href="http://www.hangli-hl.com/recent-publications.html">Paper</a></p>

<p>*<a href="http://memkite.com/deep-learning-bibliography/">《DEEPLEARNING.UNIVERSITY – AN ANNOTATED DEEP LEARNING BIBLIOGRAPHY》</a><br/>
介紹: DEEPLEARNING.UNIVERSITY的論文庫已經收錄了963篇經過分類的深度學習論文了，很多經典論文都已經收錄</p>

<p>*<a href="https://www.youtube.com/watch?v=wTp3P2UnTfQ&amp;hd=1">《MLMU.cz &ndash; Radim Řehůřek &ndash; Word2vec &amp; friends (7.1.2015)》</a><br/>
介紹: Radim Řehůřek(Gensim開發者)在一次機器學習聚會上的報告，關於word2vec及其優化、應用和擴展，很實用.<a href="http://pan.baidu.com/s/1c03wd24">國內網盤</a></p>

<p>*<a href="http://databricks.com/blog/2015/01/28/introducing-streaming-k-means-in-spark-1-2.html">《Introducing streaming k-means in Spark 1.2》</a><br/>
介紹:很多公司都用機器學習來解決問題，提高用戶體驗。那麽怎麽可以讓機器學習更實時和有效呢？Spark MLlib 1.2裏面的Streaming K-means，由斑馬魚腦神經研究的Jeremy Freeman腦神經科學家編寫，最初是為了實時處理他們每半小時1TB的研究數據，現在發布給大家用了。</p>

<p>*<a href="http://www.hankcs.com/nlp/lda-java-introduction-and-implementation.html">《LDA入門與Java實現》</a><br/>
介紹: 這是一篇面向工程師的LDA入門筆記，並且提供一份開箱即用Java實現。本文只記錄基本概念與原理，並不涉及公式推導。文中的LDA實現核心部分采用了arbylon的LdaGibbsSampler並力所能及地註解了，在搜狗分類語料庫上測試良好，開源在<a href="https://github.com/hankcs/LDA4j">GitHub</a>上。</p>

<p>*<a href="http://aminer.org/">《AMiner &ndash; Open Science Platform》</a><br/>
介紹: AMiner是一個學術搜索引擎，從學術網絡中挖掘深度知識、面向科技大數據的挖掘。收集近4000萬作者信息、8000萬論文信息、1億多引用關系、鏈接近8百萬知識點；支持專家搜索、機構排名、科研成果評價、會議排名。</p>

<p>*<a href="https://www.quora.com/What-are-some-interesting-Word2Vec-results">《What are some interesting Word2Vec results?》</a><br/>
介紹: Quora上的主題，討論Word2Vec的有趣應用，Omer Levy提到了他在CoNLL2014最佳論文裏的分析結果和新方法，Daniel Hammack給出了找特異詞的小應用並提供了<a href="https://github.com/dhammack/Word2VecExample">(Python)代碼</a></p>

<p>*<a href="http://blog.coursegraph.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%AC%E5%BC%80%E8%AF%BE%E6%B1%87%E6%80%BB">《機器學習公開課匯總》</a><br/>
介紹: 機器學習公開課匯總,雖然裏面的有些課程已經歸檔過了，但是還有個別的信息沒有。感謝課程圖譜的小編</p>

<p>*<a href="http://linear.ups.edu/download.html">《A First Course in Linear Algebra》</a><br/>
介紹: 【A First Course in Linear Algebra】Robert Beezer 有答案 有移動版、打印版 使用GNU自由文檔協議 引用了傑弗遜1813年的信</p>

<p>*<a href="https://github.com/ShiqiYu/libfacedetection">《libfacedetection》</a><br/>
介紹:libfacedetection是深圳大學開源的一個人臉圖像識別庫。包含正面和多視角人臉檢測兩個算法.優點:速度快(OpenCV haar+adaboost的2-3倍), 準確度高 (FDDB非公開類評測排名第二），能估計人臉角度。</p>

<p>*<a href="http://dl.acm.org/citation.cfm?doid=2684822.2685310">《Inverting a Steady-State》</a><br/>
介紹:WSDM2015最佳論文 把馬爾可夫鏈理論用在了圖分析上面，比一般的propagation model更加深刻一些。通過全局的平穩分布去求解每個節點影響系數模型。假設合理（轉移受到相鄰的影響系數影響）。可以用來反求每個節點的影響系數</p>

<p>*<a href="http://pan.baidu.com/s/1pJogO7x">《機器學習入門書單》</a><br/>
介紹:機器學習入門書籍，<a href="http://www.hankcs.com/ml/machine-learning-entry-list.html">具體介紹</a></p>

<p>*<a href="http://v1v3kn.tumblr.com/post/47193952400/the-trouble-with-svms">《The Trouble with SVMs》</a><br/>
介紹: 非常棒的強調特征選擇對分類器重要性的文章。情感分類中，根據互信息對復雜高維特征降維再使用樸素貝葉斯分類器，取得了比SVM更理想的效果，訓練和分類時間也大大降低——更重要的是，不必花大量時間在學習和優化SVM上——特征也一樣no free lunch</p>

<p>*<a href="http://www.stat.cmu.edu/%7Elarry/Wasserman.pdf">《Rise of the Machines》</a><br/>
介紹:CMU的統計系和計算機系知名教授Larry Wasserman 在《機器崛起》,對比了統計和機器學習的差異</p>

<p>*<a href="http://tech.meituan.com/mt-mlinaction-how-to-ml.html">《實例詳解機器學習如何解決問題》</a><br/>
介紹:隨著大數據時代的到來，機器學習成為解決問題的一種重要且關鍵的工具。不管是工業界還是學術界，機器學習都是一個炙手可熱的方向，但是學術界和工業界對機器學習的研究各有側重，學術界側重於對機器學習理論的研究，工業界側重於如何用機器學習來解決實際問題。這篇文章是美團的實際環境中的實戰篇</p>

<p>*<a href="http://www.gaussianprocess.org/gpml/">《Gaussian Processes for Machine Learning》</a><br/>
介紹:面向機器學習的高斯過程，章節概要：回歸、分類、協方差函數、模型選擇與超參優化、高斯模型與其他模型關系、大數據集的逼近方法等,<a href="http://vdisk.weibo.com/s/ayG13we2vfWuT">微盤下載</a></p>

<p>*<a href="http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/">《FuzzyWuzzy: Fuzzy String Matching in Python》</a><br/>
介紹:Python下的文本模糊匹配庫，老庫新推，可計算串間ratio(簡單相似系數)、partial_ratio(局部相似系數)、token_sort_ratio(詞排序相似系數)、token_set_ratio(詞集合相似系數)等。<a href="https://github.com/seatgeek/fuzzywuzzy">Github</a></p>

<p>*<a href="http://blocks.readthedocs.org/en/latest/">《Blocks》</a><br/>
介紹:Blocks是基於Theano的神經網絡搭建框架，集成相關函數、管道和算法，幫你更快地創建和管理NN模塊.</p>

<p>*<a href="http://alex.smola.org/teaching/10-701-15/">《Introduction to Machine Learning》</a><br/>
介紹:機器學習大神Alex Smola在CMU新一期的機器學習入門課程”Introduction to Machine Learning“近期剛剛開課，課程4K高清視頻同步到Youtube上，目前剛剛更新到 2.4 Exponential Families,課程視頻<a href="https://www.youtube.com/playlist?list=PLZSO_6-bSqHTTV7w9u7grTXBHMH-mw3qn">playlist</a>, 感興趣的同學可以關註，非常適合入門.</p>

<p>*<a href="http://arxiv.org/abs/1502.01423">《Collaborative Feature Learning from Social Media》</a><br/>
介紹:用社交用戶行為學習圖片的協同特征，可更好地表達圖片內容相似性。由於不依賴於人工標簽(標註)，可用於大規模圖片處理，難在用戶行為數據的獲取和清洗；利用社會化特征的思路值得借鑒.</p>

<p>*<a href="https://blog.twitter.com/2015/introducing-practical-and-robust-anomaly-detection-in-a-time-series">《Introducing practical and robust anomaly detection in a time series》</a><br/>
介紹:Twitter技術團隊對前段時間開源的時間序列異常檢測算法(S-H-ESD)R包的介紹，其中對異常的定義和分析很值得參考，文中也提到——異常是強針對性的，某個領域開發的異常檢測在其他領域直接用可不行.</p>

<p>*<a href="http://www.destinationcrm.com/Articles/Web-Exclusives/Viewpoints/Empower-Your-Team-to-Deal-with-Data-Quality-Issues-101308.aspx">《Empower Your Team to Deal with Data-Quality Issues》</a><br/>
介紹:聚焦數據質量問題的應對，數據質量對各種規模企業的性能和效率都至關重要，文中總結出(不限於)22種典型數據質量問題顯現的信號，以及典型的數據質量解決方案(清洗、去重、統一、匹配、權限清理等)</p>

<p>*<a href="http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E8%B5%84%E6%BA%90">《中文分詞入門之資源》</a><br/>
介紹:中文分詞入門之資源.</p>

<p>*<a href="https://www.youtube.com/playlist?list=PLnDbcXCpYZ8lCKExMs8k4PtIbani9ESX3">《Deep Learning Summit, San Francisco, 2015》</a><br/>
介紹:15年舊金山深度學習峰會視頻集萃,<a href="http://pan.baidu.com/s/1ntiLMcT">國內雲盤</a></p>

<p>*<a href="http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/">《Introduction to Conditional Random Fields》</a><br/>
介紹:很好的條件隨機場(CRF)介紹文章,作者的學習筆記</p>

<p>*<a href="http://cs.stanford.edu/%7Edanqi/papers/emnlp2014.pdf">《A Fast and Accurate Dependency Parser using Neural Networks》</a><br/>
介紹: 來自Stanford，用神經網絡實現快速準確的依存關系解析器</p>

<p>*<a href="https://timdettmers.wordpress.com/2014/08/14/which-gpu-for-deep-learning/">《Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning》</a><br/>
介紹:做深度學習如何選擇GPU的建議</p>

<p>*<a href="http://new.livestream.com/accounts/10932136/events/3779068">《Sparse Linear Models》</a><br/>
介紹: Stanford的Trevor Hastie教授在H2O.ai Meet-Up上的報告，講稀疏線性模型——面向“寬數據”(特征維數超過樣本數)的線性模型,13年同<a href="http://pan.baidu.com/s/1jimPw">主題報告</a>、<a href="http://pan.baidu.com/s/1o6wqW6u">講義</a>.</p>

<p>*<a href="https://github.com/jbhuang0604/awesome-computer-vision">《Awesome Computer Vision》</a><br/>
介紹: 分類整理的機器視覺相關資源列表，秉承Awesome系列風格，有質有量!作者的更新頻率也很頻繁</p>

<p>*<a href="http://www.personal.ceu.hu/staff/Adam_Szeidl/">《Adam Szeidl》</a><br/>
介紹: social networks course</p>

<p>*<a href="http://radar.oreilly.com/2015/01/building-and-deploying-large-scale-machine-learning-pipelines.html/">《Building and deploying large-scale machine learning pipelines》</a><br/>
介紹: 大規模機器學習流程的構建與部署.</p>

<p>*<a href="http://download.csdn.net/detail/lswtzw/8469997">《人臉識別開發包》</a><br/>
介紹: 人臉識別二次開發包，免費，可商用，有演示、範例、說明書.</p>

<p>*<a href="http://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch/">《Understanding Natural Language with Deep Neural Networks Using Torch》</a><br/>
介紹: 采用Torch用深度學習網絡理解NLP，來自Facebook 人工智能的文章.</p>

<p>*<a href="http://arxiv.org/pdf/1503.00168.pdf">《The NLP Engine: A Universal Turing Machine for NLP》</a><br/>
介紹: 來自CMU的Ed Hovy和Stanford的Jiwei Li一篇有意思的Arxiv文章,作者用Shannon Entropy來刻畫NLP中各項任務的難度.</p>

<p>*<a href="http://staff.city.ac.uk/%7Esb317/papers/foundations_bm25_review.pdf">《TThe Probabilistic Relevance Framework: BM25 and Beyond》</a><br/>
介紹: 信息檢索排序模型BM25(Besting Matching)。1）從經典概率模型演變而來 2）捕捉了向量空間模型中三個影響索引項權重的因子：IDF逆文檔頻率；TF索引項頻率；文檔長度歸一化。3）並且含有集成學習的思想：組合了BM11和BM15兩個模型。4）作者是BM25的提出者和Okapi實現者Robertson.</p>

<p>*<a href="http://www.analyticsvidhya.com/blog/2015/03/introduction-auto-regression-moving-average-time-series/">《Introduction to ARMA Time Series Models – simplified》</a><br/>
介紹: 自回歸滑動平均(ARMA)時間序列的簡單介紹，ARMA是研究時間序列的重要方法，由自回歸模型（AR模型）與滑動平均模型（MA模型）為基礎“混合”構成.</p>

<p>*<a href="http://arxiv.org/pdf/1503.01838v1.pdf">《Encoding Source Language with Convolutional Neural Network for Machine Translation》</a><br/>
介紹: 把來自target的attention signal加入source encoding CNN的輸入，得到了比BBN的模型好的多neural network joint model</p>

<p>*<a href="http://arxiv.org/abs/1502.03815">《Spices form the basis of food pairing in Indian cuisine》</a><br/>
介紹: 揭開印度菜的美味秘訣——通過對大量食譜原料關系的挖掘，發現印度菜美味的原因之一是其中的味道互相沖突，很有趣的文本挖掘研究</p>

<p>*<a href="http://www.52nlp.cn/hmm%E7%9B%B8%E5%85%B3%E6%96%87%E7%AB%A0%E7%B4%A2%E5%BC%95">《HMM相關文章索引》</a><br/>
介紹: HMM相關文章</p>

<p>*<a href="http://www.ccs.neu.edu/home/ekanou/ISU535.09X2/Handouts/Review_Material/zipfslaw.pdf">《Zipf&rsquo;s and Heap&rsquo;s law》</a><br/>
介紹: 1)詞頻與其降序排序的關系,最著名的是語言學家齊夫(Zipf,1902-1950)1949年提出的Zipf‘s law,即二者成反比關系. 曼德勃羅(Mandelbrot,1924- 2010)引入參數修正了對甚高頻和甚低頻詞的刻畫 2)Heaps&#8217; law: 詞匯表與語料規模的平方根(這是一個參數,英語0.4-0.6)成正比</p>

<p>*<a href="http://www.reddit.com/r/MachineLearning/comments/2xcyrl/i_am_j%C3%BCrgen_schmidhuber_ama/">《I am Jürgen Schmidhuber, AMA》</a><br/>
介紹: Jürgen Schmidhuber在Reddit上的AMA(Ask Me Anything)主題，有不少RNN和AI、ML的幹貨內容，關於開源&amp;思想&amp;方法&amp;建議……耐心閱讀，相信你也會受益匪淺.</p>

<p>*<a href="http://academictorrents.com/">學術種子網站：AcademicTorrents</a><br/>
介紹: 成G上T的學術數據，HN近期熱議話題,主題涉及機器學習、NLP、SNA等。下載最簡單的方法，通過BT軟件，RSS訂閱各集合即可</p>

<p>*<a href="http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html">《機器學習交互速查表》</a><br/>
介紹: Scikit-Learn官網提供，在原有的Cheat Sheet基礎上加上了Scikit-Learn相關文檔的鏈接，方便瀏覽</p>

<p>*<a href="https://timdettmers.wordpress.com/2015/03/09/deep-learning-hardware-guide/">《A Full Hardware Guide to Deep Learning》</a><br/>
介紹: 深度學習的全面硬件指南，從GPU到RAM、CPU、SSD、PCIe</p>

<p>*<a href="http://hi.baidu.com/susongzhi/item/085983081b006311eafe38e7">《行人檢測(Pedestrian Detection)資源》</a><br/>
介紹:Pedestrian Detection paper &amp; data</p>

<p>*<a href="http://arxiv.org/abs/1502.01241">《A specialized face-processing network consistent with the representational geometry of monkey face patches》</a><br/>
介紹: 【神經科學碰撞人工智能】在臉部識別上你我都是專家，即使細微的差別也能辨認。研究已證明人類和靈長類動物在面部加工上不同於其他物種，人類使用梭狀回面孔區（FFA）。Khaligh-Razavi等通過計算機模擬出人臉識別的FFA活動，堪稱神經科學與人工智能的完美結合。</p>

<p>*<a href="https://vimeo.com/19569529">《Neural Net in C++ Tutorial》</a><br/>
介紹: 神經網絡C++教程,本文介紹了用可調節梯度下降和可調節動量法設計和編碼經典BP神經網絡，網絡經過訓練可以做出驚人和美妙的東西出來。此外作者博客的其他文章也很不錯。</p>

<p>*<a href="http://deeplearning4j.org/neuralnetworktable.html">《How to Choose a Neural Network》</a><br/>
介紹:deeplearning4j官網提供的實際應用場景NN選擇參考表，列舉了一些典型問題建議使用的神經網絡</p>

<p>*<a href="https://github.com/yusugomori/DeepLearning">《Deep Learning (Python, C/C++, Java, Scala, Go)》</a><br/>
介紹:一個深度學習項目,提供了Python, C/C++, Java, Scala, Go多個版本的代碼</p>

<p>*<a href="http://deeplearning.net/tutorial/">《Deep Learning Tutorials》</a><br/>
介紹:深度學習教程</p>

<p>*<a href="http://www.ccf.org.cn/resources/1190201776262/2015/03/12/15.pdf">《自然語言處理的發展趨勢——訪卡內基梅隆大學愛德華·霍威教授》</a><br/>
介紹:自然語言處理的發展趨勢——訪卡內基梅隆大學愛德華·霍威教授.</p>

<p>*<a href="http://arxiv.org/abs/1503.03832">《FaceNet: A Unified Embedding for Face Recognition and Clustering》</a><br/>
介紹:Google對Facebook DeepFace的有力回擊—— FaceNet，在LFW(Labeled Faces in the Wild)上達到99.63%準確率(新紀錄)，FaceNet embeddings可用於人臉識別、鑒別和聚類.</p>

<p>*<a href="http://databricks.com/blog/2015/01/21/random-forests-and-boosting-in-mllib.html">《MLlib中的Random Forests和Boosting》</a><br/>
介紹:本文來自Databricks公司網站的一篇博客文章，由Joseph Bradley和Manish Amde撰寫，文章主要介紹了Random Forests和Gradient-Boosted Trees（GBTs）算法和他們在MLlib中的分布式實現，以及展示一些簡單的例子並建議該從何處上手.<a href="http://www.csdn.net/article/2015-03-11/2824178">中文版</a>.</p>

<p>*<a href="http://spn.cs.washington.edu/index.shtml">《Sum-Product Networks(SPN)》</a><br/>
介紹:華盛頓大學Pedro Domingos團隊的DNN，提供論文和實現代碼.</p>

<p>*<a href="http://nlp.stanford.edu/software/nndep.shtml">《Neural Network Dependency Parser》</a><br/>
介紹:基於神經網絡的自然語言依存關系解析器(已集成至Stanford CoreNLP)，特點是超快、準確，目前可處理中英文語料，基於<a href="http://cs.stanford.edu/%7Edanqi/papers/emnlp2014.pdf">《A Fast and Accurate Dependency Parser Using Neural Networks》</a>思路實現.</p>

<p>*<a href="http://www.flickering.cn/nlp/2015/03/%E6%88%91%E4%BB%AC%E6%98%AF%E8%BF%99%E6%A0%B7%E7%90%86%E8%A7%A3%E8%AF%AD%E8%A8%80%E7%9A%84-3%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">《神經網絡語言模型》</a><br/>
介紹:本文根據神經網絡的發展歷程，詳細講解神經網絡語言模型在各個階段的形式，其中的模型包含NNLM[Bengio,2003]、Hierarchical NNLM[Bengio, 2005], Log-Bilinear[Hinton, 2007],SENNA等重要變形，總結的特別好.</p>

<p>*<a href="http://www.elg.uottawa.ca/%7Enat/Courses/csi5387_Winter2014/paper13.pdf">《Classifying Spam Emails using Text and Readability Features》</a><br/>
介紹:經典問題的新研究：利用文本和可讀性特征分類垃圾郵件。</p>

<p>*<a href="https://github.com/alexandrebarachant/bci-challenge-ner-2015">《BCI Challenge @ NER 2015》</a><br/>
介紹:<a href="https://www.kaggle.com/c/inria-bci-challenge">Kaggle腦控計算機交互(BCI)競賽</a>優勝方案源碼及文檔，包括完整的數據處理流程，是學習Python數據處理和Kaggle經典參賽框架的絕佳實例</p>

<p>*<a href="http://www.ipol.im/">《IPOL Journal · Image Processing On Line》</a><br/>
介紹:IPOL（在線圖像處理）是圖像處理和圖像分析的研究期刊，每篇文章都包含一個算法及相應的代碼、Demo和實驗文檔。文本和源碼是經過了同行評審的。IPOL是開放的科學和可重復的研究期刊。我一直想做點類似的工作，拉近產品和技術之間的距離.</p>

<p>*<a href="http://eprint.iacr.org/2014/331">《Machine learning classification over encrypted data》</a><br/>
介紹:出自MIT，研究加密數據高效分類問題.</p>

<p>*<a href="https://github.com/purine/purine2">《purine2》</a><br/>
介紹:新加坡LV實驗室的神經網絡並行框架<a href="http://arxiv.org/abs/1412.6249">Purine: A bi-graph based deep learning framework</a>,支持構建各種並行的架構，在多機多卡，同步更新參數的情況下基本達到線性加速。12塊Titan 20小時可以完成Googlenet的訓練。</p>

<p>*<a href="http://michal.io/machine-learning-resources/">Machine Learning Resources</a><br/>
介紹:這是一個機器學習資源庫,雖然比較少.但蚊子再小也是肉.有突出部分.此外還有一個由<a href="http://zhengrui.github.io/zerryland/ML-CV-Resource.html">zheng Rui整理的機器學習資源</a>.</p>

<p>*<a href="https://github.com/cjdd3b/nicar2015/tree/master/machine-learning">《Hands-on with machine learning》</a><br/>
介紹:Chase Davis在NICAR15上的主題報告材料，用Scikit-Learn做監督學習的入門例子.</p>

<p>*<a href="http://www.cse.unsw.edu.au/%7Ebillw/nlpdict.html">《The Natural Language Processing Dictionary》</a><br/>
介紹:這是一本自然語言處理的詞典,從1998年開始到目前積累了成千上萬的專業詞語解釋,如果你是一位剛入門的朋友.可以借這本詞典讓自己成長更快.</p>

<p>*<a href="http://arxiv.org/abs/1503.01331">《PageRank Approach to Ranking National Football Teams》</a><br/>
介紹:通過分析1930年至今的比賽數據，用PageRank計算世界杯參賽球隊排行榜.</p>

<p>*<a href="http://cyclismo.org/tutorial/R/">《R Tutorial》</a><br/>
介紹:R語言教程,此外還推薦一個R語言教程<a href="http://cran.r-project.org/doc/manuals/R-intro.html">An Introduction to R</a>.</p>

<p>*<a href="http://arxiv.org/abs/0803.0476">《Fast unfolding of communities in large networks》</a><br/>
介紹:經典老文，復雜網絡社區發現的高效算法，Gephi中的<a href="https://github.com/ty4z2008/Qix/blob/master/The%20Louvain%20method%20for%20community%20detection%20in%20large%20networks">Community detection</a>即基於此.</p>

<p>*<a href="http://numl.net/">《NUML》</a><br/>
介紹: 一個面向 .net 的開源機器學習庫,<a href="https://github.com/sethjuarez/numl">Github</a></p>

<p>*<a href="http://synaptic.juancazala.com/">《synaptic.Js》</a><br/>
介紹: 支持node.js的JS神經網絡庫，可在客戶端瀏覽器中運行，支持LSTM等。<a href="https://github.com/cazala/synaptic">Github</a></p>

<p>*<a href="http://tjo-en.hatenablog.com/entry/2015/03/20/191614">《Machine learning for package users with R (1): Decision Tree》</a><br/>
介紹: 決策樹</p>

<p>*<a href="http://www.kdnuggets.com/2015/03/deep-learning-curse-dimensionality-autoencoders.html">《Deep Learning, The Curse of Dimensionality, and Autoencoders》</a><br/>
介紹: 討論深度學習自動編碼器如何有效應對維數災難,<a href="http://www.36dsj.com/archives/26223">中文翻譯</a></p>

<p>*<a href="http://www.cs.cmu.edu/%7Esuvrit/teach/">《Advanced Optimization and Randomized Methods》</a><br/>
介紹: CMU的優化與隨機方法課程，由A. Smola和S. Sra主講，優化理論是機器學習的基石，值得深入學習。<a href="http://pan.baidu.com/s/1c0cZtQC">國內雲(視頻)</a></p>

<p>*<a href="http://cs231n.stanford.edu/reports.html">《CS231n: Convolutional Neural Networks for Visual Recognition》</a><br/>
介紹: &ldquo;面向視覺識別的CNN&#8221;課程設計報告集錦.近百篇，內容涉及圖像識別應用的各個方面</p>

<p>*<a href="http://databricks.com/blog/2015/03/25/topic-modeling-with-lda-mllib-meets-graphx.html">《Topic modeling with LDA: MLlib meets GraphX》</a><br/>
介紹:用Spark的MLlib+GraphX做大規模LDA主題抽取.</p>

<p>*<a href="http://arxiv.org/abs/1502.05988">《Deep Learning for Multi-label Classification》</a><br/>
介紹: 基於深度學習的多標簽分類,用基於RBM的DBN解決多標簽分類(特征)問題</p>

<p>*<a href="http://deepmind.com/publications.html">《Google DeepMind publications》</a><br/>
介紹: DeepMind論文集錦</p>

<p>*<a href="http://kaldi-asr.org/">《kaldi》</a><br/>
介紹: 一個開源語音識別工具包,它目前托管在<a href="http://sourceforge.net/projects/kaldi/">sourceforge</a>上面</p>

<p>*<a href="http://datajournalismhandbook.org/">《Data Journalism Handbook》</a><br/>
介紹: 免費電子書《數據新聞手冊》, 國內有熱心的朋友翻譯了<a href="http://datajournalismhandbook.org/chinese/index.html">中文版</a>,大家也可以<a href="http://datajournalismhandbook.org/1.0/en/">在線閱讀</a></p>

<p>*<a href="https://highlyscalable.wordpress.com/2015/03/10/data-mining-problems-in-retail/">《Data Mining Problems in Retail》</a><br/>
介紹: 零售領域的數據挖掘文章.</p>

<p>*<a href="https://timdettmers.wordpress.com/2015/03/26/convolution-deep-learning/">《Understanding Convolution in Deep Learning》</a><br/>
介紹: 深度學習卷積概念詳解,深入淺出.</p>

<p>*<a href="http://pandas.pydata.org/">《pandas: powerful Python data analysis toolkit》</a><br/>
介紹: 非常強大的Python的數據分析工具包.</p>

<p>*<a href="http://breakthroughanalysis.com/2015/03/23/text-analytics-2015/">《Text Analytics 2015》</a><br/>
介紹: 2015文本分析(商業)應用綜述.</p>

<p>*<a href="http://www.slideshare.net/VincenzoLomonaco/deep-learning-libraries-and-rst-experiments-with-theano">《Deep Learning libraries and ﬁrst experiments with Theano》</a><br/>
介紹: 深度學習框架、庫調研及Theano的初步測試體會報告.</p>

<p>*<a href="http://www.iro.umontreal.ca/%7Ebengioy/dlbook/">《DEEP learning》</a><br/>
介紹: MIT的Yoshua Bengio等人講深度學習的新書，還未定稿，線上提供Draft chapters收集反饋，超贊！強烈推薦.</p>

<p>*<a href="https://github.com/hickeroar/simplebayes">《simplebayes》</a><br/>
介紹: Python下開源可持久化樸素貝葉斯分類庫.</p>

<p>*<a href="http://paracel.io/">《Paracel》</a><br/>
介紹:Paracel is a distributed computational framework designed for machine learning problems, graph algorithms and scientific computing in C++.</p>

<p>*<a href="http://hanlp.linrunsoft.com/">《HanLP:Han Language processing》</a><br/>
介紹: 開源漢語言處理包.</p>

<p>*<a href="http://www.rubylab.io/2015/03/18/simple-neural-network-implenentation-in-ruby/">《Simple Neural Network implementation in Ruby》</a><br/>
介紹: 使用Ruby實現簡單的神經網絡例子.</p>

<p>*[《Hacker&rsquo;s guide to Neural Networks》}(<a href="https://karpathy.github.io/neuralnets/">https://karpathy.github.io/neuralnets/</a>)<br/>
介紹:神經網絡黑客入門.</p>

<p>*<a href="http://datasciencemasters.org/">《The Open-Source Data Science Masters》</a><br/>
介紹:好多數據科學家名人推薦,還有資料.</p>

<p>*<a href="http://arxiv.org/abs/1502.01710">《Text Understanding from Scratch》</a><br/>
介紹:實現項目已經開源在github上面<a href="https://github.com/zhangxiangxiao/Crepe">Crepe</a></p>

<p>*<a href="https://levyomer.files.wordpress.com/2015/03/improving-distributional-similarity-tacl-2015.pdf">《Improving Distributional Similarity with Lessons Learned from Word Embeddings》</a><br/>
介紹:作者發現，經過調參，傳統的方法也能和word2vec取得差不多的效果。另外，無論作者怎麽試，GloVe都比不過word2vec.</p>

<p>*<a href="http://cs224d.stanford.edu/index.html">《CS224d: Deep Learning for Natural Language Processing》</a><br/>
介紹:Stanford深度學習與自然語言處理課程,Richard Socher主講.</p>

<p>*<a href="http://courses.washington.edu/css490/2012.Winter/lecture_slides/02_math_essentials.pdf">《Math Essentials in Machine Learning》</a><br/>
介紹:機器學習中的重要數學概念.</p>

<p>*<a href="http://arxiv.org/abs/1503.00007">《Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks》</a><br/>
介紹:用於改進語義表示的樹型LSTM遞歸神經網絡,句子級相關性判斷和情感分類效果很好.實現代碼.</p>

<p>*<a href="http://www.stat.cmu.edu/%7Elarry/=sml/">《Statistical Machine Learning》</a><br/>
介紹:卡耐基梅隆Ryan Tibshirani和Larry Wasserman開設的機器學習課程，先修課程為機器學習(10-715)和中級統計學(36-705)，聚焦統計理論和方法在機器學習領域應用.</p>

<p>*<a href="http://am207.org/">《AM207: Monte Carlo Methods, Stochastic Optimization》</a><br/>
介紹:《哈佛大學蒙特卡洛方法與隨機優化課程》是哈佛應用數學研究生課程，由V Kaynig-Fittkau、P Protopapas主講，Python程序示例，對貝葉斯推理感興趣的朋友一定要看看，提供<a href="http://nbviewer.ipython.org/github/AM207/2015/tree/master/Lectures/">授課視頻及課上IPN講義</a>.</p>

<p>*<a href="http://spark-summit.org/wp-content/uploads/2015/03/SSE15-40-Danford.pdf">《生物醫學的SPARK大數據應用》</a><br/>
介紹:生物醫學的SPARK大數據應用.並且伯克利開源了他們的big data genomics系統<a href="https://github.com/bigdatagenomics/adam">ADAM</a>，其他的內容可以關註一下<a href="http://spark-summit.org/">官方主頁</a>.</p>

<p>*<a href="http://aclanthology.info/">《ACL Anthology》</a><br/>
介紹:對自然語言處理技術或者機器翻譯技術感興趣的親們，請在提出自己牛逼到無以倫比的idea（自動歸納翻譯規律、自動理解語境、自動識別語義等等）之前，請通過谷歌學術簡單搜一下，如果谷歌不可用，這個網址有這個領域幾大頂會的論文列表,切不可斷章取義,胡亂假設.</p>

<p>*<a href="http://www.uni-weimar.de/medien/webis/publications/papers/stein_2015b.pdf">《Twitter Sentiment Detection via Ensemble Classification Using Averaged Confidence Scores》</a><br/>
介紹:論文+代碼:基於集成方法的Twitter情感分類,<a href="https://github.com/webis-de/ECIR-2015-and-SEMEVAL-2015">實現代碼</a>.</p>

<p>*<a href="http://ciml.chalearn.org/schedule">《NIPS 2014 CIML workshop》</a><br/>
介紹:NIPS CiML 2014的PPT,NIPS是神經信息處理系統進展大會的英文簡稱.</p>

<p>*<a href="http://cs231n.stanford.edu/reports.html">《CS231n: Convolutional Neural Networks for Visual Recognition》</a><br/>
介紹:斯坦福的深度學習課程的Projects 每個人都要寫一個論文級別的報告 裏面有一些很有意思的應用 大家可以看看 .</p>

<p>*<a href="http://www.sumsar.net/blog/2015/03/a-speed-comparison-between-flexible-linear-regression-alternatives-in-r/">《A Speed Comparison Between Flexible Linear Regression Alternatives in R》</a><br/>
介紹:R語言線性回歸多方案速度比較具體方案包括lm()、nls()、glm()、bayesglm()、nls()、mle2()、optim()和Stan’s optimizing()等.</p>

<p>*<a href="http://www.allthingsdistributed.com/2015/04/machine-learning.html">《Back-to-Basics Weekend Reading &ndash; Machine Learning》</a><br/>
介紹:文中提到的三篇論文（機器學習那些事、無監督聚類綜述、監督分類綜述）都很經典，Domnigos的機器學習課也很精彩</p>

<p>*<a href="http://arxiv.org/abs/1504.00641">《A Probabilistic Theory of Deep Learning》</a><br/>
介紹:萊斯大學（Rice University）的深度學習的概率理論.</p>

<p>*<a href="http://www.gregreda.com/2015/03/30/beer-review-markov-chains/">《Nonsensical beer reviews via Markov chains》</a><br/>
介紹:基於馬爾可夫鏈自動生成啤酒評論的開源Twitter機器人, <a href="https://github.com/gjreda/beer-snob-says">Github</a>.</p>

<p>*<a href="http://nlp.stanford.edu/courses/NAACL2013/">《Deep Learning for Natural Language Processing (without Magic)》</a>
介紹:視頻+講義:深度學習用於自然語言處理教程(NAACL13).</p>

<p>*<a href="https://www.youtube.com/watch?v=U4IYsLgNgoY&amp;hd=1">《Introduction to Data Analysis using Machine Learning》</a><br/>
介紹:用機器學習做數據分析,David Taylor最近在McGill University研討會上的報告，還提供了一系列講機器學習方法的ipn，很有價值。<a href="https://github.com/Prooffreader/intro_machine_learning">GitHub</a>。<a href="http://pan.baidu.com/s/1mgtE9te">國內雲盤</a></p>

<p>*<a href="http://arxiv.org/abs/1503.08909">《Beyond Short Snippets: Deep Networks for Video Classification》</a><br/>
介紹:基於CNN+LSTM的視頻分類, <a href="http://pan.baidu.com/s/1c0cZS9E">Google演示</a>.</p>

<p>*<a href="http://www.quora.com/How-does-Quora-use-machine-learning-in-2015/answer/Xavier-Amatriain">《How does Quora use machine learning in 2015?》</a><br/>
介紹:Quora怎麽用機器學習.</p>

<p>*<a href="https://aws.amazon.com/cn/blogs/aws/amazon-machine-learning-make-data-driven-decisions-at-scale/">《Amazon Machine Learning – Make Data-Driven Decisions at Scale》</a><br/>
介紹:亞馬遜在機器學習上面的一些應用,<a href="https://github.com/awslabs/machine-learning-samples">代碼示例</a>.</p>

<p>*<a href="https://github.com/ogrisel/parallel_ml_tutorial">《Parallel Machine Learning with scikit-learn and IPython》</a><br/>
介紹:並行機器學習指南(基於scikit-learn和IPython). <a href="http://nbviewer.ipython.org/github/ogrisel/parallel_ml_tutorial/tree/master/notebooks/">Notebook</a></p>

<p>*<a href="http://blog.kaggle.com/2015/04/08/new-video-series-introduction-to-machine-learning-with-scikit-learn/">《Intro to machine learning with scikit-learn》</a><br/>
介紹:DataSchool的機器學習基本概念教學.</p>

<p>*<a href="https://github.com/hughperkins/DeepCL">《DeepCLn》</a><br/>
介紹:一個基於OpenGL實現的卷積神經網絡，支持Linux及Windows系.</p>

<p>*<a href="https://www.mapr.com/blog/inside-look-at-components-of-recommendation-engine">《An Inside Look at the Components of a Recommendation Engine》</a><br/>
介紹:基於Mahout和Elasticsearch的推薦系統.</p>

<p>*<a href="http://www.ssc.upenn.edu/%7Efdiebold/Teaching221/econ221.html">《Forecasting in Economics, Business, Finance and Beyond》</a><br/>
介紹:Francis X. Diebold的《(經濟|商業|金融等領域)預測方法.</p>

<p>*<a href="http://www.ssc.upenn.edu/%7Efdiebold/Teaching706/econ706Penn.html">《Time Series Econometrics &ndash; A Concise Course》</a><br/>
介紹:Francis X. Diebold的《時序計量經濟學》.</p>

<p>*<a href="http://fotiad.is/blog/sentiment-analysis-comparison/">《A comparison of open source tools for sentiment analysis》</a><br/>
介紹:基於Yelp數據集的開源<a href="https://github.com/sfotiadis/yenlp">情感分析工具</a>比較,評測覆蓋Naive Bayes、SentiWordNet、CoreNLP等 .</p>

<p>*<a href="http://vdisk.weibo.com/s/ayG13we2u_sAZ">《Pattern Recognition And Machine Learning》</a><br/>
介紹:國內Pattern Recognition And Machine Learning讀書會資源匯總,<a href="http://vdisk.weibo.com/u/1841149974">各章pdf講稿</a>,<a href="http://www.cnblogs.com/Nietzsche/">博客</a>.</p>

<p>*<a href="https://highlyscalable.wordpress.com/2012/05/01/probabilistic-structures-web-analytics-data-mining/">《Probabilistic Data Structures for Web Analytics and Data Mining》</a><br/>
介紹:用於Web分析和數據挖掘的概率數據結構.</p>

<p>*<a href="https://blindmotion.github.io/2015/04/11/ml-in-navigation/">《Machine learning in navigation devices: detect maneuvers using accelerometer and gyroscope》</a><br/>
介紹:機器學習在導航上面的應用.</p>

<p>*<a href="https://www.youtube.com/user/Taylorns34/videos">《Neural Networks Demystified》</a><br/>
介紹:Neural Networks Demystified系列視頻，Stephen Welch制作，純手繪風格，淺顯易懂,<a href="http://pan.baidu.com/s/1i3AFURj">國內雲盤</a>.</p>

<p>*<a href="https://www.datacamp.com/swirl-r-tutorial">《swirl + DataCamp》</a><br/>
介紹:{swirl}數據訓練營:R&amp;數據科學在線交互教程.</p>

<p>*<a href="http://blog.terminal.com/recurrent-neural-networks-deep-net-optimization-lstm/">《Learning to Read with Recurrent Neural Networks》</a><br/>
介紹:關於深度學習和RNN的討論<a href="http://arxiv.org/abs/1409.3215">Sequence to Sequence Learning with Neural Networks</a>.</p>

<p>*<a href="http://wanghaitao8118.blog.163.com/blog/static/13986977220153811210319/">深度強化學習（Deep Reinforcement Learning）的資源</a><br/>
介紹:Deep Reinforcement Learning.</p>

<p>*<a href="https://github.com/jakevdp/sklearn_pycon2015">《Machine Learning with Scikit-Learn》</a><br/>
介紹:(PyCon2015)Scikit-Learn機器學習教程,<a href="https://github.com/ogrisel/parallel_ml_tutorial">Parallel Machine Learning with scikit-learn and IPython</a>.</p>

<p>*<a href="http://www.cs.cmu.edu/%7Eymiao/pdnntk.html">《PDNN》</a><br/>
介紹:PDNN: A Python Toolkit for Deep Learning.</p>

<p>*<a href="http://alex.smola.org/teaching/10-701-15/index.html">《Introduction to Machine Learning》</a><br/>
介紹:15年春季學期CMU的機器學習課程，由Alex Smola主講，提供講義及授課視頻，很不錯.<a href="http://pan.baidu.com/s/1pJxBePX">國內雲盤</a>.</p>

<p>*<a href="http://www.st.ewi.tudelft.nl/%7Ehauff/TI2736-B.html">《Big Data Processing》</a><br/>
介紹:大數據處理課.內容覆蓋流處理、MapReduce、圖算法等.</p>

<p>*<a href="https://www.hakkalabs.co/articles/spark-mllib-making-practical-machine-learning-easy-and-scalable">《Spark MLlib: Making Practical Machine Learning Easy and Scalable》</a><br/>
介紹:用Spark MLlib實現易用可擴展的機器學習,<a href="http://pan.baidu.com/s/1gdxSOZh">國內雲盤</a>.</p>

<p>*<a href="http://mrkulk.github.io/www_cvpr15/">《Picture: A Probabilistic Programming Language for Scene Perception》</a><br/>
介紹:以往上千行代碼概率編程(語言)實現只需50行.</p>

<p>*<a href="http://zevross.com/blog/2014/08/04/beautiful-plotting-in-r-a-ggplot2-cheatsheet-3/">《Beautiful plotting in R: A ggplot2 cheatsheet》</a><br/>
介紹:ggplot2速查小冊子,<a href="http://www.ling.upenn.edu/%7Ejoseff/avml2012/">另外一個</a>,此外還推薦<a href="http://zevross.com/blog/2015/01/13/a-new-data-processing-workflow-for-r-dplyr-magrittr-tidyr-ggplot2/">《A new data processing workflow for R: dplyr, magrittr, tidyr, ggplot2》</a>.</p>

<p>*<a href="http://emnlp2014.org/papers/pdf/EMNLP2014148.pdf">《Using Structured Events to Predict Stock Price Movement: An Empirical Investigation》</a><br/>
介紹:用結構化模型來預測實時股票行情.</p>

<p>*<a href="http://ijcai-15.org/index.php/accepted-papers">《International Joint Conference on Artificial Intelligence Accepted paper》</a><br/>
介紹:<a href="http://ijcai.org/">國際人工智能聯合會議</a>錄取論文列表,大部分論文可使用Google找到.</p>

<p>*<a href="http://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/">《Why GEMM is at the heart of deep learning》</a><br/>
介紹:一般矩陣乘法(GEMM)對深度學習的重要性.</p>

<p>*<a href="https://github.com/dmlc">《Distributed (Deep) Machine Learning Common》</a><br/>
介紹:A Community of awesome Distributed Machine Learning C++ projects.</p>

<p>*<a href="http://webdocs.cs.ualberta.ca/%7Esutton/book/the-book.html">《Reinforcement Learning: An Introduction》</a><br/>
介紹:免費電子書&lt;強化學習介紹>,<a href="http://pan.baidu.com/s/1jkaMq">第一版(1998)</a>,<a href="http://pan.baidu.com/s/1dDnNEnR">第二版(2015草稿)</a>,相關課程<a href="http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/RLAIcourse/2010.html">資料</a>, <a href="http://www.inf.ed.ac.uk/teaching/courses/rl/">Reinforcement Learning</a>.</p>

<p>*<a href="http://blogs.msdn.com/b/microsoft_press/archive/2015/04/15/free-ebook-microsoft-azure-essentials-azure-machine-learning.aspx">《Free ebook: Microsoft Azure Essentials: Azure Machine Learning》</a><br/>
介紹:免費書:Azure ML使用精要.</p>

<p>*<a href="http://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks">《A Deep Learning Tutorial: From Perceptrons to Deep Networks》</a><br/>
介紹:A Deep Learning Tutorial: From Perceptrons to Deep Networks.</p>

<p>*《A Brief Overview of Deep Learning》
介紹:<a href="http://xhrwang.me/2015/01/16/a-brief-overview-of-deep-learning.html">中文版</a>.</p>

<p>*<a href="https://github.com/dmlc/wormhole">《Wormhole》</a><br/>
介紹:Portable, scalable and reliable distributed machine learning.</p>

<p>*<a href="https://github.com/soumith/convnet-benchmarks">《convnet-benchmarks》</a><br/>
介紹:CNN開源實現橫向評測,參評框架包括Caffe 、Torch-7、CuDNN 、cudaconvnet2 、fbfft、Nervana Systems等，NervanaSys表現突出.</p>

<p>*<a href="http://islpc21.is.cs.cmu.edu:3000/lti_catalogue">《This catalogue lists resources developed by faculty and students of the Language Technologies Institute.》</a><br/>
介紹:卡耐基梅隆大學計算機學院語言技術系的資源大全,包括大量的NLP開源軟件工具包，基礎數據集，論文集，數據挖掘教程，機器學習資源.</p>

<p>*<a href="https://github.com/mayank93/Twitter-Sentiment-Analysis">《Sentiment Analysis on Twitter》</a><br/>
介紹:Twitter情感分析工具SentiTweet,<a href="http://pan.baidu.com/s/1i3kXPlj">視頻+講義</a>.</p>

<p>*<a href="http://machinelearning.wustl.edu/mlpapers/venues">《Machine Learning Repository @ Wash U》</a><br/>
介紹:華盛頓大學的Machine Learning Paper Repository.</p>

<p>*<a href="https://github.com/soulmachine/machine-learning-cheat-sheet">《Machine learning cheat sheet》</a><br/>
介紹:機器學習速查表.</p>

<p>*<a href="http://spark-summit.org/east">《Spark summit east 2015 agenda》</a><br/>
介紹:最新的Spark summit會議資料.</p>

<p>*<a href="http://pan.baidu.com/s/1eQkybJG">《Learning Spark》</a><br/>
介紹:Ebook Learning Spark.</p>

<p>*<a href="http://pan.baidu.com/s/1jGot9qe">《Advanced Analytics with Spark, Early Release Edition》</a><br/>
介紹:Ebook Advanced Analytics with Spark, Early Release Edition.</p>

<p>*<a href="http://keg.cs.tsinghua.edu.cn/jietang/">國內機器學習算法及應用領域人物篇:唐傑</a><br/>
介紹:清華大學副教授，是圖挖掘方面的專家。他主持設計和實現的Arnetminer是國內領先的圖挖掘系統，該系統也是多個會議的支持商.</p>

<p>*<a href="http://www.cse.ust.hk/%7Eqyang/">國內機器學習算法及應用領域人物篇:楊強</a><br/>
介紹:遷移學習的國際領軍人物.</p>

<p>*<a href="http://cs.nju.edu.cn/zhouzh/">國內機器學習算法及應用領域人物篇:周誌華</a><br/>
介紹:在半監督學習，multi-label學習和集成學習方面在國際上有一定的影響力.</p>

<p>*<a href="http://ir.hit.edu.cn/%7Ewanghaifeng/whf_pub.htm">國內機器學習算法及應用領域人物篇:王海峰</a><br/>
介紹:信息檢索，自然語言處理，機器翻譯方面的專家.</p>

<p>*<a href="http://www.cs.jhu.edu/%7Ejunwu/">國內機器學習算法及應用領域人物篇:吳軍</a><br/>
介紹:吳軍博士是當前Google中日韓文搜索算法的主要設計者。在Google其間，他領導了許多研發項目，包括許多與中文相關的產品和自然語言處理的項目,他的新個人主頁.</p>

<p>*<a href="http://www.eecs.berkeley.edu/%7Ejunyanz/cat/cat_papers.html">《Cat Paper Collection》</a><br/>
介紹:喵星人相關論文集.</p>

<p>*<a href="http://blog.dato.com/how-to-evaluate-machine-learning-models-part-1-orientation">《How to Evaluate Machine Learning Models, Part 1: Orientation》</a><br/>
介紹:如何評價機器學習模型系列文章, <a href="http://blog.dato.com/how-to-evaluate-machine-learning-models-part-2a-classification-metrics">How to Evaluate Machine Learning Models, Part 2a: Classification Metrics</a>, <a href="http://blog.dato.com/how-to-evaluate-machine-learning-models-part-2b-ranking-and-regression-metrics">How to Evaluate Machine Learning Models, Part 2b: Ranking and Regression Metrics</a>.</p>

<p>*<a href="https://blog.twitter.com/2015/building-a-new-trends-experience">《Building a new trends experience》</a><br/>
介紹:Twitter新trends的基本實現框架.</p>

<p>*<a href="https://www.packtpub.com/big-data-and-business-intelligence/storm-blueprints-patterns-distributed-real-time-computation">《Storm Blueprints: Patterns for Distributed Real-time Computation》</a><br/>
介紹:Storm手冊，<a href="https://github.com/cjie888/storm-trident">中文翻譯</a>.</p>

<p>*<a href="https://github.com/haifengl/smile">《SmileMiner》</a><br/>
介紹:Java機器學習算法庫SmileMiner.</p>

<p>*<a href="http://nlp.csai.tsinghua.edu.cn/%7Ely/talks/cwmt14_tut.pdf">《機器翻譯學術論文寫作方法和技巧》</a><br/>
介紹:機器翻譯學術論文寫作方法和技巧，Simon Peyton Jones的<a href="http://research.microsoft.com/en-us/um/people/simonpj/papers/giving-a-talk/giving-a-talk.htm">How to write a good research paper</a>同類視頻<a href="https://www.youtube.com/watch?v=g3dkRsTqdDA">How to Write a Great Research Paper</a>, <a href="http://vdisk.weibo.com/s/ayG13we2volht">How to paper talk</a>.</p>

<p>*<a href="http://blog.csdn.net/zouxy09/article/details/45288129">《神經網絡訓練中的Tricks之高效BP（反向傳播算法）》</a><br/>
介紹:神經網絡訓練中的Tricks之高效BP,博主的其他博客也挺精彩的.</p>

<p>*<a href="http://www.52cs.org/?p=499">《我和NLP的故事》</a><br/>
介紹:作者是NLP方向的碩士，短短幾年內研究成果頗豐,推薦新入門的朋友閱讀.</p>

<p>*<a href="http://www.cs.ucla.edu/%7Epalsberg/h-number.html">《The h Index for Computer Science 》</a><br/>
介紹:UCLA的Jens Palsberg根據Google Scholar建立了一個計算機領域的H-index牛人列表,我們熟悉的各個領域的大牛絕大多數都在榜上，包括1位諾貝爾獎得主，35位圖靈獎得主，近百位美國工程院/科學院院士，300多位ACM Fellow,在這裏推薦的原因是大家可以在google通過搜索牛人的名字來獲取更多的資源,這份資料很寶貴.</p>

<p>*<a href="http://ttic.uchicago.edu/%7Embansal/papers/acl14_structuredTaxonomy.pdf">《Structured Learning for Taxonomy Induction with Belief Propagation》</a><br/>
介紹:用大型語料庫學習概念的層次關系，如鳥是鸚鵡的上級，鸚鵡是虎皮鸚鵡的上級。創新性在於模型構造，用因子圖刻畫概念之間依存關系，因引入兄弟關系，圖有環，所以用有環擴散（loopy propagation）叠代計算邊際概率（marginal probability）.</p>

<p>*<a href="http://www.stata.com/stata14/bayesian-analysis/">《Bayesian analysis》</a><br/>
介紹: 這是一款貝葉斯分析的商業軟件,官方寫的貝葉斯分析的手冊有250多頁,雖然R語言 已經有類似的項目,但畢竟可以增加一個可選項.</p>

<p><a href="https://github.com/ty4z2008/Qix/blob/master/dl.md">Origin</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine Learning is Fun!(FW)]]></title>
    <link href="http://www.aprilzephyr.com/blog/04302015/machine-learning-is-fun/"/>
    <updated>2015-04-30T14:44:26+08:00</updated>
    <id>http://www.aprilzephyr.com/blog/04302015/machine-learning-is-fun</id>
    <content type="html"><![CDATA[<p>在聽到人們談論機器學習的時候，你是不是對它的涵義只有幾個模糊的認識呢？你是不是已經厭倦了在和同事交談時只能一直點頭？讓我們改變一下吧！</p>

<p>本指南的讀者對象是所有對機器學習有求知欲但卻不知道如何開頭的朋友。我猜很多人已經讀過了“機器學習”的<a href="http://en.wikipedia.org/wiki/Machine_learning">維基百科詞條</a>，倍感挫折，以為沒人能給出一個高層次的解釋。本文就是你們想要的東西。</p>

<p>本文目標在於平易近人，這意味著文中有大量的概括。但是誰在乎這些呢？只要能讓讀者對於ML更感興趣，任務也就完成了。<!--more--></p>

<h3>何為機器學習？</h3>

<p>機器學習這個概念認為，對於待解問題，你無需編寫任何專門的程序代碼，遺傳算法（Generic Algorithms）能夠在數據集上為你得出有趣的答案。對於遺傳算法，不用編碼，而是將數據輸入，它將在數據之上建立起它自己的邏輯。</p>

<p>舉個例子，有一類算法稱為分類算法，它可以將數據劃分為不同的組別。一個用來識別手寫數字的分類算法，不用修改一行代碼，就可以用來將電子郵件分為垃圾郵件和普通郵件。算法沒變，但是輸入的訓練數據變了，因此它得出了不同的分類邏輯。<br/>
<img src="http://www.aprilzephyr.com/images/mlf/m1.png"></p>

<h6><em>機器學習算法是個黑盒，可以重用來解決很多不同的分類問題。</em></h6>

<p>“機器學習”是一個涵蓋性術語，覆蓋了大量類似的遺傳算法。</p>

<h3>兩類機器學習算法</h3>

<p>你可以認為機器學習算法分為兩大類：<strong>監督式學習（Supervised Learning）</strong>和<strong>非監督式學習（Unsupervised Learning）</strong>。兩者區別很簡單，但卻非常重要。<em>(還有第三種學習方式——增強學習(<a href="https://class.coursera.org/neuralnets-2012-001/lecture">Reinforcement Learning</a>)：學習選擇行為來最大化效益。輸出是一個行為或一系列行為，唯一的監督信號是偶爾的有監督的獎勵，目標是選擇行為最大化未來的獎勵，通常使用一個折現因子，因此不用考慮太遠的未來。增強學習是困難的，因為獎勵是延時的，所以很難確定我們哪一步做對做錯，而且有監督的獎勵只是時而出現，並不提供太多信息，所以不能學習很多參數。&mdash;Themis_Sword注)</em></p>

<h4>監督式學習</h4>

<p>假設你是一名房產經紀，生意越做越大，因此你雇了一批實習生來幫你。但是問題來了——你可以看一眼房子就知道它到底值多少錢，實習生沒有經驗，不知道如何估價。</p>

<p>為了幫助你的實習生（也許是為了解放你自己去度個假），你決定寫個小軟件，可以根據房屋大小、地段以及類似房屋的成交價等因素來評估你所在地區房屋的價值。</p>

<p>你把3個月來城裏每筆房屋交易都寫了下來，每一單你都記錄了一長串的細節——臥室數量、房屋大小、地段等等。但最重要的是，你寫下了最終的成交價：</p>

<h6><em>這是我們的“訓練數據”。</em></h6>

<p><img src="http://www.aprilzephyr.com/images/mlf/m2.png"></p>

<p>我們要利用這些訓練數據來編寫一個程序來估算該地區其他房屋的價值：<br/>
<img src="http://www.aprilzephyr.com/images/mlf/m3.png"></p>

<p>這就稱為<strong>監督式學習</strong>。你已經知道每一棟房屋的售價，換句話說，你知道問題的答案，並可以反向找出解題的邏輯。</p>

<p>為了編寫軟件，你將包含每一套房產的訓練數據輸入你的機器學習算法。算法嘗試找出應該使用何種運算來得出價格數字。</p>

<p>這就像是算術練習題，算式中的運算符號都被擦去了：<br/>
<img src="http://www.aprilzephyr.com/images/mlf/m4.png"></p>

<h6><em>天哪！一個陰險的學生將老師答案上的算術符號全擦去了。</em></h6>

<p>看了這些題，你能明白這些測驗裏面是什麽樣的數學問題嗎？你知道，你應該對算式左邊的數字“做些什麽”以得出算式右邊的答案。</p>

<p>在監督式學習中，你是讓計算機為你算出數字間的關系。而一旦你知道了解決這類特定問題所需要的數學方法後，你就可以解答同類的其它問題了。</p>

<h4>非監督式學習</h4>

<p>讓我們回到開頭那個房地產經紀的例子。要是你不知道每棟房子的售價怎麽辦？即使你所知道的只是房屋的大小、位置等信息，你也可以搞出很酷的花樣。這就是所謂的<strong>非監督式學習</strong>。<br/>
<img src="http://www.aprilzephyr.com/images/mlf/m5.png"></p>

<h6><em>即使你不是想去預測未知的數據（如價格），你也可以運用機器學習完成一些有意思的事。</em></h6>

<p>這就有點像有人給你一張紙，上面列出了很多數字，然後對你說:“我不知道這些數字有什麽意義，也許你能從中找出規律或是能將它們分類，或是其它什麽-祝你好運！”</p>

<p>你該怎麽處理這些數據呢？首先，你可以用個算法自動地從數據中劃分出不同的細分市場。也許你會發現大學附近的買房者喜歡戶型小但臥室多的房子，而郊區的買房者偏好三臥室的大戶型。這些信息可以直接幫助你的營銷。</p>

<p>你還可以作件很酷的事，自動找出房價的離群數據，即與其它數據迥異的值。這些鶴立雞群的房產也許是高樓大廈，而你可以將最優秀的推銷員集中在這些地區，因為他們的傭金更高。</p>

<p>本文余下部分我們主要討論監督式學習，但這並不是因為非監督式學習用處不大或是索然無味。實際上，隨著算法改良，不用將數據和正確答案聯系在一起，因此非監督式學習正變得越來越重要。</p>

<h6>老學究請看:還有很多其它種類的機器學習算法。但初學時這樣理解不錯了。</h6>

<h3>太酷了，但是評估房價真能被看作“學習”嗎？</h3>

<p>作為人類的一員，你的大腦可以應付絕大多數情況，並且沒有任何明確指令也能夠學習如何處理這些情況。如果你做房產經紀時間很長，你對於房產的合適定價、它的最佳營銷方式以及哪些客戶會感興趣等等都會有一種本能般的“感覺”。強人工智能（Strong AI）研究的目標就是要能夠用計算機復制這種能力。</p>

<p>但是目前的機器學習算法還沒有那麽好——它們只能專註於非常特定的、有限的問題。也許在這種情況下，“學習”更貼切的定義是“在少量範例數據的基礎上找出一個等式來解決特定的問題”。</p>

<p>不幸的是，“機器在少量範例數據的基礎上找出一個等式來解決特定的問題”這個名字太爛了。所以最後我們用“機器學習”取而代之。</p>

<p>當然，要是你是在50年之後來讀這篇文章，那時我們已經得出了強人工智能算法，而本文看起來就像個老古董。未來的人類，你還是別讀了，叫你的機器仆人給你做份三明治吧。</p>

<h3>讓我們寫代碼吧!</h3>

<p>前面例子中評估房價的程序，你打算怎麽寫呢？往下看之前，先思考一下吧。</p>

<p>如果你對機器學習一無所知，很有可能你會嘗試寫出一些基本規則來評估房價，如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood):
</span><span class='line'>  price = 0
</span><span class='line'> 
</span><span class='line'>  # In my area, the average house costs $200 per sqft
</span><span class='line'>  price_per_sqft = 200
</span><span class='line'> 
</span><span class='line'>  if neighborhood == "hipsterton":
</span><span class='line'>    # but some areas cost a bit more
</span><span class='line'>    price_per_sqft = 400
</span><span class='line'> 
</span><span class='line'>  elif neighborhood == "skid row":
</span><span class='line'>    # and some areas cost less
</span><span class='line'>    price_per_sqft = 100
</span><span class='line'> 
</span><span class='line'>  # start with a base price estimate based on how big the place is
</span><span class='line'>  price = price_per_sqft * sqft
</span><span class='line'> 
</span><span class='line'>  # now adjust our estimate based on the number of bedrooms
</span><span class='line'>  if num_of_bedrooms == 0:
</span><span class='line'>    # Studio apartments are cheap
</span><span class='line'>    price = price — 20000
</span><span class='line'>  else:
</span><span class='line'>    # places with more bedrooms are usually
</span><span class='line'>    # more valuable
</span><span class='line'>    price = price + (num_of_bedrooms * 1000)
</span><span class='line'> 
</span><span class='line'> return price</span></code></pre></td></tr></table></div></figure>


<p>假如你像這樣瞎忙幾個小時，也許會取得一點成效，但是你的程序永不會完美，而且當價格變化時很難維護。</p>

<p>如果能讓計算機找出實現上述函數功能的辦法，這樣豈不更好？只要返回的房價數字正確，誰會在乎函數具體幹了些什麽呢？</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood):
</span><span class='line'>  price = &lt;computer, plz do some math for me&gt;
</span><span class='line'> 
</span><span class='line'>  return price</span></code></pre></td></tr></table></div></figure>


<p>考慮這個問題的一種角度是將房價看做一碗美味的湯，而湯中成分就是臥室數、面積和地段。如果你能算出每種成分對最終的價格有多大影響，也許就能得到各種成分混合起來形成最終價格的具體比例。</p>

<p>這樣可以將你最初的程序（全是瘋狂的if else語句）簡化成類似如下的樣子：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood):
</span><span class='line'> price = 0
</span><span class='line'> 
</span><span class='line'> # a little pinch of this
</span><span class='line'> price += num_of_bedrooms * .841231951398213
</span><span class='line'> 
</span><span class='line'> # and a big pinch of that
</span><span class='line'> price += sqft * 1231.1231231
</span><span class='line'> 
</span><span class='line'> # maybe a handful of this
</span><span class='line'> price += neighborhood * 2.3242341421
</span><span class='line'> 
</span><span class='line'> # and finally, just a little extra salt for good measure
</span><span class='line'> price += 201.23432095
</span><span class='line'> 
</span><span class='line'> return price</span></code></pre></td></tr></table></div></figure>


<p>請註意那些用粗體標註的神奇數字
&mdash;&ndash;<strong>.841231951398213, 1231.1231231,2.3242341421</strong>和<strong>201.23432095</strong>。它們稱為<strong>權重</strong>。如果我們能找出對每棟房子都適用的完美權重，我們的函數就能預測所有的房價！</p>

<p>找出最佳權重的一種笨辦法如下所示：</p>

<h4>步驟1：</h4>

<p>首先，將每個權重都設為1.0：
&#8220;`
def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood):
  price = 0</p>

<p>  # a little pinch of this
  price += num_of_bedrooms * 1.0</p>

<p>  # and a big pinch of that
  price += sqft * 1.0</p>

<p>  # maybe a handful of this
  price += neighborhood * 1.0</p>

<p>  # and finally, just a little extra salt for good measure
  price += 1.0</p>

<p>  return price
 &#8220;`</p>

<h4>步驟2：</h4>

<p>將每棟房產帶入你的函數運算，檢驗估算值與正確價格的偏離程度：<br/>
<img src="http://www.aprilzephyr.com/images/mlf/m6.png"></p>

<h6><em>運用你的程序預測房屋價格。</em></h6>

<p>例如：上表中第一套房產實際成交價為25萬美元，你的函數估價為17.8萬，這一套房產你就差了7.2萬。</p>

<p>再將你的數據集中的每套房產估價偏離值平方後求和。假設數據集中有500套房產交易，估價偏離值平方求和總計為86,123,373美元。這就反映了你的函數現在的“正確”程度。</p>

<p>現在，將總計值除以500，得到每套房產的估價偏離平均值。將這個平均誤差值稱為你函數的代價。</p>

<p>如果你能調整權重使得這個代價變為0，你的函數就完美了。它意味著，根據輸入的數據，你的程序對每一筆房產交易的估價都是分毫不差。而這就是我們的目標——嘗試不同的權重值以使代價盡可能的低。</p>

<h4>步驟3：</h4>

<p>不斷重復步驟2，嘗試<strong>所有可能的權重值組合</strong>。哪一個組合使得代價最接近於0，它就是你要使用的，你只要找到了這樣的組合，問題就得到了解決!</p>

<h3>思想擾動時間</h3>

<p>這太簡單了，對吧？想一想剛才你做了些什麽。你取得了一些數據，將它們輸入至三個通用的簡單步驟中，最後你得到了一個可以對你所在區域的房屋進行估價的函數。房價網，要當心咯！
但是下面的事實可能會擾亂你的思想：</p>

<p>1.過去40年來，很多領域（如語言學/翻譯學）的研究表明，這種通用的“攪動數據湯”（我編造的詞）式的學習算法已經勝過了需要利用真人明確規則的方法。機器學習的“笨”辦法最終打敗了人類專家。</p>

<p>2.你最後寫出的函數真是笨，它甚至不知道什麽是“面積”和“臥室數”。它知道的只是攪動，改變數字來得到正確的答案。</p>

<p>3.很可能你都不知道為何一組特殊的權重值能起效。所以你只是寫出了一個你實際上並不理解卻能證明的函數。
4.試想一下，你的程序裏沒有類似“面積”和“臥室數”這樣的參數，而是接受了一組數字。假設每個數字代表了你車頂安裝的攝像頭捕捉的畫面中的一個像素，再將預測的輸出不稱為“價格”而是叫做“方向盤轉動度數”，<strong>這樣你就得到了一個程序可以自動操縱你的汽車了！</strong></p>

<p>太瘋狂了，對吧？</p>

<h3>步驟3中的“嘗試每個數字”怎麽回事？</h3>

<p>好吧，當然你不可能嘗試所有可能的權重值來找到效果最好的組合。那可真要花很長時間，因為要嘗試的數字可能無窮無盡。
為避免這種情況，數學家們找到了很多<a href="http://en.wikipedia.org/wiki/Gradient_descent">聰明的辦法</a>來快速找到優秀的權重值，而不需要嘗試過多。下面是其中一種：
首先，寫出一個簡單的等式表示前述步驟2：</p>

<h6><em>這是你的代價函數。</em></h6>

<p><img src="http://www.aprilzephyr.com/images/mlf/m7.png"></p>

<p>接著，讓我們將這同一個等式用機器學習的數學術語（現在你可以忽略它們）進行重寫：</p>

<h6><em>θ表示當前的權重值。 J(θ) 意為“當前權重值對應的代價”。</em></h6>

<p><img src="http://www.aprilzephyr.com/images/mlf/m8.png"></p>

<p>這個等式表示我們的估價程序在當前權重值下偏離程度的大小。
如果將所有賦給臥室數和面積的可能權重值以圖形形式顯示，我們會得到類似下圖的圖表：<br/>
<img src="http://www.aprilzephyr.com/images/mlf/m9.png"></p>

<h6><em>代價函數的圖形像一支碗。縱軸表示代價。</em></h6>

<p>圖中藍色的最低點就是代價最低的地方——即我們的程序偏離最小。最高點意味著偏離最大。所以，如果我們能找到一組權重值帶領我們到達圖中的最低點，我們就找到了答案！</p>

<p><img src="http://www.aprilzephyr.com/images/mlf/m10.png"></p>

<p>因此，我們只需要調整權重值使我們在圖上能向著最低點“走下坡路”。如果對於權重的細小調節能一直使我們保持向最低點移動，那麽最終我們不用嘗試太多權重值就能到達那裏。</p>

<p>如果你還記得一點微積分的話，你也許記得如果你對一個函數求導，結果會告訴你函數在任一點的斜率。換句話說，對於圖上給定一點，它告訴我們那條路是下坡路。我們可以利用這一點朝底部進發。</p>

<p>所以，如果我們對代價函數關於每一個權重求偏導，那麽我們就可以從每一個權重中減去該值。這樣可以讓我們更加接近山底。一直這樣做，最終我們將到達底部，得到權重的最優值。（讀不懂？不用擔心，接著往下讀）。</p>

<p>這種找出最佳權重的辦法被稱為<strong>批量梯度下降</strong>，上面是對它的高度概括。如果想搞懂細節，不要害怕，繼續<a href="https://hbfs.wordpress.com/2012/04/24/introduction-to-gradient-descent/">深入下去</a>吧。</p>

<p>當你使用機器學習算法庫來解決實際問題，所有這些都已經為你準備好了。但明白一些具體細節總是有用的。</p>

<h3>還有什麽你隨便就略過了？</h3>

<p>上面我描述的三步算法被稱為<strong>多元線性回歸</strong>。你估算等式是在求一條能夠擬合所有房價數據點的直線。然後，你再根據房價在你的直線上可能出現的位置用這個等式來估算從未見過的房屋的價格。這個想法威力強大，可以用它來解決“實際”問題。</p>

<p>但是，我為你展示的這種方法可能在簡單的情況下有效，它不會在所有情況下都有用。原因之一是因為房價不會一直那麽簡單地跟隨一條連續直線。</p>

<p>但是，幸運的是，有很多辦法來處理這種情況。對於非線性數據，很多其他類型的機器學習算法可以處理（如神經網絡或有核向量機）。還有很多方法運用線性回歸更靈活，想到了用更復雜的線條來擬合。在所有的情況中，尋找最優權重值這一基本思路依然適用。</p>

<p>還有，我忽略了<strong>過擬合</strong>的概念。很容易碰上這樣一組權重值，它們對於你原始數據集中的房價都能完美預測，但對於原始數據集之外的任何新房屋都預測不準。這種情況的解決之道也有不少（如正則化以及使用交叉驗證數據集）。學會如何處理這一問題對於順利應用機器學習至關重要。</p>

<p>換言之，基本概念非常簡單，要想運用機器學習得到有用的結果還需要一些技巧和經驗。但是，這是每個開發者都能學會的技巧。</p>

<h3>機器學習法力無邊嗎？</h3>

<p>一旦你開始明白機器學習技術很容易應用於解決貌似很困難的問題（如手寫識別），你心中會有一種感覺，只要有足夠的數據，你就能夠用機器學習解決任何問題。只需要將數據輸入進去，就能看到計算機變戲法一樣找出擬合數據的等式。</p>

<p>但是很重要的一點你要記住，機器學習只能對用你占有的數據實際可解的問題才適用。</p>

<p>例如，如果你建立了一個模型來根據每套房屋內盆栽數量來預測房價，它就永遠不會成功。房屋內盆栽數量和房價之間沒有任何的關系。所以，無論它怎麽去嘗試，計算機也推導不出兩者之間的關系。<br/>
<img src="http://www.aprilzephyr.com/images/mlf/m11.png"></p>

<h6><em>你只能對實際存在的關系建模。</em></h6>

<h3>怎樣深入學習機器學習</h3>

<p>我認為，當前機器學習的最大問題是它主要活躍於學術界和商業研究組織中。對於圈外想要有個大體了解而不是想成為專家的人們，簡單易懂的學習資料不多。但是這一情況每一天都在改善。</p>

<p>吳恩達教授（Andrew Ng）在Coursera上的<a href="https://www.coursera.org/course/ml">機器學習免費課程</a>非常不錯。我強烈建議由此入門。任何擁有計算機科學學位、還能記住一點點數學的人應該都能理解。</p>

<p>另外，你還可以下載安裝<a href="http://scikit-learn.org/stable/">SciKit-Learn</a>，用它來試驗成千上萬的機器學習算法。它是一個python框架，對於所有的標準算法都有“黑盒”版本。</p>

<ol>
<li><a href="https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471">Origin</a></li>
<li><a href="https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471">中文翻譯</a></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[神經網絡(書摘).]]></title>
    <link href="http://www.aprilzephyr.com/blog/04302015/shen-jing-wang-luo-shu-zhai/"/>
    <updated>2015-04-30T11:15:35+08:00</updated>
    <id>http://www.aprilzephyr.com/blog/04302015/shen-jing-wang-luo-shu-zhai</id>
    <content type="html"><![CDATA[<p><img src="http://www.aprilzephyr.com/images/astonishing.jpg"></p>

<p> “……我相信，對一個模型的最好的檢驗是它的設計者能否回答這些問題：‘現在你知道哪些原本不知道的東西？’以及‘你如何證明它是否是對的？’”  ——詹姆斯·鮑爾（<a href="http://en.wikipedia.org/wiki/James_M._Bower">James M.Bower</a>)<!--more--></p>

<p>神經網絡是由具有各種相互聯系的單元組成的集合。每個單元具有極為簡化的神經元的特性。神經網絡常常被用來模擬神經系統中某些部分的行為，生產有用的商業化裝置以及檢驗腦是如何工作的一般理論。</p>

<p>神經科學家們究竟為什麽那麽需要理論呢？如果他們能了解單個神經元的確切行為，他們就有可能預測出具有相互作用的神經元群體的特性。令人遺憾的是，事情並非如此輕而易舉。事實上，單個神經元的行為通常遠不那麽簡單，而且神經元幾乎總是以一種復雜的方式連接在一起。此外，整個系統通常是高度非線性的。線性系統，就其最簡單形式而言，當輸入加倍時，它的輸出也嚴格加倍——即輸出與輸入呈比例關系。①例如，在池塘的表面，當
兩股行進中的小湍流彼此相遇時，它們會彼此穿過而互不幹擾。為了計算兩股小水波聯合產生的效果，人們只需把第一列波與第二列波的效果在空間和時間的每一點上相加即可。這樣，每一列波都獨立於另一列的行為。對於大振幅的波則通常不是這樣。物理定律表明，大振幅情況下均衡性被打破。沖破一列波的過程是高度非線性的：一旦振幅超過某個閾值，波的行為完全以全新的方式出現。那不僅僅是“更多同樣的東西”，而是某些新的特性。非線性行為在日常生活中很普遍，特別是在愛情和戰爭當中。正如歌中的：“吻她一次遠不及吻她兩次的一半那麽美妙。”</p>

<p>如果一個系統是非線性的，從數學上理解它通常比線性系統要困難得多。它的行為可能更為復雜。因此對相互作用的神經元群體進行預測變得十分困難，特別是最終的結果往往與直覺相反。</p>

<p>高速數字計算機是近50年來最重要的技術發展之一。它時常被稱作馮.諾依曼計算機，以紀念這位傑出的科學家、計算機的締造者。由於計算機能像人腦一樣對符號和數字進行操作，人們自然地想像腦是某種形式相當復雜的馮·諾依曼計算機。這種比較，如果陷入極端的話，將導致不切實際的理論。</p>

<p>計算機是構建在固有的高速組件之上的。即便是個人計算機，其基本周期，或稱時鐘頻率，也高於每秒1000萬次操作。相反地，一個神經元的典型發放率僅僅在每秒100個脈沖的範圍內。計算機要快上百萬倍。而像克雷型機那樣的高速超級計算機速度甚至更高。大致說來，計算機的操作是序列式的，即一條操作接著一條操作。與此相反，腦的工作方式則通常是大規模並行的，例如，從每只眼睛到達腦的軸突大約有100萬個，它們全都同時工作。
在系統中這種高度的並行情況幾乎重復出現在每個階段。這種連線方式在某種程度上彌補了神經元行為上的相對緩慢性。它也意味著即使失去少數分散的神經元也不大可能明顯地改變腦的行為。用專業術語講，腦被稱作“故障弱化”（Degrade Gracefully)。而計算機則是脆弱的，哪怕是對它極小的損傷，或是程序中的一個小錯誤，也會引起大的災難。計算機中出現錯誤則是災難性的（Degrade Catastrophically)。</p>

<p>計算機在工作中是高度穩定的。因為其單個組件是很可靠的，當給定相同的輸入時通常產生完全同樣的輸出。反之，單個神經元則具有更多的變化。它們受可以調節其行為的信號所支配，有些特性邊“計算”邊改變。</p>

<p>一個典型的神經元可能具有來自各處的上百乃至數萬個輸入，其軸突又有大量投射。而計算機的一個基本元件——晶體管，則只有極少數的輸入和輸出。</p>

<p>在計算機中，信息被編碼成由0和1組成的脈沖序列。計算機通過這種形式高度精確地將信息從一個特定的地方傳送到另一個地方。信息可以到達特定的地址，提取或者改變那裏所儲存的內容。這樣就能夠將信息存入記憶體的某個特殊位置，並在以後的某些時刻進一步加以利用。這種精確性在腦中是不會出現的。盡管一個神經元沿它的軸突發送的脈沖的模式（而不僅僅是其平均發放率）可能攜帶某些信息，但並不存在精確的由脈沖編碼的信息。①這樣，記憶必然將以不同的形式“存儲”。</p>

<p>腦看起來一點也不像通用計算機。腦的不同部分，甚至是新皮層的不同部分，都是專門用來處理不同類型的信息的（至少在某種程度上是這樣的）。看來大多數記憶存儲在進行當前操作的那個地方。所有這些與傳統的馮·諾依曼計算機完全不同，因為執行計算機的基本操作（如加法.乘法等等）僅在一個或少數幾個地方，而它的記憶卻存貯在許多很不同的地方。</p>

<p>最後，計算機是由工程師精心設計出來的，而腦則是動物經自然選擇一代又一代進化而來的。這就產生了如第一章所述的本質上不同的設計形式。</p>

<p>人們習慣於從硬件和軟件的角度來談論計算機。由於人們編寫軟件（計算機程序）時幾乎不必了解硬件（回路等）的細節，所以人們——特別是心理學家——爭論說沒必要了解有關腦的“硬件”的任何知識。實際上想把這種理論強加到腦的操作過程中是不恰當的，腦的硬件與軟件之間並沒有明顯的差異。對於這種探討的一種合理的解釋是，雖然腦的活動是高度並行的，在所有這些平行操作的頂端有某些形式的（由註意控制的）序列機制，因而，在腦的操作的較高層次，在那些遠離感覺輸入的地方，可以膚淺地說腦與計算機有某種相似之處。</p>

<p>人們可以從一個理論途徑的成果來對它作判斷。計算機按編寫的程序執行，因而擅長解決諸如大規模數字處理、嚴格的邏輯推理以及下棋等某些類型的問題。這些事情大多數人都沒有它們完成得那麽快、那麽好。但是，面對常人能快速、不費氣力就能完成的任務，如觀察物體並理解其意義，即便是最現代的計算機也顯得無能為力。</p>

<p>近幾年在設計新一代的、以更加並行方式工作的計算機方面取得了重要進展。大多數設計使用了許多小型計算機，或是小型計算機的某些部件。它們被連接在一起，並同時運行。由一些相當復雜的設備來處理小計算機之間的信息交換並對計算進行全局控制。像天氣預測等類似問題，其基本要素在多處出現。此時超級計算機特別有用。</p>

<p>人工智能界也采取了行動設計更具有腦的特點的程序。他們用一種模糊邏輯取代通常計算中使用的嚴格的邏輯。命題不再一定是真的或假的，而只需是具有更大或更小的可能性。程序試圖在一組命題中發現具有最大可能性的那種組合，並以之作為結論，而不是那些它認為可能性較小的結論。</p>

<p>在概念的設置上，這種方法確實比早期的人工智能方法與腦更為相像，但在其他方面，特別是在記憶的存貯上，則不那麽像腦。因此，要檢查它與真實的腦在所有層次上行為的相似性可能會有困難。</p>

<p>一群原先很不知名的理論工作者發展了一種更具有腦的特性的方法。如今它被稱為PDP方法（即平行分布式處理）。這個話題有很長的歷史，我只能概述一二。在1943年沃侖·麥卡洛克（Warrenc McCulloch）和沃爾特·皮茲（Walter Pitts）的工作是這方面最早的嘗試之一。他們表明，在原則上由非常簡單的單元連接在一起組成的“網絡”可以對任何邏輯和算術函數進行計算。因為網絡的單元有些像大大簡化的神經元，它現在常被稱作“神經網絡”。</p>

<p>這個成就非常令人鼓舞，以致它使許多人受到誤導，相信腦就是這樣工作的。或許它對現代計算機的設計有所幫助，但它的最引人註目的結論就腦而言則是極端錯誤的。</p>

<p>下一個重要的進展是弗蘭克·羅森布拉特（Frank Rosenblatt）發明的一種非常簡單的單層裝置，他稱之為感知機（Perceptron)。意義在於，雖然它的連接最初是隨機的，它能使用一種簡單而明確的規則改變這些連接，因而可以教會它執行某些簡單的任務，如識別固定位置的印刷字母。感知機的工作方式是，它對任務只有兩種反應：正確或是錯誤。你只需告訴它它所作出的（暫時的）回答是否正確。然後它根據一種感知機學習規則來改變其連接。羅森布拉特證明，對於某一類簡單的問題——“線性可分”的問題——感知機通過有限次訓練就能學會正確的行為。</p>

<p>由於這個結果在數學上很優美，從而吸引了眾人的註目。只可惜它時運不濟，它的影響很快就消退了。馬文·明斯基（MarVinMinsky)和西摩·佩伯特（Segmour Papert)證明感知機的結構及學習規則無法執行“異或問題”（如，判斷這是蘋果還是桔子，但不是二者皆是），因而也不可能學會它。他們寫了一本書，通篇詳述了感知機的局限性。這在許多年內扼殺了人們對感知機的興趣（明斯基後來承認做得過分了）。此問大部分工作將註意力轉向人工智能方法。①</p>

<p>用簡單單元構建一個多層網絡，使之完成簡單的單層網絡所無法完成的異或問題（或類似任務），這是可能的。這種網絡必定具有許多不同層次上的連接，問題在於，對哪些最初是隨機的連接進行修改才能使網絡完成所要求的操作。如果明斯基和佩伯特為這個問題提供了解答，而不是把感知機打入死路的話，他們的貢獻會更大些。</p>

<p>下一個引起廣泛註意的發展來自約翰·霍普菲爾德（John Hop-field)，一位加利福尼亞州理工學院的物理學家，後來成為分子生物學家和腦理論家。1982年他提出了一種網絡，現在被稱為霍普菲爾德網絡(見圖53）。這是一個具有自反饋的簡單網絡。每個單元只能有兩種輸出：一1（表示抑制）或十1（表示興奮）。但每個單元具有多個輸入。每個連接均被指派一個特定的強度。在每個時刻單元把來自它的全部連接的效果(2)總和起來。如果這個總和大於0則置輸出狀態為十1（平均而言，當單元興奮性輸入大於抑制性輸人時，則輸出為正），否則就輸出一1。有些時候這意味著一個單元的輸出會因為來自其他單元的輸入發生了改變而改變。</p>

<p>盡管如此，仍有不少理論工作者默默無聞地繼續工作。這其中包括斯蒂芬.格羅斯伯格（Stephen Grossberg），吉姆·安德森（Jim Anderson），托伊沃.科霍寧（TeuvoKohonen）和戴維·威爾肖（Devid Willshaw）。(2)每個輸入對單元的影響是將當前的輸入信號（+1或-1）與其相應的權值相乘而得到的。（如果當前信號是-1，權重是+2，則影響為-2。）</p>

<p>計算將被一遍遍地反復進行，直到所有單元的輸出都穩定為止。①在霍普菲爾德網絡中，所有單元的狀態並不是同時改變的，而是按隨機次序一個接一個進行，霍普菲爾德從理論上證明了，給定一組權重（連接強度）以及任何輸入，網絡將不會無限制地處於漫遊狀態，也不會進入振蕩，而是迅速達到一個穩態。①</p>

<p>霍普菲爾德的論證令人信服，表達也清晰有力。他的網絡對數學家和物理學家有巨大的吸引力，他們認為終於找到了一種他們可以涉足腦研究的方法（正如我們在加利福尼亞州所說的）。雖然這個網絡在許多細節上嚴重違背生物學，但他們並不對此感到憂慮。</p>

<p>如何調節所有這些連接的強度呢？194年，加拿大心理學家唐納德·赫布（Donald Hebb）出版了《行為的組織》一書。當時人們就像現在一樣普遍相信，在學習過程中，一個關鍵因素是神經元的連接（突觸）強度的調節。赫布意識到，僅僅因為一個突觸是活動的，就增加其強度，這是不夠的。他期望一種只在兩個神經元的活動相關時才起作用的機制。他的書中有一個後來被廣泛引用的段落：“當細胞A的一個軸突和細胞B 很近，足以對它產生影響，並且持久地、不斷地參與了對細胞B 的興奮，那麽在這兩個細胞或其中之一會發生某種生長過程或新陳代謝變化，以致於A作為能使B 興奮的細胞之一，它的影響加強了。”這個機制以及某些類似規則，現在稱為“赫布律”。</p>

<p>霍普菲爾德在他的網絡中使用了一種形式的赫布規則來調節連接權重。對於問題中的一種模式，如果兩個單元具有相同的輸出，則它們之間的相互連接權重都設為+1。如果它們具有相反的輸出，則兩個權重均設為-1。大致他說，每個單元激勵它的“朋友”並試圖削弱它的“敵人”。</p>

<p>霍普菲爾德網絡是如何工作的呢？如果網絡輸入的是正確的單元活動模式，它將停留在該狀態。這並沒有什麽特別的，因為此時給予它的就是答案。值得註意的是，如果僅僅給出模式的一小部分作為“線索”，它在經過短暫的演化後，會穩定在正確的輸出即整個模式上，在不斷地調節各個單元的輸出之後，網絡所揭示的是單元活動的穩定聯系。最終它將有效地從某些僅僅與其存貯的“記憶”接近的東西中恢復出該記憶，此外，這種記憶也被稱作是按“內容尋址”的——即它沒有通常計算機中具有的分離的、唯一用於作為“地址”的信號。輸入模式的任何可察覺的部分都將作為地址。這開始與人的記憶略微有些相似了。</p>

<p>請註意記憶並不必存貯在活動狀態中，它也可以完全是被動的，因為它是鑲嵌在權重的模式之中的即在所有各個單元之間的連接強度之中。網絡可以完全不活動（所有輸出置為0），但只要有信號輸入，網絡突然活動起來並在很短時間內進入與其應當記住的模式相對應的穩定的活動狀態。據推測，人類長期記憶的回憶具有這種一般性質（只是活動模式不能永久保持）。你能記住大量現在一時想不起來的事情。</p>

<p>神經網絡（特別是霍普菲爾德網絡）能“記住”一個模式，但是除此以外它還能再記住第二個模式嗎？如果幾個模式彼此不太相似，一個網絡是能夠全部記住這幾個不同模式，即給出其中一個模式的足夠大的一部分，網絡經過少數幾個周期後將輸出該模式。因為任何一個記憶都是分布在許多連接當中的，所以整個系統中記憶是分布式的。因為任何一個連接都可能包含在多個記憶中，因而記憶是可以疊加的。此外，記憶具有魯棒性，改變少數連接通常不會顯著改變網絡的行為。</p>

<p>為了實現這些特性就需要付出代價，這不足為奇。如果將過多的記憶加到網絡之中則很容易使它陷入混亂。即使給出線索，甚至以完整的模式作為輸入，網絡也會產生毫無意義的輸出。①</p>

<p>有人提出這是我們做夢時出現的現象（弗洛伊德稱之為“凝聚”——Condensation），但這是題外話。值得註意的是，所有這些特性是“自然發生”的。它們並不是網絡設計者精心設置的，而是由單元的本性、它們連接的模式以及權重調節規則所決定的。</p>

<p>霍普菲爾德網絡還有另一個性質，即當幾個輸人事實上彼此大致相似時，在適當計算網絡的連接權重後，它“記住”的將是訓練的模式的某種平均。這是另一個與腦有些類似的性質。對我們人類而言，當我們聽某個特定的聲調時，即便它在一定範圍內發生變化，我們也會覺得它是一樣的。輸入是相似但不同的，而輸出——我們所聽到的——則是一樣的。</p>

<p>這些簡單網絡是不能和腦的復雜性相提並論的，但這種簡化確實使我們可能對它們的行為有所了解，即使是簡單網絡中出現的特點也可能出現在具有相同普遍特性的更復雜的網絡中，此外，它們向我們提供了多種觀點，表明特定的腦回路所可能具有的功能。例如，海馬中有一個稱為CA3的區域，它的連接事實上很像一個按內容尋址的網絡。當然，這是否正確尚需實驗檢驗。</p>

<p>有趣的是，這些簡單的神經網絡具有全息圖的某些特點。在全息圖中，幾個影像可以彼此重疊地存貯在一起；全息圖的任何一部分都能用來恢復整個圖像，只不過清晰度會下降；全息圖對於小的缺陷是魯棒的。對腦和全息圖兩者均知之甚少的人經常會熱情地支持這種類比。幾乎可以肯定這種比較是沒有價值的。原因有兩個。詳細的數學分析表明神經網絡和全息圖在數學上是不同的。更重要的是，雖然神經網絡是由那些與真實神經元有些相似的單元
構建的，沒有證據表明腦中具有全息圖所需的裝置或處理過程。（1）</p>

<p>一本更新的書產生了巨大的沖擊力，這就是戴維·魯梅爾哈特（David Rumelhart）、詹姆斯·麥克萊蘭（James McClelland）和PDP小組所編的一套很厚的兩卷著作《平行分布式處理》（1)。該書於1986年問世，並很快至少在學術界成為最暢銷書。名義上我也是PDP小組的成員，並和淺沼智行（Chiko Asanuma）合寫了其中的一個章節。不過我起的作用很小。我幾乎只有一個貢獻，就是堅持要求他們停止使用神經元一詞作為他們網絡的單元。</p>

<p>加利福尼亞州立大學聖叠戈分校心理系離索爾克研究所僅有大約一英裏。在70年代末80年代初我經常步行去參加他們的討論小組舉行的小型非正式會議。那時我時常漫步的地方如今已變成了巨大的停車場。生活的步伐越來越快，我現在已改為驅車飛馳於兩地之間了。</p>

<p>研究小組當時是由魯梅爾哈特和麥克萊蘭領導的，但是不久麥克萊蘭就離開前往東海岸了。他們倆最初都是心理學家，但他們對符號處理器感到失望並共同研制了處理單詞的“相互作用激勵器”的模型。在克裏斯托夫·朗格特-希金斯（Christopher Longuet-Higgins）的另一位學生傑弗裏·希爾頓（Geoffrey Hinton）的鼓勵下，他們著手研究一個更加雄心勃勃的“聯結主義”方案。他們采納了平行分布式處理這個術語，因為它比以前的術語——聯想記憶②——的覆蓋面更廣。</p>

<p>在人們發明網絡的初期，一些理論家勇敢地開始了嘗試。他們把一些仍顯笨拙的小型電子回路（其中常包括有老式繼電器）連接在一起來模擬他們的非常簡單的網絡。現在已發展出了復雜得多的神經網絡，這得益於現代計算機的運算速度得到了極大的提高，也很便宜。現在可以在計算機（這主要是數字計算機）上模擬檢驗關於網絡的新思想，而不必像早期的研究那樣僅靠粗糙的模擬線路或是用相當困難的數學論證。</p>

<p>1986年出版的《平行分布式處理》一書從1981年底開始經過了很長時間的醞釀。這很幸運，因為它是一個特殊算法的最新發展（或者說是它的復興或應用），在其早期工作基礎上，很快給人留下了深刻的印象。該書的熱情讀者不僅包括腦理論家和心理學家，還有數學家、物理學家和工程師，甚至有人工智能領域的工作者。不過後者最初的反應是相當敵視的。最終神經科學家和分子生物學家也對它的消息有所耳聞。</p>

<p>該書的副標題是“認知微結構的探索”。它是某種大雜燴，但是其中一個的特殊的算法產生了驚人的效果。該算法現在稱作“誤差反傳算法”，通常簡稱為“反傳法”。為了理解這個算法，你需要知道一些關於學習算法的一般性知識。</p>

<p>在神經網絡有些學習形式被稱作是“無教師的”。這意味著沒有外界輸入的指導信息。對任何連接的改變只依賴於網絡內部的局部狀態。簡單的赫布規則具有這種特點。與之相反，在有教師學習中，從外部向網絡提供關於網絡執行狀況的指導信號。</p>

<p>無教師學習具有很誘人的性質，因為從某種意義上說網絡是在自己指導自己。理論家們設計了一種更有效的學習規則，但它需要一位“教師”來告訴網絡它對某些輸入的反應是好、是差還是很糟。這種規則中有一個稱作“δ律”。</p>

<p>訓練一個網絡需要有供訓練用的輸入集合，稱作“訓練集”。很快我們在討論網絡發音器（NETtalk）時將看到一個這樣的例子。這有用的訓練集必須是網絡在訓練後可能遇到的輸入的合適的樣本。通常需要將訓練集的信號多次輸入，因而在網絡學會很好地執行之前需要進行大量的訓練。其部分原因是這種網絡的連接通常是隨機的。而從某種意義上講，腦的初始連接是由遺傳機制控制的，通常不完全是隨機的。</p>

<p>網絡是如何進行訓練的呢？當訓練集的一個信號被輸入到網絡中，網絡就會產生一個輸出。這意味著每個輸出神經元都處在一個特殊的活動狀態。教師則用信號告訴每個輸出神經元它的誤差，即它的狀態與正確之間的差異，δ這個名稱便來源於這個真實活動與要求之間的差異（數學上δ常用來表示小而有限的差異）。網絡的學習規則利用這個信息計算如何調整權重以改進網絡的性能。</p>

<p>Adaline網絡是使用有教師學習的一個較早的例子。它是1960年由伯納德·威德羅（Bernard Widrow）和霍夫（M.E.Hoff）設計的，因此δ律又稱作威德羅-霍夫規則。他們設計規則使得在每一步修正中總誤差總是下降的。①這意味著隨著訓練過程網絡最終會達到一個誤差的極小值。這是毫無疑問的，但還不能確定它是真正的全局極小還是僅僅是個局域極小值。用自然地理的術語說就是，我們達到的是一個火山口中的湖，還是較低的池塘。海洋，還是像死海那樣的凹下去的海（低於海平面的海）？</p>

<p>訓練算法是可以調節的，因而趨近局域極小的步長可大可小。如果步長過大，算法會使網絡在極小值附近跳來跳去（開始時它會沿下坡走，但走得太遠以致又上坡了）。如果步子小，算法就需要極長的時間才能達到極小值的底端。人們也可以使用更精細的調節方案。</p>

<p>反傳算法是有教師學習算法中的一個特殊例子。為了讓它工作，網絡的單元需要具有一些特殊性質。它們的輸出不必是二值的（即，或0，或者＋1或-1），而是分成若幹級。它通常在0到+1之間取值。理論家們盲目地相信這對應於神經元的平均發放率（取最大發放率為＋1），但他們常常說不清應該在什麽時候取這種平均。</p>

<p>如何確定這種“分級”輸出的大小呢？像以前一樣，每個單元對輸入加權求和，但此時不再有一個真實的閾值。如果總和很小，輸出幾乎是0。總和稍大一些時，輸出便增加。當總和很大時，輸出接近於最大值。圖54所示的S形函數（Sigmoid函數）體現了這種輸入總和與輸出間的典型關系。如果將一個真實神經元的平均發放率視為它的輸出，那麽它的行為與此相差不大。</p>

<p>這條看似平滑的曲線有兩個重要性質。它在數學上是“可微的”，即任意一處的斜率都是有限的；反傳算法正依賴於這個特性。更重要的是，這條曲線是非線性的，而真實神經元即是如此。當（內部）輸入加倍時輸出並不總是加倍。這種非線性使得它能處理的問題比嚴格的線性系統更加廣泛。</p>

<p>現在讓我們看一個典型的反傳網絡。它通常具有三個不同的單元層（見圖55）。最底層是輸入層。下一層被稱作“隱單元”層，因為這些單元並不直接與網絡外部的世界連接。最頂層是輸出層。最底層的每個單元都與上一層的所有單元連接。中間層也是如此。網絡只有前向連接，而沒有側向連接，除了訓練以外也沒有反向的投射。它的結構幾乎不能被簡化。</p>

<p>訓練開始的時候，所有的權重都被隨機賦值，因而網絡最初對所有信號的反應是無意義的。此後給定一個訓練輸入，產生輸出並按反傳訓練規則調節權重。過程如下：在網絡對訓練產生輸出以後，告訴高層的每個單元它的輸出與“正確”輸出之間的差。單元利用該信息來對每個從低層單元達到它的突觸的權重進行小的調整。然後它將該信息反傳到隱層的每個單元。每個隱層單元則收集所有高層單元傳未的誤差信息，並以此調節來自最底層的所有突觸。</p>

<p>從整體上看具體的算法使得網絡總是不斷調節以減小誤差。這個過程被多次重復。（該算法是普適的，可以用於多於三層的前向網絡。）</p>

<p>經過了足夠數量的訓練之後網絡就可以使用了。此時有一個輸入的測試集來檢驗網絡。測試集是經過選擇的，它的一般（統計）特性與訓練集相似，但其他方面則不同。（權重在這個階段保持不變，以便考察訓練後網絡的行為。）如果結果不能令人滿意，設計者會從頭開始，修改網絡的結構、輸入和輸出的編碼方式、訓練規則中的參數或是訓練總數。</p>

<p>所有這些看上去顯得很抽象。舉個例子或許能讓讀者清楚一些。特裏·塞吉諾斯基和查爾斯·羅森堡（Charles Rosenberg）在1987年提供了一個著名的演示。他們把他們的網絡稱為網絡發音器（NETtalk）。它的任務是把書寫的英文轉化成英文發音。英文的拼法不規則,這使它成為一門發音特別困難的語言，因而這個任務並不那麽簡單易行。當然，事先並不把英語的發音規則清楚地告訴網絡。在訓練過程中，網絡每次嘗試後將得到修正信號，網絡則從中學習。輸入是通過一種特殊的方式一個字母接一個字母地傳到網絡中。NETtalk的全部輸出是與口頭發音相對應的一串符號，為了讓演示更生動，網絡的輸出與一個獨立的以前就有的機器（一種數字發音合成器）耦合。它能將NETtallk的輸出變為發音，這樣就可以聽到機器“朗讀”英語了。</p>

<p>由於一個英語字母的發音在很大程度上依賴於它前後的字母搭配，輸入層每次讀入一串7個字母。①輸出層中的單元與音素所要求的21個發音特征②相對應，還有5個單元處理音節分界和重音。圖56給出了它的一般結構。③</p>

<p>他們使用了兩段文字的摘錄來訓練網絡，每段文字都附有訓練機器所需的標音法。第一段文字摘自梅裏亞姆-韋伯斯特袖珍詞典。第二段摘錄則多少有些令人奇怪，是一個小孩的連續說話。初始權重具有小的隨機值，並在訓練期內每處理一個詞更新一次。他們編寫程序使得計算機能根據提供的輸入和（正確的）輸出信息自動地完成這一步。在對真實的輸出進行判斷時，程序會采納一個與真實發音最接近的音素作為最佳猜測，通常有好幾個“發音”輸出單元對此有關系。</p>

<p>聆聽機器學著“讀”英語是一件令人著迷的事情。①最初，由於初始連接是隨機的，只能聽到一串令人困惑的聲音。NETtalk很快就學會了區分元音和輔音。但開始時它只知道一個元音和一個輔音，因此像在咿呀學語。後來它能識別詞的邊界，並能發出像詞那樣的一串聲音。在對訓練集進行了大約十次操作之後，單詞變得清楚，讀的聲音也和幼兒說話很像了。</p>

<p>實際結果並不完美，在某種情況下英語發音依賴於詞意，而NETtalk對此一無所知。一些相似的發音通常引起混淆，如論文（Thesis)和投擲（Throw）的“th”音。把同一個小孩的另一段例文作為檢測，機器完成得很好，表明它能把從相當小的訓練集（1024個單詞）中學到的推廣到它從未遇到的新詞上。②這稱為“泛化”。</p>

<p>顯然網絡不僅僅是它所訓練過的每一個單詞的查詢表。它的泛化能力取決於英語發音的冗余度。並不是每一個英語單詞都按自己唯一的方式發音，雖然首次接觸英語的外國人容易這樣想。（這個問題是由於英語具有兩個起源造成的，即拉丁語系和日爾曼語系，這使得英語的詞匯十分豐富。）</p>

<p>相對於大多數從真實神經元上收集的資料而言，神經網絡的一個優點在於在訓練後很容易檢查它的每一個隱單元的感受野。一個字母僅會激發少數幾個隱單元，還是像全息圖那樣它的活動在許多隱單元中傳播呢？答案更接近於前者。雖然在每個字母一發音對應中並沒有特殊的隱單元，但是每個這種對應並不傳播到所有的隱單元。</p>

<p>因此便有可能檢查隱單元的行為如何成簇的（即具有相同的特性）。塞吉諾斯基和羅森堡發現“……最重要的區別是元音與輔音完全分離，然而在這兩類之中隱單元簇具有不同的模式，對於元音而言，下一個重要的變量是字母，而輔音成簇則按照了一種混合的策略，更多地依賴於它們聲音的相似性。”</p>

<p>這種相當雜亂的布置在神經網絡中是典型現象，其重要性在於它與許多真實皮層神經元（如視覺系統中的神經元）的反應驚人地相似，而與工程師強加給系統的那種巧妙的設計截然不同。</p>

<p>他們的結論是：
NETtalk是一個演示，是學習的許多方面的縮影。首先，網絡在開始時具有一些合理的“先天”的知識，體現為由實驗者選擇的輸入輸出的表達形式，但沒有關於英語的特別知識——網絡可以對任何具有相同的字母和音素集的語言進行訓練。其次，網絡通過學習獲得了它的能力，其間經歷了幾個不同的訓練階段，並達到了一種顯著的水平。最後，信息分布在網絡之中，因而沒有一個單元或連接是必不可少的，作為結果，網絡具有容錯能力，對增長的損害是故障弱化的。此外，網絡從損傷中恢復的速度比重新學習要快得多。</p>

<p>盡管這些與人類的學習和記憶很相似，但NETtalk過於簡單，還不能作為人類獲得閱讀能力的一個好的模型。網絡試圖用一個階段完成人類發育中兩個階段出現的過程，即首先是兒童學會說話；只有在單詞及其含義的表達已經建立好以後，他們才學習閱讀。同時，我們不僅具有使用字母-發音對應的能力，似乎還能達到整個單詞的發音表達，但在網絡中並沒有單詞水平的表達。註意到網絡上並沒有什麽地方清楚地表達英語的發音規則，這與標準的
計算機程序不同。它們內在地鑲嵌在習得的權重模式當中。這正是小孩學習語言的方式。它能正確他說話，但對它的腦所默認的規則一無所知。①</p>

<p>NETtalk有幾條特性是與生物學大為抵觸的。網絡的單元違背了一條規律，即一個神經元只能產生興奮性或抑制性輸出，而不會二者皆有。更為嚴重的是，照字面上說，反傳算法要求教師信息快速地沿傳遞向前的操作信息的同一個突觸發送回去。這在腦中是完全不可能發生的。試驗中用了獨立的回路來完成這一步，但對我而言它們顯得過於勉強，並不符合生物原型。</p>

<p>盡管有這些局限性，NETtalk展示了一個相對簡單的神經網絡所能完成的功能，給人印象非常深刻。別忘了那裏只有不足500個神經元和2萬個連接。如果包括（在前面的腳註中列出的）某些限制和忽略，這個數目將會大一些，但恐怕不會大10倍。而在每一側新皮層邊長大約四分之一毫米的一小塊表面（比針尖還小）有大約5000個神經元。因而與腦相比，NETtalk僅是極小的一部分。②所以它能學會這樣相對復雜的任務給人印象格外深刻。</p>

<p>另一個神經網絡是由西德尼·萊基（Sidney Lehky）和特裏·塞吉諾斯基設計的。他們的網絡所要解決的問題是在不知道光源方向的情況下試圖從某些物體的陰影中推斷出其三維形狀（第四章　描述的所謂從陰影到形狀問題）。對隱層單元的感受野進行檢查時發現了令人吃驚的結果。其中一些感受野與實驗中在腦視覺第一區（V1區）發現的一些神經元非常相似。它們總是成為邊緣檢測器或棒檢測器，但在訓練過程中，並未向網絡呈現過邊或棒，設計者也未強行規定感受野的形狀。它們的出現是訓練的結果。此外，當用一根棒來測試網絡時，其輸出層單元的反應類似於V1區具有端點抑制（End-stopping）的復雜細胞。</p>

<p>網絡和反傳算法二者都在多處與生物學違背，但這個例子提出了這樣一個回想起來應該很明顯的問題：僅僅從觀察腦中一個神經元的感受野並不能推斷出它的功能，正如第十一章描述的那樣，了解它的投射野，即它將軸突傳向哪些神經元，也同樣重要。</p>

<p>我們已經關註了神經網絡中“學習”的兩種極端情況：由赫布規則說明的無教師學習和反傳算法那樣的有教師學習。此外還有若幹種常見的類型。一種同樣重要的類型是“競爭學習”。①其基本思想是網絡操作中存在一種勝者為王機制，使得能夠最好地表達了輸入的含義的那個單元（或更實際他說是少數單元）抑制了其他所有單元。學習過程中，每一步中只修正與勝者密切相關的那些連接，而不是系統的全部連接。這通常用一個三層網絡進行模擬，如同標準的反傳網絡，但又有顯著差異，即它的中間層單元之間具有強的相互連接。這些連接的強度通常是固定的，並不改變。通常短程連接是興奮性的，而長程的則是抑制性的，一個單元傾向於與其近鄰友好而與遠處的相對抗。這種設置意味著中間層的神經元為整個網絡的活動而競爭。在一個精心設計的網絡中，在任何一次試驗中通常只有一個勝者。</p>

<p>這種網絡並沒有外部教師。網絡自己尋找最佳反應。這種學習算法使得只有勝者及其近鄰單元調節輸入權重。這種方式使得當前的那種特殊反應在將來出現可能性更大。由於學習算法自動將權重推向所要求的方向，每個隱單元將學會與一種特定種類的輸入相聯系。①</p>

<p>到此為止我們考慮的網絡處理的是靜態的輸入，並在一個時間間隔後產生一個靜態的輸出。很顯然在腦中有一些操作能表達一個時間序列，如口哨吹出一段曲調或理解一種語言並用之交談。人們初步設計了一些網絡來著手解決這個問題，但目前尚不深入。（NETtalk確實產生了一個時間序列，但這只是數據傳入和傳出網絡的一種方法，而不是它的一種特性。）</p>

<p>語言學家曾經強調，目前在語言處理方面（如句法規則）根據人工智能理論編寫的程序處理更為有效。其本質原因是網絡擅長於高度並行的處理，而這種語言學任務要求一定程度的序列式處理。腦中具有註意系統，它具有某種序列式的本性，對低層的並行處理進行操作，迄今為止神經網絡並未達到要求的這種序列處理的復雜程度，雖然它應當出現。</p>

<p>真實神經元（其軸突、突觸和樹突）都存在不可避免的時間延遲和處理過程中的不斷變化。神經網絡的大多數設計者認為這些特性很討厭，因而回避它們。這種態度也許是錯的。幾乎可以肯定進化就建立在這些改變和時間延遲上，並從中獲益。</p>

<p>對這些神經網絡的一種可能的批評是，由於它們使用這樣一種大體上說不真實的學習算法，事實上它們並不能揭示很多關於腦的情況。對此有兩種答案。一種是嘗試在生物學看來更容易接受的算法，另一種方法更有效且更具有普遍性。加利福尼亞州立大學聖叠戈分校的戴維·齊帕澤（David Zipser），一個由分子生物學家轉為神經理論學家，曾經指出，對於鑒別研究中的系統的本質而言，反傳算法是非常好的方法。他稱之為“神經系統的身份證明”。他的觀點是，如果一個網絡的結構至少近似於真實物體，並了解了系統足夠多的限制，那麽反傳算法作為一種最小化誤差的方法，通常能達到一個一般性質相似於真實生物系統的解。這樣便在朝著了解生物系統行為的正確方向上邁出了第一步。</p>

<p>如果神經元及其連接的結構還算逼真，並已有足夠的限制被加入到系統中，那麽產生的模型可能是有用的，它與現實情況足夠相似。這樣便允許仔細地研究模型各組成部分的行為。與在動物上做相同的實驗相比，這更加快速也更徹底。</p>

<p>我們必須明白科學目標並非到此為止，這很重要。例如，模型可能會顯示，在該模型中某一類突觸需要按反傳法確定的某種方式改變。但在真實系統中反傳法並不出現。因此模擬者必須為這一類突觸找到合適的真實的學習規則。例如，那些特定的突觸或許只需要某一種形式的赫布規則。這些現實性的學習規則可能是局部的，在模型的各個部分不盡相同。如果需要的話，可能會引入一些全局信號，然後必須重新運行該模型。</p>

<p>如果模型仍能工作，那麽實驗者必須表明這種學習方式確實在預測的地方出現，並揭示這種學習所包含的細胞和分子機制以支持這個觀點。只有如此我們才能從這些“有趣”的演示上升為真正科學的有說服力的結果。</p>

<p>所有這些意味著需要對大量的模型及其變體進行測試。幸運的是，隨著極高速而又廉價的計算機的發展，現在可以對許多模型進行模擬。這樣人們就可以檢測某種設置的實際行為是否與原先所希望的相同，但即便使用最先進的計算機也很難檢驗那些人們所希望的巨大而復雜的模型。</p>

<p>“堅持要求所有的模型應當經過模擬檢驗，這令人遺憾地帶來了兩個副產品。如果一個的假設模型的行為相當成功，其設計者很難相信它是不正確的。然而經驗告訴我們，若幹差異很大的模型也會產生相同的行為。為了證明這些模型哪個更接近於事實，看來還需要其他證據，諸如真實神經元及腦中該部分的分子的準確特性。</p>

<p>另一種危害是，對成功的模型過分強調會抑制對問題的更為自由的想像，從而會阻礙理論的產生。自然界是以一種特殊的方式運行的。對問題過於狹隘的討論會使人們由於某種特殊的困難而放棄極有價值的想法。但是進化或許使用了某些額外的小花招來回避這些困難。盡管有這些保留，模擬一個理論，即便僅僅為了體會一下它事實上如何工作，也是有用的。</p>

<p>我們對神經網絡能總結出些什麽呢？它們的基礎設計更像腦，而不是標準計算機的結構，然而，它們的單元並沒有真實神經元那樣復雜，大多數網絡的結構與新皮層的回路相比也過於簡單。目前，如果一個網絡要在普通計算機上在合理的時間內進行模擬，它的規模只能很小。隨著計算機變得越來越快，以及像網絡那樣高度並行的計算機的生產商業化，這會有所改善，但仍將一直是嚴重的障礙。</p>

<p>盡管神經網絡有這些局限性，它現在仍然顯示出了驚人的完成任務的能力。整個領域內充滿了新觀點。雖然其中許多網絡會被人們遺忘，但通過了解它們，抓住其局限性並設計改進它們的新方法，肯定會有堅實的發展。這些網絡有可能具有重要的商業應用。盡管有時它會導致理論家遠離生物事實，但最終會產生有用的觀點和發明。也許所有這些神經網絡方面的工作的最重要的結果是它提出了關於腦可能的工作方式的新觀點。</p>

<p>在過去，腦的許多方面看上去是完全不可理解的。得益於所有這些新的觀念，人們現在至少瞥見了將來按生物現實設計腦模型的可能性，而不是用一些毫無生物依據的模型僅僅去捕捉腦行為的某些有限方面。即便現在這些新觀念已經使我們對實驗的討論更為敏銳，我們現在更多地了解了關於個體神經元所必須掌握的知識。我們可以指出回路的哪些方面我們尚不足夠了解（如新皮層的向回的通路），我們從新的角度看待單個神經元的行為，並意識到在實驗日程上下一個重要的任務是它們整個群體的行為。神經網絡還有很長的路要走，但它們終於有了好的開端。<br/>
  ======================<br/>
①查爾斯·安德森（Charles Anderson）和戴維·範·埃森提出腦中有些裝置將信息按規定路線從一處傳至另一處。不過這個觀點尚有爭議。<br/>
①該網絡以一個早期網絡為基礎。那個網絡被稱為“自旋玻璃”，是物理學家受一種理論概念的啟發而提出的。<br/>
①這對應於一個適定的數學函數（稱為“能量函數”，來自自旋玻璃）的（局域）極小值。霍普菲爾德還給出了一個確定權重的簡單規則以使網絡的每個特定的活動模式對應於能量函數的一個極小值。<br/>
①對於霍普菲爾德網絡而言，輸出可視為網絡存貯的記憶中與輸出（似為“輸入”之誤——譯者註）緊密相關的那些記憶的加權和。<br/>
①在1968年，克裏斯托夫·朗格特-希金斯（Christopher Longuet-Higgins）從全息圖出發發明了一種稱為“聲音全息記器”（Holophone）的裝置。此後他又發明了另一種裝置稱為“相關圖”，並最終形成了一種特殊的神經網絡形式。他的學生戴維·威爾肖在完成博士論文期間對其進行了詳細的研究。<br/>
(2)他們和其他一些想法接近的理論家合作，在1981年完成了《聯想記憶的並行模式》，由傑弗裏·希爾頓（Geoffrey Hinton）和吉姆·安德森編著。這本書的讀者主要是神經網絡方面的工作者，它的影響並不像後一本書那樣廣泛。<br/>
(1)PDP即平行分布式處理（Parallel Distributed Processing）的縮寫。<br/>
①更準確他說是誤差的平方的平均值在下降，因此該規則有時又叫做最小均方（LMS）規則。<br/>
①29個“字母”各有一個相應的單元；這包括字母表中的26個字母，還有三個表示標點和邊界。因而輸入層需要29x7=203個單元。<br/>
②例如，因為輔音p和b發音時都是以攏起嘴唇開始的，所以都稱作“唇止音”。<br/>
③中間層（隱層）最初有80個隱單元，後來改為120個，結果能完成得更好。機器總共需要調節大約2萬個突觸。權重可正可負。他們並沒有構造一個真正的平行的網絡來做這件事，而是在一臺中型高速計算機上（一臺VAX 11//780FPA）模擬這個網絡。<br/>
①計算機的工作通常不夠快，不能實時地發音，因而需要先把輸出錄下來，再加速播放，這樣人們才能聽明白。<br/>
②塞吉諾斯基和羅森堡還表明，網絡對於他們設置的連接上的隨機損傷具有相當的抵抗力。在這種環境下它的行為是”故障弱化”。他們還試驗以11個字母（而不是7個字母）為一組輸入。這顯著改善了網絡的成績。加上第二個隱單元層並不能改善它的成績，但有助於網絡更好地進行泛化。<br/>
①除了上面列出的以外，NEttalk還有許多簡化。雖然作者們信奉分布式表達，在輸入輸出均有“祖母細胞”即，例如有一個單元代表“窗口中第三個位置上的字母a”。這樣做是為了降低計算所需要的時間，是一種合理的簡化形式。雖然數據順序傳入7個字母的方式在人工智能程序是完全可以接受的，卻顯得與生物事實相違背。輸出的“勝者為王”這一步並不是由“單元”完成的，也不存在一組單元去表達預計輸出與實際輸出之間的差異（即教師信號）。這些運算都是由程序執行的。<br/>
②這種比較不太公平，因為神經網絡的一個單元更好的考慮是等價於腦中一小群相神經元。因而更合適的數字大約是8萬個神經元（相當於一平方毫米皮層下神經元的數目）。<br/>
①它是由斯蒂芬·格羅斯伯格、托伊沃·科霍寧等人發展的。<br/>
①我不打算討論競爭網絡的局限性。顯然必須有足夠多的隱單元來容納網絡試圖從提供的輸入中所學的所有東西，訓練不能太快，也不能太慢，等等。這種網絡要正確工作需要仔細設計。毫無疑問，不久的將來會發明出基於競爭學習基本思想的更加復雜的應用。</p>

<p><a href="http://en.wikipedia.org/wiki/The_Astonishing_Hypothesis">《驚人的假說&mdash;靈魂的科學探索》Francis Crick 1994&#8217;</a> 湖南科學技術出版社出版</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[I Am a Nerd/Weirdo/Crackpot.]]></title>
    <link href="http://www.aprilzephyr.com/blog/04272015/i-am-a-nerd-slash-weirdo-slash-crackpot/"/>
    <updated>2015-04-27T19:50:26+08:00</updated>
    <id>http://www.aprilzephyr.com/blog/04272015/i-am-a-nerd-slash-weirdo-slash-crackpot</id>
    <content type="html"><![CDATA[<p><img src="http://www.aprilzephyr.com/images/rain.jpg"></p>

<hr />

<p><strong>Nerd</strong><br/>
: a person who behaves awkwardly around other people and usually has unstylish clothes, hair, etc.<br/>
: a person who is very interested in technical subjects, computers, etc.</p>

<p><strong>Weirdo</strong><br/>
: a strange or unusual person</p>

<p><strong>Crackpot</strong><br/>
: a person who is crazy or very strange</p>

<p>&mdash;From <strong><em><a href="http://www.merriam-webster.com">Merriam Webster Dictionary</a></em></strong></p>

<hr />

<!--more-->


<p>I have got no idea about the noun which could exactly describe myself&mdash;in Chinese I mean myself a 奇葩(QiPa).</p>

<p>I am a QiPa cuz I am so languish not to lay down a backup flat sheet after the dirty one&rsquo;s been washed. However, I volunteered to have shower everyday as soon as getting back to my apartment.</p>

<p>I am a QiPa cuz I need washing my hair every morning. However, I could tolerant some tiny dirty spots on my pants or shoes and my those so-casual shirts.</p>

<p>I am a QiPa cuz I am too alone to attend &ldquo;unnecessary&rdquo; social or other activities. However, I am kind of eager to be understood and admitted.</p>

<p>I am a QiPa cuz I am only interested in American Justice related TV series and enough aware of their fictionality. However, I am hooked in such ethereal Justice, human right and happy ends.</p>

<p>I am a QiPa cuz I read a lot including fictional, non-fictional, scientific, technical, even Philosophical and Chinese medical. However, I could not make myself a thing nor be unconventional.</p>

<p>I am a QiPa cuz I try to be abstemious about daily basic necessities. However, I am somewhat squandering money on travelling, books(now I have a kindle), civil aircraft models and have no schedule about buying a house(or somewhere to shelter) even at present.</p>

<p>I am a QiPa cuz I know it&rsquo;s meaningless to be too reasonable with woman so well. However, I keep indulging myself into a dead-end state with the loved in so many specific aspects.</p>

<p>I am a QiPa cuz I give top priority nothing but love. However, I am so likely to be messed up in those trifles.</p>

<p>I am a QiPa cuz I thought I am firm and resolute sufficiently. However, I am thirsting for support and encouragement from the intimates.</p>

<p>I am a QiPa cuz I feel I understand, however, I don&rsquo;t.</p>

<p>Maybe, I am who I am, who is actually, non-existent.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[About Love]]></title>
    <link href="http://www.aprilzephyr.com/blog/04212015/about-love/"/>
    <updated>2015-04-21T15:40:14+08:00</updated>
    <id>http://www.aprilzephyr.com/blog/04212015/about-love</id>
    <content type="html"><![CDATA[<p><img src="http://www.aprilzephyr.com/images/love.jpg"></p>

<p><em>Whenever I get gloomy with the state of the world, I think about the arrival&rsquo;s gate at Heathrow airport. General opinion start to make out that we live in a world of hatred and greed. But I don&rsquo;t see that, seems to me that love is everywhere. Often it&rsquo;s not particularly dignified or newsworthy, but it&rsquo;s always there, fathers and sons, mothers and daughters, husbands and wives, boyfriend, girlfriend, old friends. When the plane hit the Twin Tower, as far as I know, none of the phone calls from people on board were messages of hate or revenge. They are all messages of love. If you look for it, I&rsquo;ve got a sneaky feeling that love actually is all around.</em>   <br/>
<strong>&mdash;&ldquo;Love Actually&rdquo; (Movie, 2003&#8217;)</strong></p>

<!--more-->


<p><em>I have to tell you this and you need to hear it. I&rsquo;ve loved you since I met you&hellip; But I wouldn&rsquo;t allow myself to truly feel it until today. I was always thinking ahead. Making decisions out of fear. Today, because of what I learned from you&hellip; every choice I made was different and my life has completely changed. And I&rsquo;ve learned that if you do that, you&rsquo;re living your life fully. It doesn&rsquo;t matter if you have five minutes or fifty years. Samantha, if not for today, if not for you&hellip; I would never have known love at all. So thank you for being the person who taught me to love, and to be loved.</em>      <br/>
<strong>&mdash;&ldquo;If Only&rdquo; (Movie, 2004&#8217;)</strong></p>

<p><em>The fountains mingle with the river
And the rivers with the Ocean,
The winds of Heaven mix for ever
With a sweet emotion;
Nothing in the world is single;
All things by a law divine
In one spirit meet and mingle,
Why not I with thine?</em></p>

<p><em>See the mountains kiss high Heaven
And the waves clasp one another;
No sister-flower would be forgiven
If it disdained its brother;
And the sunlight clasps the earth
And the moonbeams kiss the sea;
What is all this sweet work worth
If thou kiss not me?</em><br/>
<strong>&mdash;&ldquo;Love&rsquo;s Philosophy&rdquo; (Percy Bysshe Shelley, 1820&#8217;)</strong></p>

<p><em>Let me not to the marriage of true minds
Admit impediments. Love is not love
Which alters when it alteration finds,
Or bends with the remover to remove:
O no; it is an ever-fixed mark,
That looks on tempests, and is never shaken;
It is the star to every wandering bark,
Whose worth&rsquo;s unknown, although his height be taken.
Love&rsquo;s not Time&rsquo;s fool, though rosy lips and cheeks
Within his bending sickle&rsquo;s compass come;
Love alters not with his brief hours and weeks,
But bears it out even to the edge of doom.
If this be error and upon me proved,
I never writ, nor no man ever loved.</em> <br/>
<strong>&mdash;&ldquo;Sonnet 116&rdquo; (Shakespeare, 1609&#8217;)</strong></p>

<p>Love is not a word, it is a state of mind<br/>
Love is beyond a promise, it is a belief<br/>
Love is hope, it is also fortitude<br/>
Love makes a little green, but not so jealous<br/>
Love leads to a colorful dream, which is worth weaving and cherishing<br/>
Love means tolerating, which permits you being real<br/>
Love paints your life, furthermore harbors your soul.<br/>
Love is so hard to achieve, but it trully lasts.<br/>
<strong>&mdash;&ldquo;About Love&rdquo; (Themis_Sword, 2015&#8217;)</strong></p>

<p><strong>I Love Thee! <a href="http://www.weibo.com/leah0204">@leah0204</a></strong></p>

<iframe width="420" height="315" src="https://www.youtube.com/embed/1moWxHdTkT0" frameborder="0" allowfullscreen></iframe>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The Eve of Another Birthday]]></title>
    <link href="http://www.aprilzephyr.com/blog/04182015/the-eve-of-another-birthday/"/>
    <updated>2015-04-18T13:48:27+08:00</updated>
    <id>http://www.aprilzephyr.com/blog/04182015/the-eve-of-another-birthday</id>
    <content type="html"><![CDATA[<p><img src="http://www.aprilzephyr.com/images/life.jpg"></p>

<p>I almost forgot how old I am this year.</p>

<p>Thanks my parents for their affectionate SMS this morning wishing me a happy birthday and take-care overseas alone.</p>

<p>Actually, I might be solitary in a way rather than lonely.<!--more--></p>

<p>Retrospecting, It seems like I even don&rsquo;t tolerate continuing the &ldquo;insipid and mundane&rdquo; life traces no matter in which city of China. The gap between the so-called &ldquo;dream&rdquo; and reality keeps on tearing. I am kind of negative for picking that apple always higher despite my leaping and attempting. So I chose to leave(flounder) and chase a new style sitting in a classroom as a postgraduate student, overseas. There would be other choices for sure, nevertheless, I also need some fresh air.</p>

<p>Adaptability would never be an issue for me in the last ten years since my admission to university away from hometown without regarding to geographical position nor language speaking. So I live well for now, or, in other words, I survive well, for not self-sufficient.</p>

<p>I&rsquo;m trying to stay hungary and foolish, so I study and review knowledge almost everyday without any passion of social networkings nor outdoor activities. Meanwhile, the efficiency of learning is not so satisfactory, frankly. The more materials or resources I consult, the more self-ignorant I feel, which is so frustrated. I could recall a professor&rsquo;s words <strong>&ldquo;Time Is the Only Enemy!&rdquo;</strong> on the registration day.</p>

<p>It doesn&rsquo;t mean living in a vacuum while no comparing with others&#8217; gains. Especially I&rsquo;m nothing but a student on the margin of my twenties. I remind myself that actually I am gaining much, in mental.</p>

<p>Thankfully, our transnational love evolves smoothly. As my motivity to move on so fearlessly, she is fabulous. Feeling comforting and cozy with her so much. Weaving our dreams and moreover, we are struggling, inch by inch, towards the brilliant future of ours.</p>

<p>It would be only quite an ordinary day of tomorrow, as for me, I&rsquo;d rather consider it another stair up.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The 20 Most Popular TED Talks of All Time]]></title>
    <link href="http://www.aprilzephyr.com/blog/04032015/the-20-most-popular-ted-talks-of-all-time/"/>
    <updated>2015-04-03T20:00:00+08:00</updated>
    <id>http://www.aprilzephyr.com/blog/04032015/the-20-most-popular-ted-talks-of-all-time</id>
    <content type="html"><![CDATA[<p><img src="http://www.aprilzephyr.com/images/think.jpg"></p>

<p>Nonprofit organization TED launched in 1984 with a mission to present ideas worth sharing.</p>

<p>It has since become a cultural phenomenon, bringing together thought leaders from around the globe to give short, a few minutes talks about ideas that could change the world.</p>

<p>Of the more than 1,800 TED Talks, which have been viewed a total of 2.5 billion times across all platforms, a few have risen to the top. The following 20 talks are the most popular ever on Ted.com.<!--more--></p>

<iframe src="https://embed-ssl.ted.com/talks/lang/en/ken_robinson_says_schools_kill_creativity.html" width="640" height="360" frameborder="0" scrolling="no" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>


<p>
Sir Ken Robinson makes an entertaining and profoundly moving case for creating an education system that nurtures (rather than undermines) creativity.</p>

<iframe src="https://embed-ssl.ted.com/talks/lang/en/amy_cuddy_your_body_language_shapes_who_you_are.html" width="640" height="360" frameborder="0" scrolling="no" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>


<p>Body language affects how others see us, but it may also change how we see ourselves. Social psychologist Amy Cuddy shows how “power posing” — standing in a posture of confidence, even when we don’t feel confident — can affect testosterone and cortisol levels in the brain, and might even have an impact on our chances for success.</p>

<iframe src="https://embed-ssl.ted.com/talks/lang/en/simon_sinek_how_great_leaders_inspire_action.html" width="640" height="360" frameborder="0" scrolling="no" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>


<p>Simon Sinek has a simple but powerful model for inspirational leadership all starting with a golden circle and the question &ldquo;Why?&rdquo; His examples include Apple, Martin Luther King, and the Wright brothers &hellip; (Filmed at TEDxPugetSound.)</p>

<iframe src="https://embed-ssl.ted.com/talks/lang/en/brene_brown_on_vulnerability.html" width="640" height="360" frameborder="0" scrolling="no" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>


<p>Brené Brown studies human connection — our ability to empathize, belong, love. In a poignant, funny talk, she shares a deep insight from her research, one that sent her on a personal quest to know herself as well as to understand humanity. A talk to share.</p>

<iframe src="https://embed-ssl.ted.com/talks/lang/en/jill_bolte_taylor_s_powerful_stroke_of_insight.html" width="640" height="360" frameborder="0" scrolling="no" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>


<p>Jill Bolte Taylor got a research opportunity few brain scientists would wish for: She had a massive stroke, and watched as her brain functions — motion, speech, self-awareness — shut down one by one. An astonishing story.</p>

<iframe src="https://embed-ssl.ted.com/talks/lang/en/pranav_mistry_the_thrilling_potential_of_sixthsense_technology.html" width="640" height="360" frameborder="0" scrolling="no" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>


<p>At TEDIndia, Pranav Mistry demos several tools that help the physical world interact with the world of data — including a deep look at his SixthSense device and a new, paradigm-shifting paper &ldquo;laptop.&rdquo; In an onstage Q&amp;A, Mistry says he&rsquo;ll open-source the software behind SixthSense, to open its possibilities to all.</p>

<iframe src="https://embed-ssl.ted.com/talks/lang/en/mary_roach_10_things_you_didn_t_know_about_orgasm.html" width="640" height="360" frameborder="0" scrolling="no" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>


<p>&ldquo;Bonk&rdquo; author Mary Roach delves into obscure scientific research, some of it centuries old, to make 10 surprising claims about sexual climax, ranging from the bizarre to the hilarious. (This talk is aimed at adults. Viewer discretion advised.)</p>

<iframe src="https://embed-ssl.ted.com/talks/lang/en/tony_robbins_asks_why_we_do_what_we_do.html" width="640" height="360" frameborder="0" scrolling="no" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>


<p>Tony Robbins discusses the &ldquo;invisible forces&rdquo; that motivate everyone&rsquo;s actions — and high-fives Al Gore in the front row.</p>

<iframe src="https://embed-ssl.ted.com/talks/lang/en/dan_pink_on_motivation.html" width="640" height="360" frameborder="0" scrolling="no" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>


<p>Career analyst Dan Pink examines the puzzle of motivation, starting with a fact that social scientists know but most managers don&rsquo;t: Traditional rewards aren&rsquo;t always as effective as we think. Listen for illuminating stories — and maybe, a way forward.</p>

<iframe src="https://embed-ssl.ted.com/talks/lang/en/david_gallo_shows_underwater_astonishments.html" width="640" height="360" frameborder="0" scrolling="no" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>


<p>David Gallo shows jaw-dropping footage of amazing sea creatures, including a color-shifting cuttlefish, a perfectly camouflaged octopus, and a Times Square&rsquo;s worth of neon light displays from fish who live in the blackest depths of the ocean. This short talk celebrates the pioneering work of ocean explorers like Edith Widder and Roger Hanlon.</p>

<iframe src="https://embed-ssl.ted.com/talks/lang/en/dan_gilbert_asks_why_are_we_happy.html" width="640" height="360" frameborder="0" scrolling="no" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>


<p>Dan Gilbert, author of &ldquo;Stumbling on Happiness,&rdquo; challenges the idea that we’ll be miserable if we don’t get what we want. Our &ldquo;psychological immune system&rdquo; lets us feel truly happy even when things don’t go as planned.</p>

<iframe src="https://embed-ssl.ted.com/talks/lang/en/susan_cain_the_power_of_introverts.html" width="640" height="360" frameborder="0" scrolling="no" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>


<p>In a culture where being social and outgoing are prized above all else, it can be difficult, even shameful, to be an introvert. But, as Susan Cain argues in this passionate talk, introverts bring extraordinary talents and abilities to the world, and should be encouraged and celebrated.</p>

<iframe src="https://embed-ssl.ted.com/talks/lang/en/pattie_maes_demos_the_sixth_sense.html" width="640" height="360" frameborder="0" scrolling="no" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>


<p>This demo — from Pattie Maes&#8217; lab at MIT, spearheaded by Pranav Mistry — was the buzz of TED. It&rsquo;s a wearable device with a projector that paves the way for profound interaction with our environment. Imagine &ldquo;Minority Report&rdquo; and then some.</p>

<iframe src="https://embed-ssl.ted.com/talks/lang/en/elizabeth_gilbert_on_genius.html" width="640" height="360" frameborder="0" scrolling="no" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>


<p>Elizabeth Gilbert muses on the impossible things we expect from artists and geniuses — and shares the radical idea that, instead of the rare person &ldquo;being&rdquo; a genius, all of us &ldquo;have&rdquo; a genius. It&rsquo;s a funny, personal and surprisingly moving talk.</p>

<iframe src="https://embed-ssl.ted.com/talks/lang/en/hans_rosling_shows_the_best_stats_you_ve_ever_seen.html" width="640" height="360" frameborder="0" scrolling="no" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>


<p>You&rsquo;ve never seen data presented like this. With the drama and urgency of a sportscaster, statistics guru Hans Rosling debunks myths about the so-called &ldquo;developing world.&rdquo;</p>

<iframe src="https://embed-ssl.ted.com/talks/lang/en/pamela_meyer_how_to_spot_a_liar.html" width="640" height="360" frameborder="0" scrolling="no" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>


<p>On any given day we&rsquo;re lied to from 10 to 200 times, and the clues to detect those lie can be subtle and counter-intuitive. Pamela Meyer, author of Liespotting, shows the manners and &ldquo;hotspots&rdquo; used by those trained to recognize deception — and she argues honesty is a value worth preserving.</p>

<iframe src="https://embed-ssl.ted.com/talks/lang/en/shawn_achor_the_happy_secret_to_better_work.html" width="640" height="360" frameborder="0" scrolling="no" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>


<p>We believe that we should work to be happy, but could that be backwards? In this fast-moving and entertaining talk, psychologist Shawn Achor argues that actually happiness inspires productivity. (Filmed at TEDxBloomington.)</p>

<iframe src="https://embed-ssl.ted.com/talks/lang/en/david_blaine_how_i_held_my_breath_for_17_min.html" width="640" height="360" frameborder="0" scrolling="no" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>


<p>In this highly personal talk from TEDMED, magician and stuntman David Blaine describes what it took to hold his breath underwater for 17 minutes — a world record (only two minutes shorter than this entire talk!) — and what his often death-defying work means to him. Warning: do NOT try this at home.</p>

<iframe src="https://embed-ssl.ted.com/talks/lang/en/keith_barry_does_brain_magic.html" width="640" height="360" frameborder="0" scrolling="no" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>


<p>First, Keith Barry shows us how our brains can fool our bodies — in a trick that works via podcast too. Then he involves the audience in some jaw-dropping (and even a bit dangerous) feats of brain magic.</p>

<iframe src="https://embed-ssl.ted.com/talks/lang/en/cameron_russell_looks_aren_t_everything_believe_me_i_m_a_model.html" width="640" height="360" frameborder="0" scrolling="no" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe>


<p>Cameron Russell admits she won “a genetic lottery”: she&rsquo;s tall, pretty and an underwear model. But don&rsquo;t judge her by her looks. In this fearless talk, she takes a wry look at the industry that had her looking highly seductive at barely 16 years old.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[...]]></title>
    <link href="http://www.aprilzephyr.com/blog/12012014/dot-dot-dot/"/>
    <updated>2014-12-01T16:37:15+08:00</updated>
    <id>http://www.aprilzephyr.com/blog/12012014/dot-dot-dot</id>
    <content type="html"><![CDATA[<p><img src="http://www.aprilzephyr.com/images/adventure.jpg"></p>

<p>今天，2014年最後一個月的第一天，是該收拾下情緒了。</p>

<p>2014年，普通的不能再普通的一年，卻夾雜了太多的變化，這些變化，甚至改變了未來的方向。<!--more--></p>

<p>去年的辭職空檔期甚至延續到了今年的前五個月，超過整整七個月的空檔期，要算焦灼，還是焦慮，此刻的自己都回憶不起來那時，到底是什麼樣的心境。</p>

<p>還好，工作只是糊口的家什，事業才是值得追求和爭取的。</p>

<p>於是，工作，辭職，又工作。</p>

<p>直到現在，依然還算是沒有脫離“工作”的羈絆吧。</p>

<p>苦盡甘來？或許之前也不算苦，即使大半年像個游魂。</p>

<p>結識了True Friends，也遇到了她。</p>

<p>更安心了，就像久枯之地突然流過潺潺溪水；就靜靜的，靜靜的，直到心裏。如果硬要說有什麼不安，可能，是擔心自己不夠出色，不夠優秀，讓她委屈。</p>

<p>也收到了大學的offer，即將在新年啟程，去另一個國家，開始求學之旅，開始另一種人生。</p>

<p>自己很幸運，很幸運，不然，不會同時擁有，擁有一個愛人，擁有一份美好的未來。</p>

<p>此刻，耳機裏正好傳來最喜歡的那首“Forever in Love”。</p>

<p>感恩節剛過，如果說把“感恩”說出來，一遍兩遍三遍，未免太矯情。如此，心裏知道，便好。不求其他的，只求無愧、無悔。</p>

<p>還好，掙扎了這麼多年，還沒放棄。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Pieces]]></title>
    <link href="http://www.aprilzephyr.com/blog/08302014/pieces/"/>
    <updated>2014-08-30T22:15:03+08:00</updated>
    <id>http://www.aprilzephyr.com/blog/08302014/pieces</id>
    <content type="html"><![CDATA[<p><img src="http://www.aprilzephyr.com/images/owlcoffee.jpg"></p>

<p>A friend said I am a traditional person intrinsically somehow with an attempt to break the invisible shackles, the boundary of the traditional and the nontraditional. I know it&rsquo;s true for I am raised and influenced since my juvenile in such family and society.<!--more--></p>

<p>Let&rsquo;s define the word &ldquo;traditional&rdquo; with the meaning of life styles with or close to Eastern world, and the word &ldquo;nontraditional&rdquo; with the opposite one.</p>

<p>With thousands years&#8217; culture of the doctrine of the mean in Eastern world, it would be so easy about choosing just like an objective question, following the tide and keeping away from too leftward or too rightward. It is also believed safe and reliable. If I am always as pure as a plain paper about my mind, there would be never so troublesome.</p>

<p>If comparing the modes of thinking as a curve of Normal Distribution, the curve in the Eastern world is supposed to be steep and thin while the curve in the Western world is supposed to be flat and fat, that is the difference. Persons&#8217; innate thoughts and exterior activities in the two environments are pulled or pushed from the two types of curve.</p>

<p>No readings, no worries. Especially when reading so many books about different life styles, thoughts, viewpoints and experiences of this flat world, notions&#8217; sparking and colliding in the consciousness. While endeavoring to experience colorful diversities of life, the farther you deviate, the stronger viscous forces pull you back to the curve. (Based on the theory of evolution, it would take tens or hundreds of generations to have a thorough change, of that curve, for the whole society.) And I also believe, it&rsquo;s true.</p>

<p>Retrospecting myself, what I would like to do is exploring this magnificent earth by reading thousands of books and traveling thousands of miles. Only after that, I could have a proper state of mind to choose and settle down. I would like to live a fresh and unique myself without considering what the general trends&#8217; choices. Obviously that&rsquo;s the reason  I am who I am. I would try to make some tiny changes within an invisible scope, the so-called margin of safety. As for others, I would be not so sure. Maybe that, the flounder, is some proof of my youth.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hurt]]></title>
    <link href="http://www.aprilzephyr.com/blog/07102014/hurt/"/>
    <updated>2014-07-10T16:52:28+08:00</updated>
    <id>http://www.aprilzephyr.com/blog/07102014/hurt</id>
    <content type="html"><![CDATA[<iframe width="560" height="315" src="https://www.youtube.com/embed/vt1Pwfnh5pc" frameborder="0" allowfullscreen></iframe>


<p>I hurt myself today<br/>
To see if I still feel  <!--more-->
I focus on the pain<br/>
The only thing that&rsquo;s real<br/>
The needle tears a hole<br/>
The old familiar sting<br/>
Try to kill it all away<br/>
But I remember everything</p>

<p>What have I become<br/>
My sweetest friend<br/>
Everyone I know goes away<br/>
In the end<br/>
And you could have it all<br/>
My empire of dirt<br/>
I will let you down<br/>
I will make you hurt</p>

<p>I wear this crown of thorns<br/>
Upon my liar&rsquo;s chair<br/>
Full of broken thoughts<br/>
I cannot repair<br/>
Beneath the stains of time<br/>
The feelings disappear<br/>
You are someone else<br/>
I am still right here</p>

<p>What have I become<br/>
My sweetest friend<br/>
Everyone I know goes away<br/>
In the end<br/>
And you could have it all<br/>
My empire of dirt<br/>
I will let you down<br/>
I will make you hurt</p>

<p>If I could start again<br/>
A million miles away<br/>
I would keep myself<br/>
I would find a way</p>

<p><strong>What would remain imperishably while lines&#8217; etching on the face? A word, some memories, or just serenity with or without regret.</strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[New Job, New Way]]></title>
    <link href="http://www.aprilzephyr.com/blog/06142014/new-job/"/>
    <updated>2014-06-14T10:04:25+08:00</updated>
    <id>http://www.aprilzephyr.com/blog/06142014/new-job</id>
    <content type="html"><![CDATA[<p><img src="http://www.aprilzephyr.com/images/bigdata.jpg"></p>

<p>It&rsquo;s been a whole month working in a new company, which is a venture company with the area of distributed storage focused.<!--more--></p>

<p>Returning back to Beijing after half a year&rsquo;s separation, living in a high-rise apartment, I soon realized, it would be a new way.</p>

<p>Everything&rsquo;s changed whereas everything seems the same.</p>

<p>NEW CHALLENGES come with the new position. Studying and reading fulfill every day and almost every minute, that joys me. I could see vigor&rsquo;s back and fresh air&rsquo;s pervading.</p>

<p>Time to transform and evolve.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[You Are Alive]]></title>
    <link href="http://www.aprilzephyr.com/blog/05042014/you-are-alive/"/>
    <updated>2014-05-04T20:21:19+08:00</updated>
    <id>http://www.aprilzephyr.com/blog/05042014/you-are-alive</id>
    <content type="html"><![CDATA[<p><img src="http://www.aprilzephyr.com/images/alive.jpg"></p>

<p>如果你的內心有不安<br/>
If you&rsquo;re carrying your restlessness in your heart,<!--more--><br/>
那麽你還活著<br/>
you are alive<br/>
如果你的眼中有對夢想的渴望<br/>
If you&rsquo;re carrying the flames of dreams in your eyes,<br/>
那麽你還活著<br/>
you are alive<br/>
像一陣風一般的自由生活<br/>
Like a gust of wind, learn to live free<br/>
學著像大海的波浪般流動<br/>
Learn to flow like the waves that make a sea<br/>
張開你的雙臂擁抱每個時刻，你都可能收獲一份問候<br/>
May every moment gift you a new sight to greet<br/>
如果你的眼中有期望<br/>
If you&rsquo;re carrying wonder in your eyes,<br/>
那麽你還活著<br/>
you are alive<br/>
如果你的內心有不安<br/>
If you&rsquo;re carrying your restlessness in your heart<br/>
那麽你還活著<br/>
you are alive</p>

<p>&mdash;From Indian Movie <a href="http://www.imdb.com/title/tt1562872/">&ldquo;Zindagi Na Milegi Dobara&rdquo;</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python 正則表達式指南(轉)]]></title>
    <link href="http://www.aprilzephyr.com/blog/05042014/python-zheng-ze-biao-da-shi-zhi-nan-zhuan/"/>
    <updated>2014-05-04T14:49:04+08:00</updated>
    <id>http://www.aprilzephyr.com/blog/05042014/python-zheng-ze-biao-da-shi-zhi-nan-zhuan</id>
    <content type="html"><![CDATA[<h4>1. 正則表達式基礎</h4>

<h5>1.1 簡單介紹</h5>

<p>正則表達式並不是Python的一部分。正則表達式是用於處理字符串的強大工具，擁有自己獨特的語法以及一個獨立的處理引擎，效率上可能不如str自帶的方法，但功能十分強大。得益於這一點，在提供了正則表達式的語言裏，正則表達式的語法都是一樣的，區別只在於不同的編程語言實現支持的語法數量不同；但不用擔心，不被支持的語法通常是不常用的部分。如果已經在其他語言裏使用過正則表達式，只需要簡單看一看就可以上手了。</p>

<p>下圖展示了使用正則表達式進行匹配的流程：<!--more--><br/>
<img src="http://www.aprilzephyr.com/images/zhengzbds.png"></p>

<p>正則表達式的大致匹配過程是：依次拿出表達式和文本中的字符比較，如果每一個字符都能匹配，則匹配成功；一旦有匹配不成功的字符則匹配失敗。如果表達式中有量詞或邊界，這個過程會稍微有一些不同，但也是很好理解的，看下圖中的示例以及自己多使用幾次就能明白。</p>

<p>下錶列出了Python支持的正則表達式元字符和語法：</p>

<table><tbody>
<tr><td><em> 語法 </em></td><td><em> 說明 </em></td><td><em> 表達式實例 </em></td><td><em> 完整匹配的字符串 </em></td></tr>
<tr><td></td><td><em> 字符 </em><td></td></td><td></td></tr>
<tr><td> 一般字符 </td><td> 匹配自身 </td><td> abc </td><td> abc </td></tr>
<tr><td> . </td><td> 匹配任意除換行符&#8221;\n&#8221;外的字符。在DOTALL模式中也能匹配換行符 </td><td> a.c </td><td> abc </td></tr>
<tr><td> \ </td><td> 轉義字符，使後一個字符改變原來的意思。如果字符串中有字符*需要匹配，可以使用\*或者字符集[*] </td><td> a&#46;c a&#92;c </td><td> a.c a\c </td></tr>
<tr><td> [&#8230;] </td><td> 字符集(字符類)。對應的位置可以是字符集中任意字符。字符集中的字符可以逐個列出，也可以給出範圍，如[abc]或[a-c]。第一個字符如果是^則表示取反，如[^abc]表示不是abc的其他字符。 所有的特殊字符在字符集中都失去原油的特殊含義。在字符集中如果要使用]、-或^，可以在前面加上反斜槓，或把]、-放在第一個字符，把^放在非第一個字符 </td><td> a[bdc]e </td><td> abe ace ade </td></tr>
<tr><td></td><td><em> 預定義字符集(可以寫在字符集[&#8230;]中) </em><td></td></td><td></td></tr>
<tr><td> \d </td><td> 數字:[0-9] </td><td> a\dc </td><td> a1c </td></tr>
<tr><td> \D </td><td> 非數字:[^\d] </td><td> a\Dc </td><td> abc </td></tr>
<tr><td> \s </td><td> 非白字符:[<空格>\t\r\n\f\v] </td><td> a\sc </td><td> a c </td></tr>
<tr><td> \S </td><td> 非空白字符:[^\s] </td><td> a\Sc </td><td> abc </td></tr>
<tr><td> \w </td><td> 單詞字符:[A-Z a-z 0-9] </td><td> a\wc </td><td> abc </td></tr>
<tr><td> \W </td><td> 非單詞字符:[^\W] </td><td> a\Wc </td><td> a c </td></tr>
<tr><td></td><td><em> 數量詞(用在字符或(&#8230;)之後) </em><td></td></td><td></td></tr>
<tr><td> * </td><td> 匹配前一個字符0或無限次 </td><td> abc* </td><td> ab abccc </td></tr>
<tr><td> + </td><td> 匹配前一個字符1或無限次 </td><td> abc+ </td><td> abc abccc </td></tr>
<tr><td> ? </td><td> 匹配前一個字符0或1次 </td><td> abc? </td><td> ab abc </td></tr>
<tr><td> {m} </td><td> 匹配前一個字符m次 </td><td> ab{2}c </td><td> abbc </td></tr>
<tr><td> {m,n} </td><td> 匹配前一個字符m至n次。m和n可以省略：若省略m，則匹配0至n次；若省略n，則匹配m至無限次 </td><td> ab{1,2}c </td><td> abc abbc </td></tr>
<tr><td> \*?+? ?? {m,n}? </td><td> 使*+?{m,n}變成非貪婪模式 </td><td> 示例在下文中介紹 </td><td> </td></tr>
<tr><td></td><td><em> 邊界匹配(不消耗待匹配字符串中的字符) </em><td></td></td><td></td></tr>
<tr><td> ^ </td><td> 匹配字符串開頭。在多行模式中匹配每一行的開頭。 </td><td> ^abc </td><td> abc </td></tr>
<tr><td> $ </td><td> 匹配字符串末尾。在多行模式中匹配每一行的末尾。 </td><td> abc$ </td><td> abc </td></tr>
<tr><td> \A </td><td> 儘匹配字符串開頭。 </td><td> \Aabc </td><td> abc </td></tr>
<tr><td> \Z </td><td> 儘匹配字符串末尾。 </td><td> \Zabc </td><td> abc </td></tr>
<tr><td> \b </td><td> 匹配\w和\W之間。 </td><td> a\b!bc </td><td> a!bc </td></tr>
<tr><td> \B </td><td> [^\b] </td><td> a\Bbc </td><td> abc </td></tr>
<tr><td></td><td><em> 邏輯、分組 </em><td></td></td><td></td></tr>
<tr><td> | </td><td> |代表左右表達式任意匹配一個。總是先嘗試匹配左邊的表達式，一旦成功匹配則跳過匹配右邊的表達式。如果|沒有被包括在()中，則它的範圍是整個正則表達式。 </td><td> abc|def </td><td> abc def </td></tr>
<tr><td> (&#8230;) </td><td> 被擴起來的表達式將作為分組，從表達式左邊開始每遇到一個分組的左括號&#8217;(&#8216;，編號+1。另外，分組表達式作為一個整體，可以後接數量詞。表達式中的|儘在該組中有效。 </td><td> (abc){2} a(123|456)c </td><td> abcabc a456c </td></tr>
<tr><td> (?P<name>&#8230;) </td><td> 分組，除了原有的編號外再指定一個額外的別名。 </td><td> (?P<id>abc){2} </td><td> abcabc </td></tr>
<tr><td> \<number> </td><td> 引用編號為<number>的分組匹配到的字符串 </td><td> (\d)abc\1 </td><td> 1abc1 5abc5 </td></tr>
<tr><td> (?P=name) </td><td> 引用別名為<name>的分組匹配到的字符串。 </td><td> (?P<id>\d)abc(?P=id) </td><td> 1abc 5abc5 </td></tr>
<tr><td></td><td><em> 特殊構造(不作為分組) </em><td></td></td><td></td></tr>
<tr><td> (?:&#8230;) </td><td> (&#8230;)的不分組版本，用於使用&#8217;|&#8217;或後接數量詞 </td><td> (?:abc){2} </td><td> abc abc </td></tr>
<tr><td> (?iLmsux) </td><td> iLmsux的每個字符串代表一個匹配模式，只能用在正則表達式的開頭，可選多個。匹配模式將在下文中介紹。 </td><td> (?i)(abc) </td><td> AbC </td></tr>
<tr><td> (?#&#8230;) </td><td> #後的內容將作為註釋被忽略 </td><td> abc(?#comment)123 </td><td> abc123 </td></tr>
<tr><td> (?=&#8230;) </td><td> 之後的字符串內容需要匹配表達式才能成功匹配。不消耗字符串內容。 </td><td> abc(?=\d) </td><td> 後面是數字的a </td></tr>
<tr><td> (?!&#8230;) </td><td> 之後的字符串內容需要不匹配表達式才能成功匹配。不消耗字符串內容。 </td><td> abc(?!\d) </td><td> 後面不是數字的a </td></tr>
<tr><td> (?<=...) </td><td> 之前的字符串內容需要匹配表達式才能成功匹配。不消耗字符串內容。 </td><td> (?<=\d)a </td><td> 前面是數字的a </td></tr>
<tr><td> (?< !...) </td><td> 之前的字符串內容需要不匹配表達式才能成功匹配。不消耗字符串內容。 </td><td> (?<!\d)a </td><td> 前面不是數字的a </td></tr>
<tr><td> (?(id/name)yes-pattern|no-pattern) </td><td> 如果編號為id/別名為name的組匹配到字符，則需要皮皮yes-pattern，否則需要匹配no-pattern。 |np-pattern可以省略。 </td><td> (\d)abc(?(1)\d|abc) </td><td> 1abc2 abcabc </td></tr>
</tbody></table>


<p></p>

<h5>1.2 數量詞的貪婪模式與非貪婪模式</h5>

<p>正則表達式通常用於在文本中查找匹配的字符串。Python裏數量詞默認是貪婪的（在少數語言裏也可能是默認非貪婪），總是嘗試匹配盡可能多的字符；非貪婪的則相反，總是嘗試匹配盡可能少的字符。例如：正則表達式&#8221;a*&ldquo;如果用於查找&#8221;abbbc&#8221;，將找到&#8221;abbb&#8221;。而如果使用非貪婪的數量詞&#8221;ab*?&#8221;，將找到&#8221;a&#8221;。</p>

<h5>1.3. 反斜杠的困擾</h5>

<p>與大多數編程語言相同，正則表達式裏使用&#8221;\&ldquo;作為轉義字符，這就可能造成反斜杠困擾。假如你需要匹配文本中的字符&rdquo;\&ldquo;，那麽使用編程語言表示的正則表達式裏將需要4個反斜杠&rdquo;&#92;&#92;&ldquo;：前兩個和後兩個分別用於在編程語言裏轉義成反斜杠，轉換成兩個反斜杠後再在正則表達式裏轉義成一個反斜杠。Python裏的原生字符串很好地解決了這個問題，這個例子中的正則表達式可以使用r&rdquo;&#92;&ldquo;表示。同樣，匹配一個數字的&rdquo;&#92;d&#8221;可以寫成r&#8221;\d&#8221;。有了原生字符串，你再也不用擔心是不是漏寫了反斜杠，寫出來的表達式也更直觀。</p>

<h5>1.4. 匹配模式</h5>

<p>正則表達式提供了一些可用的匹配模式，比如忽略大小寫、多行匹配等，這部分內容將在Pattern類的工廠方法re.compile(pattern[, flags])中一起介紹。</p>

<h4>2. re模塊</h4>

<h5>2.1 開始使用re</h5>

<p>Python通過re模塊提供對正則表達式的支持。使用re的一般步驟是先將正則表達式的字符串形式編譯為Pattern實例，然後使用Pattern實例處理文本並獲得匹配結果（一個Match實例），最後使用Match實例獲得信息，進行其他的操作。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="c"># encoding: UTF-8</span>
</span><span class='line'><span class="kn">import</span> <span class="nn">re</span>
</span><span class='line'>
</span><span class='line'><span class="c"># 将正则表达式编译成Pattern对象</span>
</span><span class='line'><span class="n">pattern</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s">r&#39;hello&#39;</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c"># 使用Pattern匹配文本，获得匹配结果，无法匹配时将返回None</span>
</span><span class='line'><span class="n">match</span> <span class="o">=</span> <span class="n">pattern</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="s">&#39;hello world!&#39;</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="k">if</span> <span class="n">match</span><span class="p">:</span>
</span><span class='line'>    <span class="c"># 使用Match获得分组信息</span>
</span><span class='line'>    <span class="k">print</span> <span class="n">match</span><span class="o">.</span><span class="n">group</span><span class="p">()</span>
</span><span class='line'>
</span><span class='line'><span class="c">### 输出 ###</span>
</span><span class='line'><span class="c"># hello</span>
</span></code></pre></td></tr></table></div></figure>


<p></p>

<p><strong>re.compile(strPattern[, flag]):</strong><br/>
這個方法是Pattern類的工廠方法，用於將字符串形式的正則表達式編譯為Pattern對象。 第二個參數flag是匹配模式，取值可以使用按位或運算符&#8217;|&lsquo;表示同時生效，比如re.I | re.M。另外，你也可以在regex字符串中指定模式，比如re.compile(&#8216;pattern&rsquo;, re.I | re.M)與re.compile(&lsquo;(?im)pattern&rsquo;)是等價的。
可選值有：<br/>
* re.I(re.IGNORECASE): 忽略大小寫（括號內是完整寫法，下同）<br/>
* M(MULTILINE): 多行模式，改變&#8217;^&lsquo;和&rsquo;$&lsquo;的行為（參見上圖）<br/>
* S(DOTALL): 點任意匹配模式，改變&rsquo;.&lsquo;的行為<br/>
* L(LOCALE): 使預定字符類 \w \W \b \B \s \S 取決於當前區域設定<br/>
* U(UNICODE): 使預定字符類 \w \W \b \B \s \S \d \D 取決於unicode定義的字符屬性<br/>
* X(VERBOSE): 詳細模式。這個模式下正則表達式可以是多行，忽略空白字符，並可以加入註釋。以下兩個正則表達式是等價的：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">a</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s">r&quot;&quot;&quot;\d +  # the integral part</span>
</span><span class='line'><span class="s">                   \.    # the decimal point</span>
</span><span class='line'><span class="s">                   \d *  # some fractional digits&quot;&quot;&quot;</span><span class="p">,</span> <span class="n">re</span><span class="o">.</span><span class="n">X</span><span class="p">)</span>
</span><span class='line'><span class="n">b</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s">r&quot;\d+\.\d*&quot;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>
re提供了眾多模塊方法用於完成正則表達式的功能。這些方法可以使用Pattern實例的相應方法替代，唯一的好處是少寫一行re.compile()代碼，但同時也無法復用編譯後的Pattern對象。這些方法將在Pattern類的實例方法部分一起介紹。如上面這個例子可以簡寫為：</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">m</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="s">r&#39;hello&#39;</span><span class="p">,</span> <span class="s">&#39;hello world!&#39;</span><span class="p">)</span>
</span><span class='line'><span class="k">print</span> <span class="n">m</span><span class="o">.</span><span class="n">group</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure>


<p>
re模塊還提供了一個方法escape(string)，用於將string中的正則表達式元字符如*/+/?等之前加上轉義符再返回，在需要大量匹配元字符時有那麽一點用。</p>

<h5>2.2 Match</h5>

<p>Match对象是一次匹配的结果，包含了很多关于此次匹配的信息，可以使用Match提供的可读属性或方法来获取这些信息。</p>

<p>属性：<br/>
1) string: 匹配时使用的文本。<br/>
2) re: 匹配时使用的Pattern对象。<br/>
3) pos: 文本中正则表达式开始搜索的索引。值与Pattern.match()和Pattern.seach()方法的同名参数相同。<br/>
4) endpos: 文本中正则表达式结束搜索的索引。值与Pattern.match()和Pattern.seach()方法的同名参数相同。<br/>
5) lastindex: 最后一个被捕获的分组在文本中的索引。如果没有被捕获的分组，将为None。<br/>
6) lastgroup: 最后一个被捕获的分组的别名。如果这个分组没有别名或者没有被捕获的分组，将为None。</p>

<p>方法：<br/>
1) <strong>group([group1, …]):</strong> <br/>
获得一个或多个分组截获的字符串；指定多个参数时将以元组形式返回。group1可以使用编号也可以使用别名；编号0代表整个匹配的子串；不填写参数时，返回group(0)；没有截获字符串的组返回None；截获了多次的组返回最后一次截获的子串。<br/>
2) <strong>groups([default]):</strong><br/>
以元组形式返回全部分组截获的字符串。相当于调用group(1,2,…last)。default表示没有截获字符串的组以这个值替代，默认为None。<br/>
3) <strong>groupdict([default]):</strong><br/>
返回以有别名的组的别名为键、以该组截获的子串为值的字典，没有别名的组不包含在内。default含义同上。<br/>
4) <strong>start([group]):</strong><br/>
返回指定的组截获的子串在string中的起始索引（子串第一个字符的索引）。group默认值为0。<br/>
5) <strong>end([group]):</strong><br/>
返回指定的组截获的子串在string中的结束索引（子串最后一个字符的索引+1）。group默认值为0。<br/>
6) <strong>span([group]):</strong><br/>
返回(start(group), end(group))。<br/>
7) <strong>expand(template):</strong><br/>
将匹配到的分组代入template中然后返回。template中可以使用\id或\g<id>、\g<name>引用分组，但不能使用编号0。\id与\g<id>是等价的；但\10将被认为是第10个分组，如果你想表达\1之后是字符&#8217;0&#8217;，只能使用\g<1>0。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">re</span>
</span><span class='line'><span class="n">m</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="s">r&#39;(\w+) (\w+)(?P&lt;sign&gt;.*)&#39;</span><span class="p">,</span> <span class="s">&#39;hello world!&#39;</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="k">print</span> <span class="s">&quot;m.string:&quot;</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">string</span>
</span><span class='line'><span class="k">print</span> <span class="s">&quot;m.re:&quot;</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">re</span>
</span><span class='line'><span class="k">print</span> <span class="s">&quot;m.pos:&quot;</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">pos</span>
</span><span class='line'><span class="k">print</span> <span class="s">&quot;m.endpos:&quot;</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">endpos</span>
</span><span class='line'><span class="k">print</span> <span class="s">&quot;m.lastindex:&quot;</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">lastindex</span>
</span><span class='line'><span class="k">print</span> <span class="s">&quot;m.lastgroup:&quot;</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">lastgroup</span>
</span><span class='line'>
</span><span class='line'><span class="k">print</span> <span class="s">&quot;m.group(1,2):&quot;</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span><span class='line'><span class="k">print</span> <span class="s">&quot;m.groups():&quot;</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">groups</span><span class="p">()</span>
</span><span class='line'><span class="k">print</span> <span class="s">&quot;m.groupdict():&quot;</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">groupdict</span><span class="p">()</span>
</span><span class='line'><span class="k">print</span> <span class="s">&quot;m.start(2):&quot;</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">start</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</span><span class='line'><span class="k">print</span> <span class="s">&quot;m.end(2):&quot;</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">end</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</span><span class='line'><span class="k">print</span> <span class="s">&quot;m.span(2):&quot;</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">span</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</span><span class='line'><span class="k">print</span> <span class="s">r&quot;m.expand(r&#39;\2 \1\3&#39;):&quot;</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="s">r&#39;\2 \1\3&#39;</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c">### output ###</span>
</span><span class='line'><span class="c"># m.string: hello world!</span>
</span><span class='line'><span class="c"># m.re: &lt;_sre.SRE_Pattern object at 0x016E1A38&gt;</span>
</span><span class='line'><span class="c"># m.pos: 0</span>
</span><span class='line'><span class="c"># m.endpos: 12</span>
</span><span class='line'><span class="c"># m.lastindex: 3</span>
</span><span class='line'><span class="c"># m.lastgroup: sign</span>
</span><span class='line'><span class="c"># m.group(1,2): (&#39;hello&#39;, &#39;world&#39;)</span>
</span><span class='line'><span class="c"># m.groups(): (&#39;hello&#39;, &#39;world&#39;, &#39;!&#39;)</span>
</span><span class='line'><span class="c"># m.groupdict(): {&#39;sign&#39;: &#39;!&#39;}</span>
</span><span class='line'><span class="c"># m.start(2): 6</span>
</span><span class='line'><span class="c"># m.end(2): 11</span>
</span><span class='line'><span class="c"># m.span(2): (6, 11)</span>
</span><span class='line'><span class="c"># m.expand(r&#39;\2 \1\3&#39;): world hello!</span>
</span></code></pre></td></tr></table></div></figure>


<p></p>

<h5>2.3 Pattern</h5>

<p>Pattern對象是一個編譯好的正則表達式，通過Pattern提供的一系列方法可以對文本進行匹配查找。<br/>
Pattern不能直接實例化，必須使用re.compile()進行構造。<br/>
Pattern提供了幾個可讀屬性用於獲取表達式的相關信息：<br/>
1) pattern: 編譯時用的表達式字符串。<br/>
2) flags: 編譯時用的匹配模式。數字形式。<br/>
3) groups: 表達式中分組的數量。<br/>
4) groupindex: 以表達式中有別名的組的別名為鍵、以該組對應的編號為值的字典，沒有別名的組不包含在內。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">re</span>
</span><span class='line'><span class="n">p</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s">r&#39;(\w+) (\w+)(?P&lt;sign&gt;.*)&#39;</span><span class="p">,</span> <span class="n">re</span><span class="o">.</span><span class="n">DOTALL</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="k">print</span> <span class="s">&quot;p.pattern:&quot;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">pattern</span>
</span><span class='line'><span class="k">print</span> <span class="s">&quot;p.flags:&quot;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">flags</span>
</span><span class='line'><span class="k">print</span> <span class="s">&quot;p.groups:&quot;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">groups</span>
</span><span class='line'><span class="k">print</span> <span class="s">&quot;p.groupindex:&quot;</span><span class="p">,</span> <span class="n">p</span><span class="o">.</span><span class="n">groupindex</span>
</span><span class='line'>
</span><span class='line'><span class="c">### output ###</span>
</span><span class='line'><span class="c"># p.pattern: (\w+) (\w+)(?P&lt;sign&gt;.*)</span>
</span><span class='line'><span class="c"># p.flags: 16</span>
</span><span class='line'><span class="c"># p.groups: 3</span>
</span><span class='line'><span class="c"># p.groupindex: {&#39;sign&#39;: 3}</span>
</span></code></pre></td></tr></table></div></figure>


<p>
实例方法[ | re模块方法]：<br/>
1) <strong>match(string[, pos[, endpos]]) | re.match(pattern, string[, flags]):</strong><br/>
这个方法将从string的pos下标处起尝试匹配pattern；如果pattern结束时仍可匹配，则返回一个Match对象；如果匹配过程中pattern无法匹配，或者匹配未结束就已到达endpos，则返回None。<br/>
pos和endpos的默认值分别为0和len(string)；re.match()无法指定这两个参数，参数flags用于编译pattern时指定匹配模式。<br/>
注意：这个方法并不是完全匹配。当pattern结束时若string还有剩余字符，仍然视为成功。想要完全匹配，可以在表达式末尾加上边界匹配符&#8217;$&lsquo;。<br/>
示例参见2.1小节。
2) <strong>search(string[, pos[, endpos]]) | re.search(pattern, string[, flags]):</strong><br/>
这个方法用于查找字符串中可以匹配成功的子串。从string的pos下标处起尝试匹配pattern，如果pattern结束时仍可匹配，则返回一个Match对象；若无法匹配，则将pos加1后重新尝试匹配；直到pos=endpos时仍无法匹配则返回None。<br/>
pos和endpos的默认值分别为0和len(string))；re.search()无法指定这两个参数，参数flags用于编译pattern时指定匹配模式。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="c"># encoding: UTF-8 </span>
</span><span class='line'><span class="kn">import</span> <span class="nn">re</span>
</span><span class='line'>
</span><span class='line'><span class="c"># 将正则表达式编译成Pattern对象 </span>
</span><span class='line'><span class="n">pattern</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s">r&#39;world&#39;</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c"># 使用search()查找匹配的子串，不存在能匹配的子串时将返回None </span>
</span><span class='line'><span class="c"># 这个例子中使用match()无法成功匹配 </span>
</span><span class='line'><span class="n">match</span> <span class="o">=</span> <span class="n">pattern</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="s">&#39;hello world!&#39;</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="k">if</span> <span class="n">match</span><span class="p">:</span>
</span><span class='line'>    <span class="c"># 使用Match获得分组信息 </span>
</span><span class='line'>    <span class="k">print</span> <span class="n">match</span><span class="o">.</span><span class="n">group</span><span class="p">()</span>
</span><span class='line'>
</span><span class='line'><span class="c">### 输出 ### </span>
</span><span class='line'><span class="c"># world</span>
</span></code></pre></td></tr></table></div></figure>


<p>
3) <strong>split(string[, maxsplit]) | re.split(pattern, string[, maxsplit]):</strong><br/>
按照能夠匹配的子串將string分割後返回列表。maxsplit用於指定最大分割次數，不指定將全部分割。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">re</span>
</span><span class='line'>
</span><span class='line'><span class="n">p</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s">r&#39;\d+&#39;</span><span class="p">)</span>
</span><span class='line'><span class="k">print</span> <span class="n">p</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&#39;one1two2three3four4&#39;</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c">### output ###</span>
</span><span class='line'><span class="c"># [&#39;one&#39;, &#39;two&#39;, &#39;three&#39;, &#39;four&#39;, &#39;&#39;]</span>
</span></code></pre></td></tr></table></div></figure>


<p>
4) <strong>findall(string[, pos[, endpos]]) | re.findall(pattern, string[, flags]):</strong><br/>
搜索string，以列表形式返回全部能匹配的子串。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">re</span>
</span><span class='line'>
</span><span class='line'><span class="n">p</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s">r&#39;\d+&#39;</span><span class="p">)</span>
</span><span class='line'><span class="k">print</span> <span class="n">p</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="s">&#39;one1two2three3four4&#39;</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c">### output ###</span>
</span><span class='line'><span class="c"># [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;]</span>
</span></code></pre></td></tr></table></div></figure>


<p>
5) <strong>finditer(string[, pos[, endpos]]) | re.finditer(pattern, string[, flags]):</strong><br/>
搜索string，返回一個順序訪問每一個匹配結果（Match對象）的叠代器。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">re</span>
</span><span class='line'>
</span><span class='line'><span class="n">p</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s">r&#39;\d+&#39;</span><span class="p">)</span>
</span><span class='line'><span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">finditer</span><span class="p">(</span><span class="s">&#39;one1two2three3four4&#39;</span><span class="p">):</span>
</span><span class='line'>    <span class="k">print</span> <span class="n">m</span><span class="o">.</span><span class="n">group</span><span class="p">(),</span>
</span><span class='line'>
</span><span class='line'><span class="c">### output ###</span>
</span><span class='line'><span class="c"># 1 2 3 4</span>
</span></code></pre></td></tr></table></div></figure>


<p>
6) <strong>sub(repl, string[, count]) | re.sub(pattern, repl, string[, count]):</strong><br/>
使用repl替換string中每一個匹配的子串後返回替換後的字符串。<br/>
當repl是一個字符串時，可以使用\id或\g<id>、\g<name>引用分組，但不能使用編號0。<br/>
當repl是一個方法時，這個方法應當只接受一個參數（Match對象），並返回一個字符串用於替換（返回的字符串中不能再引用分組）。<br/>
count用於指定最多替換次數，不指定時全部替換。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">re</span>
</span><span class='line'>
</span><span class='line'><span class="n">p</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s">r&#39;(\w+) (\w+)&#39;</span><span class="p">)</span>
</span><span class='line'><span class="n">s</span> <span class="o">=</span> <span class="s">&#39;i say, hello world!&#39;</span>
</span><span class='line'>
</span><span class='line'><span class="k">print</span> <span class="n">p</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s">r&#39;\2 \1&#39;</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
</span><span class='line'>    <span class="k">return</span> <span class="n">m</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">title</span><span class="p">()</span> <span class="o">+</span> <span class="s">&#39; &#39;</span> <span class="o">+</span> <span class="n">m</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">title</span><span class="p">()</span>
</span><span class='line'>
</span><span class='line'><span class="k">print</span> <span class="n">p</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c">### output ###</span>
</span><span class='line'><span class="c"># say i, world hello!</span>
</span><span class='line'><span class="c"># I Say, Hello World!</span>
</span></code></pre></td></tr></table></div></figure>


<p>
7) <strong>subn(repl, string[, count]) |re.sub(pattern, repl, string[, count]):</strong><br/>
返回 (sub(repl, string[, count]), 替換次數)。</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">import</span> <span class="nn">re</span>
</span><span class='line'>
</span><span class='line'><span class="n">p</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s">r&#39;(\w+) (\w+)&#39;</span><span class="p">)</span>
</span><span class='line'><span class="n">s</span> <span class="o">=</span> <span class="s">&#39;i say, hello world!&#39;</span>
</span><span class='line'>
</span><span class='line'><span class="k">print</span> <span class="n">p</span><span class="o">.</span><span class="n">subn</span><span class="p">(</span><span class="s">r&#39;\2 \1&#39;</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
</span><span class='line'>    <span class="k">return</span> <span class="n">m</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">title</span><span class="p">()</span> <span class="o">+</span> <span class="s">&#39; &#39;</span> <span class="o">+</span> <span class="n">m</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">title</span><span class="p">()</span>
</span><span class='line'>
</span><span class='line'><span class="k">print</span> <span class="n">p</span><span class="o">.</span><span class="n">subn</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'><span class="c">### output ###</span>
</span><span class='line'><span class="c"># (&#39;say i, world hello!&#39;, 2)</span>
</span><span class='line'><span class="c"># (&#39;I Say, Hello World!&#39;, 2)</span>
</span></code></pre></td></tr></table></div></figure>


<p></p>

<p><a href="http://www.cnblogs.com/huxi/archive/2010/07/04/1771073.html">Origin</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tips on Writing(Fw)]]></title>
    <link href="http://www.aprilzephyr.com/blog/05022014/tips-on-writing/"/>
    <updated>2014-05-02T16:41:00+08:00</updated>
    <id>http://www.aprilzephyr.com/blog/05022014/tips-on-writing</id>
    <content type="html"><![CDATA[<h4>Decide What You Think First</h4>

<p>When working on a challenging task — writing a speech, preparing an important presentation, or developing a new idea — it&rsquo;s helpful to get feedback from others. Do they think it&rsquo;s any good? In what direction do they think you should take it? But sometimes, too much feedback can drown out the most important opinion: your own. If you feel like you&rsquo;re getting too much input or are no longer sure what you think of your own work, take a break from the feedback. Decide what you think. This will build your confidence and trust in yourself. Once you&rsquo;ve articulated and refined your own perspective, reach back out to your trusted advisors to get theirs.<br/>
Adapted from <a href="http://blogs.hbr.org/2010/11/how-to-teach-yourself-to-trust/">&ldquo;How to Teach Yourself to Trust Yourself&rdquo; by Peter Bregman.</a><!--more--></p>

<h4>Three Ways to Tighten Your Writing</h4>

<p>Writing today—a report, memo, or email—must be short if you want people to read it. But succinctly expressing yourself can be tough. Here are three ways to trim your writing and say what you want in fewer words:<br/>
* <strong>Refine it.</strong> Take a hard look at the structure of your writing. Only include sections that are necessary to support your points.<br/>
* <strong>Consider an informal tone.</strong> Just because you&rsquo;re writing a report doesn&rsquo;t mean you need to be formal. Writing like a bureaucrat makes you use longer words and a complicated sentence structure. Adopting a more informal tone often helps you be direct and concise.<br/>
* <strong>Cut and then cut more.</strong> Look over your document sentence by sentence. If a sentence doesn&rsquo;t serve an important purpose, get rid of it.<br/>
Adapted from <a href="http://hbr.org/product/guide-to-better-business-writing-2nd-edition/an/10919-PDF-ENG">Guide to Better Business Writing.</a></p>

<h4>Choose Clarity over Brevity</h4>

<p>Writing experts emphasize the importance of using as few words as possible to deliver your message. The evolution of technology has supported this trend toward brevity; see tweets, status updates, and text messages as examples. But we may have gone too far. Sometimes messages that are too brief sacrifice clarity and leave out crucial information. When crafting your next message, choose clarity over brevity; include all relevant information and be sure it is logically organized. This is as true for PowerPoint presentations and research reports as it is for emails. Being brief is important but not at the risk of being misunderstood.<br/>
Adapted from <a href="http://blogs.hbr.org/2009/10/when-clarity-is-not-the-same-a/">&ldquo;When Clarity is Not the Same as Brevity&rdquo; by David Silverman.</a></p>

<h4>Three Tips for Writing Reader-Friendly Memos</h4>

<p>In business today, readers are time-pressed, content-driven, and decision-focused. To write effectively, remember that they want simple and direct communications. Here are three tips for giving readers what they want and need:<br/>
* <strong>Avoid complex phrasing.</strong> Writing elegantly is not important; delivering smart content is. Let the message stand out more than your language.<br/>
* <strong>Be concise.</strong> Many memo writers get hung up on &ldquo;flow.&rdquo; But flowing sentences tend to be long and dense. You don&rsquo;t need choppy sentences, just hardworking ones that deliver content concisely.<br/>
* <strong>Skip the jargon.</strong> Jargon can be a useful way to communicate among experts, but you should never use jargon if it&rsquo;s meaningless, if you don&rsquo;t understand it, or when your audience isn&rsquo;t familiar with it.<br/>
Adapted from <a href="http://hbr.org/product/guide-to-better-business-writing-2nd-edition/an/10919-PDF-ENG">Guide to Better Business Writing.</a></p>

<h4>Three Rules for Making Your Writing Clear</h4>

<p>In business writing, you get points for clarity, not style. Instead of trying to wax poetic about your division&rsquo;s plans for the next 60 days, just make your point. Here are three ways to do that:<br/>
* <strong>One idea per paragraph.</strong> Novels hold several complex ideas and emotions in a single paragraph. In business writing, limit your thoughts to one per paragraph. When you have another suggestion, thought or idea, start a  new paragraph.<br/>
* <strong>Put your point in the first sentence.</strong> Don&rsquo;t entice your readers with background information and build-up. No one has time for that. Make your primary point first. Then go into supporting detail.<br/>
* <strong>Make it &ldquo;scannable.&rdquo;</strong> Few people read every word in an email. Use headers and bullet points so that your audience can quickly scan your message and understand your point.<br/>
Adapted from <a href="http://blogs.hbr.org/2011/03/how-to-succeed-in-business-wri/">&ldquo;How to Succeed in Business Writing: Don&rsquo;t Be Dickens&rdquo; by David Silverman.</a></p>

<p><a href="http://hbr.org/web/management-tip/tips-on-writing">Origin</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[這樣寫英文Email]]></title>
    <link href="http://www.aprilzephyr.com/blog/04172014/zhe-yang-xie-ying-wen-email/"/>
    <updated>2014-04-17T17:15:01+08:00</updated>
    <id>http://www.aprilzephyr.com/blog/04172014/zhe-yang-xie-ying-wen-email</id>
    <content type="html"><![CDATA[<p><strong>1. 郵件的開頭</strong><br/>
感謝讀者是郵件開場白的好辦法。感謝您的讀者能讓對方感到高興，特別是之後你有事相求的情況下會很有幫助。</p>

<p>Thank you for contacting us.如果有人寫信來詢問公司的服務，就可以使用這句句子開頭。向他們對公司的興趣表示感謝。</p>

<p>Thank you for your prompt reply.當一個客戶或是同事很快就回復了你的郵件，一定記得要感謝他們。如果回復並不及時，只要將“prompt”除去即可，你還可以說，“Thank you for getting back to me.”<!--more--></p>

<p>Thank you for providing the requested information.如果你詢問某人一些信息，他們花了點時間才發送給你，那就用這句句子表示你仍然對他們的付出表示感激。</p>

<p>Thank you for all your assistance.如果有人給了你特別的幫助，那一定要感謝他們！如果你想對他們表示特別的感激，就用這個句子，“I truly appreciate … your help in resolving the problem.”Thank you raising your concerns.</p>

<p>就算某個客戶或是經理寫郵件給你對你的工作提出了一定的質疑，你還是要感謝他們。這樣你能表現出你對他們的認真態度表示尊重及感激。同時，你也可以使用，“Thank you for your feedback.”</p>

<p><strong>2. 郵件的結尾</strong> <br/>
在郵件開頭表示感謝一般是表示對對方過去付出的感謝，而在郵件結尾處表示感謝是對將來的幫助表示感謝。事先表示感謝，能讓對方在行動時更主動更樂意。</p>

<p>Thank you for your kind cooperation.如果你需要讀者幫助你做某事，那就先得表示感謝。</p>

<p>Thank you for your attention to this matter.與以上的類似，本句包含了你對對方將來可能的幫助表示感謝。</p>

<p>Thank you for your understanding.如果你寫到任何會對讀者產生負面影響的內容那就使用這句句子吧。</p>

<p>Thank you for your consideration.如果您是在尋求機會或是福利，例如你在求職的話，就用這封郵件結尾。</p>

<p>Thank you again for everything you&rsquo;ve done.這句句子可以用在結尾，和以上有所不同。如果你在郵件開頭已經謝過了讀者，你就可以使用這句話，但是因為他們的幫助，你可以著重再次感謝你們的付出。</p>

<p><strong>3. 十種場合的表達</strong><br/>
1) <em>Greeting message 祝福</em></p>

<p>Hope you have a good trip back. 祝旅途愉快。</p>

<p>How are you? 你好嗎?</p>

<p>How is the project going? 項目進行順利嗎?</p>

<p><em>2) Initiate a meeting 發起會議</em><br/>
I suggest we have a call tonight at 9:30pm (China Time) with you and Brown. Please let me know if the time is okay for you and Ben.<br/>
我建議我們今晚九點半和Brown小聚一下,你和Ben有沒有空?</p>

<p>I would like to hold a meeting in the afternoon about our development planning for the project A.<br/>
今天下午我建議我們就A項目的發展計劃開會討論一下。</p>

<p>We’d like to have the meeting on Thu Oct 30. Same time.<br/>
十月三十號(周四),老時間,開會。</p>

<p>Let’s make a meeting next Monday at 5:30 PM SLC time.<br/>
下周一鹽湖城時區下午五點半開會。</p>

<p>I want to talk to you over the phone regarding issues about report development and the XXX project.<br/>
我想跟你電話討論下報告進展和XXX項目的情況。</p>

<p><em>3) Seeking for more information/feedbacks/suggestions 咨詢信息/反饋/建議</em></p>

<p>Should you have any problem accessing the folders, please let me know.<br/>
如果存取文件有任何問題請和我聯系。</p>

<p>Thank you and look forward to having your opinion on the estimation and schedule.<br/>
謝謝你,希望能聽到更多你對評估和日程計劃的建議。</p>

<p>Look forward to your feedbacks and suggestions soon.<br/>
期待您的反饋建議!</p>

<p>What is your opinion on the schedule and next steps we proposed?<br/>
你對計劃方面有什麽想法?下一步我們應該怎麽做?</p>

<p>What do you think about this?<br/>
這個你怎麽想?</p>

<p>Feel free to give your comments.<br/>
請隨意提出您的建議。</p>

<p>Any question, please don’t hesitate to let me know.<br/>
有任何問題,歡迎和我們聯系。</p>

<p>Any question, please let me know.<br/>
有任何問題,歡迎和我們聯系。</p>

<p>Please contact me if you have any questions.<br/>
有任何問題,歡迎和我們聯系。</p>

<p>Please let me know if you have any question on this.<br/>
有任何問題,歡迎和我聯系。</p>

<p>Your comments and suggestions are welcome!<br/>
歡迎您的評論和建議!</p>

<p>Please let me know what you think?<br/>
歡迎您的評論和建議!</p>

<p>Do you have any idea about this?<br/>
對於這個您有什麽建議嗎?</p>

<p>It would be nice if you could provide a bit more information on the user’s behavior.<br/>
您若是能夠就用戶行為方面提供更多的信息就太感激了!</p>

<p>At your convenience, I would really appreciate you looking into this matter/issue.<br/>
如果可以,我希望你能負責這件事情。</p>

<p><em>4) Give feedback 意見反饋</em><br/>
Please see comments below.<br/>
請看下面的評論。</p>

<p>My answers are in blue below.<br/>
我的回答已標藍。</p>

<p>I add some comments to the document for your reference.<br/>
我加了些評論給你參考。</p>

<p><em>5) Attachment 附件</em><br/>
I enclose the evaluation report for your reference.<br/>
我附加了評估報告供您閱讀。</p>

<p>Attached please find today’s meeting notes.<br/>
今天的會議記錄在附件裏。</p>

<p>Attach is the design document, please review it.<br/>
設計文檔在附件裏,請評閱。</p>

<p>For other known issues related to individual features, please see attached release notes.<br/>
其他個人特征方面的信息請見附件。</p>

<p><em>6) Point listing 列表</em><br/>
Today we would like to finish following tasks by the end of today:1…….2…….<br/>
今天我們要完成的任務:1…….2…….</p>

<p>Some known issues in this release:1…….2…….<br/>
聲明中涉及的一些問題:1…….2…….</p>

<p>Our team here reviewed the newest SCM policy and has following concerns:1…….2…….<br/>
我們閱讀了最新的供應鏈管理政策,做出如下考慮:1…….2…….</p>

<p>Here are some more questions/issues for your team:1…….2…….<br/>
以下是對你們團隊的一些問題:1…….2…….</p>

<p>The current status is as following: 1…… 2……<br/>
目前數據如下: 1…… 2……</p>

<p>Some items need your attention:1…….2…….<br/>
以下方面需提請註意:1…….2…….</p>

<p><em>7) Raise question 提出問題</em><br/>
I have some questions about the report XX-XXX.<br/>
我對XX-XXX報告有一些疑問。</p>

<p>For the assignment ABC, I have the following questions:…<br/>
就ABC協議,我有以下幾個問題:……</p>

<p><em>8) Proposal 提議</em><br/>
For the next step of platform implementation, I am proposing…<br/>
關於平臺啟動的下一步計劃,我有一個提議……</p>

<p>I suggest we can have a weekly project meeting over the phone call in the near future.<br/>
我建議我們就一周項目開一個電話會議。</p>

<p>Achievo team suggest to adopt option A to solve outstanding issue……<br/>
Achievo團隊建議應對突出問題采用A辦法。</p>

<p><em>9) Thanks note 感謝信</em><br/>
Thank you so much for the cooperation.<br/>
感謝你的合作!</p>

<p>Thanks for the information.<br/>
謝謝您提供的信息!</p>

<p>I really appreciate the effort you all made for this sudden and tight project.<br/>
對如此緊急的項目您做出的努力我表示十分感謝。</p>

<p>Thank you for your attention!</p>

<p>Thanks to your attention!<br/>
謝謝關心!</p>

<p>Your kind assistance on this are very much appreciated.<br/>
我們對您的協助表示感謝。</p>

<p>Really appreciate your help!<br/>
非常感謝您的幫助!</p>

<p><em>10) Apology 道歉</em><br/>
I sincerely apologize for this misunderstanding!<br/>
對造成的誤解我真誠道歉!</p>

<p>I apologize for the late asking but we want to make sure the correctness of our implementation ASAP.<br/>
很抱歉現在才進行詢問,但是我們需要盡快核實執行信息。</p>

<p><strong>4. 分清目標</strong><br/>
Informal – Thanks for the email of 15 February.<br/>
Formal – Thank you for your email received 15 February.</p>

<p>Informal – Sorry, I can’t make it.<br/>
Formal – I am afraid I will not be able to attend.</p>

<p>Informal – Could you…?<br/>
Formal – I was wondering if you could….?</p>

<p><strong>5. 直接與間接表達</strong><br/>
Direct – I need this in half an hour.<br/>
Indirect and polite – Would it be possible to have this in half an hour?</p>

<p>Direct – There will be a delay.<br/>
Indirect – I’m afraid there will be a slight delay.</p>

<p>Direct – It’s a bad idea.<br/>
Indirect – To be honest, I’m not sure it would be a good idea.</p>

<p><strong>6. 用詞正面</strong><br/>
Look at these words: helpful, good question, agreed, together, useful, I’d be delighted, mutual, opportunity.</p>

<p>Now look at these: busy, crisis, failure, forget it, I can’t, it’s impossible, waste, hard.</p>

<p>The words you use show your attitude to life so choose your words wisely.</p>

<p><strong>7. Business Email Format</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Subject: ___________
</span><span class='line'>
</span><span class='line'>Dear Sir,
</span><span class='line'>
</span><span class='line'>Reference to your _______ dated ______ regarding ____, I would like to intimate that ___________. You will soon receive a detailed hard copy regarding the same.
</span><span class='line'>
</span><span class='line'>For any further queries, please feel free to contact me on my email address or phone number xxx-xxxx.
</span><span class='line'>
</span><span class='line'>Thanks and best regards,
</span><span class='line'>
</span><span class='line'>Sender information
</span><span class='line'>Sender Designation
</span><span class='line'>Company Name
</span><span class='line'>Contact number
</span><span class='line'>
</span><span class='line'>PS: This is a computer generated message and thus bears no signatures.</span></code></pre></td></tr></table></div></figure>


<p></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>To: "Anna Jones" 
</span><span class='line'>Cc: All Staff
</span><span class='line'>From: "James Brown" jamesbrown@abcd.com
</span><span class='line'>Subject: Welcome to our Hive!
</span><span class='line'>
</span><span class='line'>Dear Anna,
</span><span class='line'>
</span><span class='line'>Welcome to our Hive!
</span><span class='line'>
</span><span class='line'>It is a pleasure to welcome you to the team of ___________. We are excited to have you join our team, and we hope that you will enjoy working with our company.
</span><span class='line'>
</span><span class='line'>On the last Saturday of each month we hold a special staff party to welcome any new employees. Please be sure to come next week to meet all of our senior staff and any other new staff members who have joined ___________ this month. You will receive an e-mail regarding the same with further details.
</span><span class='line'>
</span><span class='line'>If you have any questions during your training period, please do not hesitate to contact me. You can reach me at my email address or on my office line at 000-0001.
</span><span class='line'>
</span><span class='line'>Warm regards,
</span><span class='line'>James
</span><span class='line'>
</span><span class='line'>Jackie Brown, Manager, Staff
</span><span class='line'>jamesbrown@abcd.com
</span><span class='line'>Tel: 000-0001
</span><span class='line'>Read more at Buzzle: http://www.buzzle.com/articles/business-email-format.html</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Stay Hungry, Stay Foolish]]></title>
    <link href="http://www.aprilzephyr.com/blog/04082014/stay-hungry/"/>
    <updated>2014-04-08T14:43:38+08:00</updated>
    <id>http://www.aprilzephyr.com/blog/04082014/stay-hungry</id>
    <content type="html"><![CDATA[<p><img src="http://www.aprilzephyr.com/images/stayhsf.png"><br/>
<strong>&ldquo;Stay Hungry, Stay Foolish.&rdquo;</strong> Such resounding words delivered by Steve Jobs in 2005 Stanford Commencement Address which, was actually not the topic today. The quote is just about some reflections of a movie <a href="http://www.imdb.com/title/tt0107048/"><em>Groundhog Day</em></a>.<!--more--></p>

<p><strong>&ldquo;What would you do if you were stuck in one place and every day was exactly the same, and nothing that you did mattered?&rdquo;</strong></p>

<p>Phil Connors, the hero in that movie, an arrogant and egocentric Pittsburgh TV weatherman who, during a disgusted assignment covering the annual Groundhog Day event in Punxsutawney, finds himself in a time loop, repeating the same day over and over and over again. After indulging in periods of hedonism, dismay, bitterness, despair then numerous suicide attempts, he begins to re-examine his life and tries to change &mdash; improving and enriching himself through learning new skills and helping people.</p>

<p>Some plots are interesting that when Phil tries harder and harder to cater to someone he is interested, either terrible consequences(such as slaps on his face) come or void&rsquo;s arising even he succeeds. It might be because Phil attempts to alter the original himself which he actually couldn&rsquo;t. Such alternation would not last long even though he makes it for a flash. Only when Phil accepts the circumstances calmly, patiently and ready to do some &ldquo;changes&rdquo; would he handle the &ldquo;magic&rdquo; to break such time loop and win his queen&rsquo;s heart.</p>

<p>And so is the story telling us.</p>

<p>In real life, things are so similar that few ones are satisfied with their presents living roughly the same days over and over and over again. Senses of helplessness and so far as abomination towards their own never vanish. Sorrowfully that only fewer ones would like or dare to make a change as to be sluggish, revolted and desperate dying finally.</p>

<p>Staying hungry and foolish, is only for adequate constant advances for the nature of oneself could not be changed but the substance. There would be one and the only way to get rid of such suck life rhythm is to get cultured. Never scared, even if looked like deserted weeds temporarily. Just endeavor, struggle, enrich oneself, waiting patiently for the moment handling that fabulous magic to break one&rsquo;s own &ldquo;time loop&rdquo;.</p>
]]></content>
  </entry>
  
</feed>
